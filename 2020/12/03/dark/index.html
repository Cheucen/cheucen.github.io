
<!DOCTYPE html>
<html lang="zh-CN" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>pit - Cheucen</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Cheucen,"> 
    <meta name="description" content="目录
第1章    项目涉及技术    12

  1.1 Linux&amp;amp;Shell    12


  1.1.1 Linux常用高级命令    12


  1.1.2 Shell常用工具,"> 
    <meta name="author" content="Cheucen"> 
    <link rel="alternative" href="atom.xml" title="Cheucen" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
    
<link rel="stylesheet" href="/css/diaspora.css">

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
<meta name="generator" content="Hexo 5.2.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">Cheucen</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://example.com"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">pit</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">pit</h1>
        <div class="stuff">
            <span>十二月 03, 2020</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/big-data/" rel="tag">big_data</a></li></ul>


        </div>
        <div class="content markdown">
            <p>目录</p>
<p><a href="#_Toc42539336">第1章    项目涉及技术    12</a></p>
<blockquote>
<p>  <a href="#_Toc42539337">1.1 Linux&amp;Shell    12</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539338">1.1.1 Linux常用高级命令    12</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539339">1.1.2 Shell常用工具及写过的脚本    13</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539340">1.1.3<br>  Shell中提交了一个脚本，进程号已经不知道了，但是需要kill掉这个进程，怎么操作?    13</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539341">1.1.4 Shell中单引号和双引号区别    13</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539342">1.2 Hadoop    14</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539343">1.2.1 Hadoop常用端口号    14</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539344">1.2.2 Hadoop配置文件以及简单的Hadoop集群搭建    14</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539345">1.2.3 HDFS读流程和写流程    14</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539346">1.2.4 HDFS小文件处理    15</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539347">1.2.5 Shuffle及优化    15</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539348">1.2.6 Yarn工作机制    18</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539349">1.2.7 Yarn调度器    18</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539350">1.2.8 项目经验之基准测试    19</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539351">1.2.9 Hadoop宕机    19</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539352">1.2.10 Hadoop解决数据倾斜方法    19</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539353">1.2.11 集群资源分配参数（项目中遇到的问题）    20</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539354">1.3 Zookeeper    20</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539355">1.3.1 选举机制    20</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539356">1.3.2 常用命令    20</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539357">1.3.3 Paxos算法（扩展）    20</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539358">1.3.4<br>  讲一讲什么是CAP法则？Zookeeper符合了这个法则的哪两个？（扩展）    21</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539359">1.4 Flume    21</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539360">1.4.1 Flume组成，Put事务，Take事务    21</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539361">1.4.2 Flume拦截器    23</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539362">1.4.3 Flume Channel选择器    24</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539363">1.4.4 Flume监控器    24</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539364">1.4.5 Flume采集数据会丢失吗?（防止数据丢失的机制）    24</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539365">1.5 Kafka    24</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539366">1.5.1 Kafka架构    24</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539367">1.5.2 Kafka的机器数量    25</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539368">1.5.3 副本数设定    25</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539369">1.5.4 Kafka压测    26</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539370">1.5.5 Kafka日志保存时间    26</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539371">1.5.6 Kafka中数据量计算    26</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539372">1.5.7 Kafka的硬盘大小    26</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539373">1.5.8 Kafka监控    26</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539374">1.5.9 Kakfa分区数    26</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539375">1.5.10 多少个Topic    27</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539376">1.5.11 Kafka的ISR副本同步队列    27</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539377">1.5.12 Kafka分区分配策略    27</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539378">1.5.13 Kafka挂掉    27</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539379">1.5.14 Kafka丢不丢数据    28</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539380">1.5.15 Kafka数据重复    28</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539381">1.5.16 Kafka消息数据积压，Kafka消费能力不足怎么处理？    28</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539382">1.5.17 Kafka参数优化    28</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539383">1.5.18 Kafka高效读写数据    29</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539384">1.5.19 Kafka支持传输    29</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539385">1.5.20 Kafka过期数据清理    29</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539386">1.5.21 Kafka可以按照时间消费数据    30</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539387">1.5.22 Kafka消费者角度考虑是拉取数据还是推送数据    30</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539388">1.5.23 Kafka中的数据是有序的吗    30</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539389">1.6 Hive    30</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539390">1.6.1 Hive的架构    30</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539391">1.6.2 Hive和数据库比较    30</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539392">1.6.3 内部表和外部表    31</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539393">1.6.4 4个By区别    31</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539394">1.6.5 系统函数    31</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539395">1.6.6 自定义UDF、UDTF函数    32</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539396">1.6.7 窗口函数    32</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539397">1.6.8 Hive优化    32</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539398">1.6.9 Hive解决数据倾斜方法    34</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539399">1.6.10<br>  Hive里边字段的分隔符用的什么？为什么用\t？有遇到过字段里边有\t的情况吗，怎么处理的？    36</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539400">1.6.11 Tez引擎优点？    36</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539401">1.6.12 MySQL元数据备份    36</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539402">1.6.13 Union与Union all区别    37</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539403">1.7 Sqoop    38</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539404">1.7.1 Sqoop参数    38</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539405">1.7.2 Sqoop导入导出Null存储一致性问题    38</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539406">1.7.3 Sqoop数据导出一致性问题    38</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539407">1.7.4 Sqoop底层运行的任务是什么    39</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539408">1.7.5 Sqoop一天导入多少数据    39</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539409">1.7.6 Sqoop数据导出的时候一次执行多长时间    39</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539410">1.7.7 Sqoop在导入数据的时候数据倾斜    39</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539411">1.7.8 Sqoop数据导出Parquet（项目中遇到的问题）    39</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539412">1.8 Azkaban    39</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539413">1.8.1 每天集群运行多少指标?    39</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539414">1.8.2 任务挂了怎么办？    39</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539415">1.9 HBase    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539416">1.9.1 HBase存储结构    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539417">1.9.2 RowKey设计原则    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539418">1.9.3 RowKey如何设计    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539419">1.9.4 Phoenix二级索引（讲原理）    40</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539420">1.10 Scala    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539421">1.10.1 开发环境    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539422">1.10.2 变量和数据类型    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539423">1.10.3 流程控制    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539424">1.10.4 函数式编程    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539425">1.10.5 面向对象    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539426">1.10.6 集合    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539427">1.10.7 模式匹配    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539428">1.10.8 异常    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539429">1.10.9 隐式转换    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539430">1.10.10 泛型    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539431">1.11 Spark Core &amp; SQL    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539432">1.11.1 Spark有几种部署方式？请分别简要论述    41</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539433">1.11.2 Spark任务使用什么进行提交，JavaEE界面还是脚本    42</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539434">1.11.3 Spark提交作业参数（重点）    42</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539435">1.11.4<br>  简述Spark的架构与作业提交流程（画图讲解，注明各个部分的作用）（重点）    43</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539436">1.11.5 如何理解Spark中的血统概念（RDD）（笔试重点）    43</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539437">1.11.6<br>  简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage又根据什么决定task个数?<br>  （笔试重点）    44</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539438">1.11.7<br>  请列举Spark的transformation算子（不少于8个），并简述功能（重点）    44</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539439">1.11.8<br>  请列举Spark的action算子（不少于6个），并简述功能（重点）    45</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539440">1.11.9 请列举会引起Shuffle过程的Spark算子，并简述功能。    45</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539441">1.11.10<br>  简述Spark的两种核心Shuffle（HashShuffle与SortShuffle）的工作流程（包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle）（重点）    45</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539442">1.11.11<br>  Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势？（重点）    47</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539443">1.11.12 Repartition和Coalesce关系与区别    48</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539444">1.11.13<br>  分别简述Spark中的缓存机制（cache和persist）与checkpoint机制，并指出两者的区别与联系    48</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539445">1.11.14<br>  简述Spark中共享变量（广播变量和累加器）的基本原理与用途。（重点）    48</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539446">1.11.15<br>  当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？    49</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539447">1.11.16<br>  如何使用Spark实现TopN的获取（描述思路或使用伪代码）（重点）    49</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539448">1.11.17<br>  京东：调优之前与调优之后性能的详细对比（例如调整map个数，map个数之前多少、之后多少，有什么提升）    49</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539449">1.11.18 简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系?<br>  （笔试重点）    49</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539450">1.11.19 append和overwrite的区别    51</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539451">1.11.20 coalesce和repartition的区别    51</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539452">1.11.21 cache缓存级别    51</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539453">1.11.22 释放缓存和缓存    51</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539454">1.11.23 Spark Shuffle默认并行度    51</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539455">1.11.24 kryo序列化    51</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539456">1.11.25 创建临时表和全局临时表    51</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539457">1.11.26 BroadCast join 广播join    51</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539458">1.11.27 控制Spark reduce缓存 调优shuffle    52</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539459">1.11.28 注册UDF函数    52</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539460">1.11.29 SparkSQL中join操作与left join操作的区别？    52</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539461">1.12 Spark Streaming    52</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539462">1.12.1 Spark Streaming第一次运行不丢失数据    52</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539463">1.12.2 Spark Streaming精准一次消费    52</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539464">1.12.3 Spark Streaming控制每秒消费数据的速度    52</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539465">1.12.4 Spark Streaming背压机制    53</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539466">1.12.5 Spark Streaming 一个stage耗时    53</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539467">1.12.6 Spark Streaming 优雅关闭    53</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539468">1.12.7 Spark Streaming 默认分区个数    53</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539469">1.12.8<br>  SparkStreaming有哪几种方式消费Kafka中的数据，它们之间的区别是什么？    53</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539470">1.12.9 简述SparkStreaming窗口函数的原理（重点）    54</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539471">1.13 数据倾斜    55</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539472">1.13.1 数据倾斜表现    55</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539473">1.13.2 数据倾斜产生原因    55</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539474">1.13.3 解决数据倾斜思路    57</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539475">1.13.4 定位导致数据倾斜代码    58</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539476">1.13.5 查看导致数据倾斜的key分布情况    60</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539477">1.13.6 Spark 数据倾斜的解决方案    60</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539478">1.13.7 Spark数据倾斜处理小结    78</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539479">1.14 Flink基础    78</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539480">1.14.1 简单介绍一下 Flink    78</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539481">1.14.2 Flink相比传统的Spark Streaming区别?    79</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539482">1.14.3 Flink的组件栈有哪些？    79</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539483">1.14.4 Flink 的运行必须依赖 Hadoop组件吗？    80</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539484">1.14.5 你们的Flink集群规模多大？    80</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539485">1.14.6 Flink的基础编程模型了解吗？    81</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539486">1.14.7 Flink集群有哪些角色？各自有什么作用？    82</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539487">1.14.8 说说 Flink 资源管理中 Task Slot 的概念    83</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539488">1.14.9 说说 Flink 的常用算子？    83</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539489">1.14.10 说说你知道的Flink分区策略？    83</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539490">1.14.11 Flink的并行度了解吗？Flink的并行度设置是怎样的？    84</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539491">1.14.12 Flink的Slot和parallelism有什么区别？    85</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539492">1.14.13 Flink有没有重启策略？说说有哪几种？    85</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539493">1.14.14 用过Flink中的分布式缓存吗？如何使用？    85</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539494">1.14.15 说说Flink中的广播变量，使用时需要注意什么？    86</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539495">1.14.16 说说Flink中的窗口？    86</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539496">1.14.17 说说Flink中的状态存储？    87</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539497">1.14.18 Flink中的时间有哪几类    88</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539498">1.14.19 Flink 中水印是什么概念，起到什么作用？    88</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539499">1.14.20 Flink Table &amp; SQL<br>  熟悉吗？TableEnvironment这个类有什么作用    88</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539500">1.14.21 Flink SQL的实现原理是什么？是如何实现 SQL<br>  解析的呢？    88</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539501">1.15 Flink中级    90</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539502">1.15.1 Flink是如何支持批流一体的？    90</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539503">1.15.2 Flink是如何做到高效的数据交换的？    90</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539504">1.15.3 Flink是如何做容错的？    90</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539505">1.15.4 Flink 分布式快照的原理是什么？    90</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539506">1.15.5 Flink是如何保证Exactly-once语义的？    91</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539507">1.15.6 Flink 的 kafka 连接器有什么特别的地方？    91</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539508">1.15.7 说说 Flink的内存管理是如何做的?    91</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539509">1.15.8 说说 Flink的序列化如何做的?    92</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539510">1.15.9 Flink中的Window出现了数据倾斜，你有什么解决办法？    92</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539511">1.15.10 Flink中在使用聚合函数 GroupBy、Distinct、KeyBy<br>  等函数时出现数据热点该如何解决？    93</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539512">1.15.11 Flink任务延迟高，想解决这个问题，你会如何入手？    93</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539513">1.15.12 Flink是如何处理反压的？    93</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539514">1.15.13 Flink的反压和Strom有哪些不同？    93</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539515">1.15.14 Operator Chains（算子链）这个概念你了解吗？    94</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539516">1.15.15 Flink什么情况下才会把Operator<br>  chain在一起形成算子链？    94</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539517">1.15.16 说说Flink1.9的新特性？    94</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539518">1.15.17 消费kafka数据的时候，如何处理脏数据？    94</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539519">1.16 Flink高级    95</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539520">1.16.1 Flink Job的提交流程    95</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539521">1.16.2 Flink所谓”三层图”结构是哪几个”图”？    95</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539522">1.16.3 JobManger在集群中扮演了什么角色？    95</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539523">1.16.4 JobManger在集群启动过程中起到什么作用？    96</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539524">1.16.5 TaskManager在集群中扮演了什么角色？    96</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539525">1.16.6 TaskManager在集群启动过程中起到什么作用？    96</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539526">1.16.7 Flink 计算资源的调度是如何实现的？    97</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539527">1.16.8 简述Flink的数据抽象及数据交换过程？    97</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539528">1.16.9 Flink 中的分布式快照机制是如何实现的？    98</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539529">1.16.10 简单说说FlinkSQL的是如何实现的？    98</a></p>
</blockquote>
<p><a href="#_Toc42539530">第2章 项目架构    99</a></p>
<blockquote>
<p>  <a href="#_Toc42539531">2.1 提高自信    99</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539532">2.2 数仓概念    100</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539533">2.3 系统数据流程设计    101</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539534">2.4 框架版本选型    101</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539535">2.5 服务器选型    102</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539536">2.6 集群规模    102</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539537">2.7 人员配置参考    103</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539538">2.7.1 整体架构    103</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539539">2.7.2 你们部门的职级等级，晋升规则    103</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539540">2.7.3 人员配置参考    103</a></p>
</blockquote>
<p><a href="#_Toc42539541">第3章 数仓分层    104</a></p>
<blockquote>
<p>  <a href="#_Toc42539542">3.1 ODS层做了哪些事？    104</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539543">3.2 DWD层做了哪些事？    104</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539544">3.2.1 数据清洗    104</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539545">3.2.2 清洗的手段    104</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539546">3.2.3 清洗掉多少数据算合理    105</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539547">3.2.4 脱敏    105</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539548">3.2.5 维度退化    105</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539549">3.2.6 压缩LZO    105</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539550">3.2.7 列式存储parquet    105</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539551">3.3 DWS层做了哪些事？    105</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539552">3.3.1 DWS层有3-5张宽表（处理100-200个指标<br>  70%以上的需求）    105</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539553">3.3.2 哪个宽表最宽？大概有多少个字段？    105</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539554">3.3.3 具体用户行为宽表字段名称    105</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539555">3.4 ADS层分析过哪些指标    106</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539556">3.4.1 分析过的指标（一分钟至少说出30个指标）    106</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539557">3.4.2 留转G复活指标    107</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539558">3.4.3 哪个商品卖的好？    108</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539559">3.5 ADS层手写指标    108</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539560">3.5.1 如何分析用户活跃？    108</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539561">3.5.2 如何分析用户新增？vivo    108</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539562">3.5.3 如何分析用户1天留存？    108</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539563">3.5.4 如何分析沉默用户？    108</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539564">3.5.5 如何分析本周回流用户？    108</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539565">3.5.6 如何分析流失用户？    108</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539566">3.5.7 如何分析最近连续3周活跃用户数？    108</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539567">3.5.8 如何分析最近七天内连续三天活跃用户数？    109</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539568">3.6 分析过最难的指标    109</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539569">3.6.1 最近连续3周活跃用户    109</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539570">3.6.2 最近7天连续3天活跃用户数    110</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539571">3.7 数据仓库建模（绝对重点）    110</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539572">3.7.1 建模工具是什么？    110</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539573">3.7.2 ODS层    110</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539574">3.7.3 DWD层    110</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539575">3.7.4 DWS层    112</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539576">3.7.5 DWT层    113</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539577">3.7.6 ADS层    114</a></p>
</blockquote>
<p><a href="#_Toc42539578">第4章 生产经验—技术    114</a></p>
<blockquote>
<p>  <a href="#_Toc42539579">4.1<br>  Linux+Shell+Hadoop+ZK+Flume+kafka+Hive+Sqoop+Azkaban那些事    114</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539580">4.2 可视化报表工具    114</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539581">4.3 集群监控工具    114</a></p>
</blockquote>
<p><a href="#_Toc42539582">第5章 生产经验—业务    114</a></p>
<blockquote>
<p>  <a href="#_Toc42539583">5.1 电商常识    114</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539584">5.1.1 SKU和SPU    114</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539585">5.1.2 订单表跟订单详情表区别？    115</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539586">5.2 埋点行为数据基本格式(基本字段)    115</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539587">5.2.1 页面    115</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539588">5.2.2 事件    116</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539589">5.2.3 曝光    117</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539590">5.2.4 启动    118</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539591">5.2.5 错误    119</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539592">5.2.6 埋点数据日志格式    119</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539593">5.3 电商业务流程    121</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539594">5.4 维度表和事实表（重点）    121</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539595">5.4.1 维度表    121</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539596">5.4.2 事实表    121</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539597">5.5 同步策略    122</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539598">5.6 关系型数据库范式理论    122</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539599">5.7 数据模型    123</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539600">5.8 拉链表    123</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539601">5.9 即席查询数据仓库    124</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539602">5.10<br>  数据仓库每天跑多少张表，大概什么时候运行，运行多久？    124</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539603">5.11 活动的话，数据量会增加多少？怎么解决？    124</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539604">5.12 并发峰值多少？大概哪个时间点？    125</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539605">5.13 100G的数据离线数仓一套跑下来大概多少时间?    125</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539606">5.14 数仓中使用的哪种文件存储格式    125</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539607">5.15 哪张表最费时间，有没有优化    125</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539608">5.16 用什么工具做权限管理    125</a></p>
</blockquote>
<p><a href="#_Toc42539609">第6章 生产经验–测试上线相关    125</a></p>
<blockquote>
<p>  <a href="#_Toc42539610">6.1 测试相关    125</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539611">6.1.1 公司有多少台测试服务器？    125</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539612">6.1.2 测试环境什么样？    125</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539613">6.1.3 测试数据哪来的？    125</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539614">6.1.4 如何保证写的sql正确性    125</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539615">6.1.5 测试之后如何上线？    126</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539616">6.2 项目实际工作流程    126</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539617">6.3 项目中实现一个需求大概多长时间    127</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539618">6.4<br>  项目在3年内迭代次数，每一个项目具体是如何迭代的。公司版本迭代多久一次，迭代到哪个版本    127</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539619">6.5 项目开发中每天做什么事    128</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539620">6.6 实时项目数据计算    128</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539621">6.6.1 跑实时任务，怎么分配内存和CPU资源    128</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539622">6.6.2 跑实时任务，每天数据量多少？    128</a></p>
</blockquote>
<p><a href="#_Toc42539623">第7章 生产经验—热点问题    128</a></p>
<blockquote>
<p>  <a href="#_Toc42539624">7.1 元数据管理（Atlas血缘系统）    128</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539625">7.2 数据质量监控（Griffin）    129</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539626">7.2.1 为什么要做数据质量监控（2019年下半年）    129</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539627">7.2.2 建设方法    130</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539628">7.2.3 监控指标    130</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539629">7.3 数据治理    131</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539630">7.4 数据中台    132</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539631">7.4.1 什么是中台？    132</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539632">7.4.2 传统项目痛点    133</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539633">7.4.3 各家中台    133</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539634">7.4.4 中台具体划分    134</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539635">7.4.5 中台使用场景    135</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539636">7.5 数据湖    136</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539637">7.6 埋点    136</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539638">7.7 电商运营经验    137</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539639">7.7.1 电商8类基本指标    137</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539640">7.7.2 直播指标    141</a></p>
</blockquote>
<p><a href="#_Toc42539641">第8章 手写代码    145</a></p>
<blockquote>
<p>  <a href="#_Toc42539642">8.1 基本算法    145</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539643">8.1.1 冒泡排序    145</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539644">8.1.2 二分查找    147</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539645">8.1.3 快排    149</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539646">8.1.4 归并    150</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539647">8.1.5 二叉树之Scala实现    151</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539648">8.2 开发代码    155</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539649">8.2.1 手写Spark-WordCount    155</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539650">8.3 手写HQL    156</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539651">8.3.1 手写HQL 第1题    156</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539652">8.3.2 手写HQL 第2题    157</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539653">8.3.3 手写HQL 第3题    159</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539654">8.3.4 手写HQL 第4题    160</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539655">8.3.5 手写HQL 第5题    161</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539656">8.3.6 手写HQL 第6题    165</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539657">8.3.7 手写HQL 第7题    166</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539658">8.3.8 手写SQL 第8题    167</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539659">8.3.9 手写HQL 第9题    168</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539660">8.3.10 手写HQL 第10题    170</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539661">8.3.11 手写HQL 第11题    173</a></p>
</blockquote>
<p><a href="#_Toc42539662">第9章 JavaSE    178</a></p>
<blockquote>
<p>  <a href="#_Toc42539663">9.1 HashMap底层源码，数据结构    178</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539664">9.2 Java自带哪几种线程池？    181</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539665">9.3 HashMap和HashTable区别    182</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539666">9.4 TreeSet和HashSet区别    183</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539667">9.5 String buffer和String build区别    183</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539668">9.6 Final、Finally、Finalize    183</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539669">9.7 ==和Equals区别    184</a></p>
</blockquote>
<p><a href="#_Toc42539670">第10章 Redis    184</a></p>
<blockquote>
<p>  <a href="#_Toc42539671">10.1 缓存穿透、缓存雪崩、缓存击穿    184</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539672">10.2 哨兵模式    185</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539673">10.3 数据类型    185</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539674">10.4 持久化    185</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539675">11.5 悲观锁    186</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539676">11.6 乐观锁    186</a></p>
</blockquote>
<p><a href="#_Toc42539677">第11章 MySql    186</a></p>
<blockquote>
<p>  <a href="#_Toc42539678">11.1 MyISAM与InnoDB的区别    186</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539679">11.2 索引优化    186</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539680">11.3 b-tree和b+tree的区别    187</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539681">11.4 redis是单线程的，为什么那么快    187</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539682">11.5 MySQL的事务    187</a></p>
</blockquote>
<p><a href="#_Toc42539683">第12章 JVM    189</a></p>
<blockquote>
<p>  <a href="#_Toc42539684">12.1 JVM内存分哪几个区，每个区的作用是什么?    189</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539685">12.2 Java类加载过程?    190</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539686">12.3 java中垃圾收集的方法有哪些?    191</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539687">12.4 如何判断一个对象是否存活?(或者GC对象的判定方法)    191</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539688">12.5 什么是类加载器，类加载器有哪些?    192</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539689">12.6 简述Java内存分配与回收策略以及Minor GC和Major GC（full<br>  GC）    192</a></p>
</blockquote>
<p><a href="#_Toc42539690">第13章 JUC    193</a></p>
<blockquote>
<p>  <a href="#_Toc42539691">13.1 Synchronized与Lock的区别    193</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539692">13.2 Runnable和Callable的区别    193</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539693">13.3 什么是分布式锁    193</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539694">13.4 什么是分布式事务    193</a></p>
</blockquote>
<p><a href="#_Toc42539695">第14章 面试说明    193</a></p>
<blockquote>
<p>  <a href="#_Toc42539696">14.1 面试过程最关键的是什么？    193</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539697">14.2 面试时该怎么说？    193</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539698">14.3 面试技巧    194</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539699">14.3.1 六个常见问题    194</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539700">14.3.2 两个注意事项    195</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539701">14.3.3 自我介绍（控制在4分半以内，不超过5分钟）    195</a></p>
</blockquote>
<p><a href="#_Toc42539702">第15章 LeetCode题目精选    195</a></p>
<blockquote>
<p>  <a href="#_Toc42539703">15.1 两数之和    195</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539704">15.1.1 问题描述    195</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539705">15.1.2 参考答案    196</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539706">15.2 爬楼梯    196</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539707">15.2.1 问题描述    196</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539708">15.2.2 参考答案    197</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539709">15.3 翻转二叉树    197</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539710">15.3.1 问题描述    198</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539711">15.3.2 参考答案    198</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539712">15.4 反转链表    199</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539713">15.4.1 问题描述    199</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539714">15.4.2 参考答案    199</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539715">15.5 LRU缓存机制    200</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539716">15.5.1 问题描述    200</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539717">15.5.2 参考答案    200</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539718">15.6 最长回文子串    202</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539719">15.6.1 问题描述    202</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539720">15.6.2 参考答案    202</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539721">15.7 有效的括号    203</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539722">15.7.1 问题描述    203</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539723">15.7.2 参考答案    204</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539724">15.8 数组中的第K个最大元素    206</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539725">15.8.1 问题描述    206</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539726">15.8.2 参考答案    207</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539727">15.9 实现 Trie (前缀树)    209</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539728">15.9.1 问题描述    209</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539729">15.9.2 参考答案    209</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539730">15.10 编辑距离    211</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539731">15.10.1 问题描述    211</a></p>
</blockquote>
<blockquote>
<p>  <a href="#_Toc42539732">15.10.2 参考答案    212</a></p>
</blockquote>
<h1 id="项目涉及技术"><a href="#项目涉及技术" class="headerlink" title="项目涉及技术"></a>项目涉及技术</h1><h2 id="1-1-Linux-amp-Shell"><a href="#1-1-Linux-amp-Shell" class="headerlink" title="1.1 Linux&amp;Shell"></a>1.1 Linux&amp;Shell</h2><h3 id="1-1-1-Linux常用高级命令"><a href="#1-1-1-Linux常用高级命令" class="headerlink" title="1.1.1 Linux常用高级命令"></a>1.1.1 Linux常用高级命令</h3><table>
<thead>
<tr>
<th>序号</th>
<th>命令</th>
<th>命令解释</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>top</td>
<td>查看内存</td>
</tr>
<tr>
<td>2</td>
<td>df -h</td>
<td>查看磁盘存储情况</td>
</tr>
<tr>
<td>3</td>
<td>iotop</td>
<td>查看磁盘IO读写(yum install iotop安装）</td>
</tr>
<tr>
<td>4</td>
<td>iotop -o</td>
<td>直接查看比较高的磁盘读写程序</td>
</tr>
<tr>
<td>5</td>
<td>netstat -tunlp | grep 端口号</td>
<td>查看端口占用情况</td>
</tr>
<tr>
<td>6</td>
<td>uptime</td>
<td>查看报告系统运行时长及平均负载</td>
</tr>
<tr>
<td>7</td>
<td>ps -aux</td>
<td>查看进程</td>
</tr>
</tbody></table>
<h3 id="1-1-2-Shell常用工具及写过的脚本"><a href="#1-1-2-Shell常用工具及写过的脚本" class="headerlink" title="1.1.2 Shell常用工具及写过的脚本"></a>1.1.2 Shell常用工具及写过的脚本</h3><p>1）awk、sed、cut、sort</p>
<p>2）用Shell写过哪些脚本</p>
<p>（1）集群启动，分发脚本</p>
<p>（2）数仓与mysql的导入导出</p>
<p>（3）数仓层级内部的导入</p>
<h3 id="1-1-3-Shell中提交了一个脚本，进程号已经不知道了，但是需要kill掉这个进程，怎么操作"><a href="#1-1-3-Shell中提交了一个脚本，进程号已经不知道了，但是需要kill掉这个进程，怎么操作" class="headerlink" title="1.1.3 Shell中提交了一个脚本，进程号已经不知道了，但是需要kill掉这个进程，怎么操作?"></a>1.1.3 Shell中提交了一个脚本，进程号已经不知道了，但是需要kill掉这个进程，怎么操作?</h3><p>ssh $i “ps -ef | grep file-flume-kafka | grep -v grep |awk ‘{print \$2}’<br>| xargs kill”</p>
<h3 id="1-1-4-Shell中单引号和双引号区别"><a href="#1-1-4-Shell中单引号和双引号区别" class="headerlink" title="1.1.4 Shell中单引号和双引号区别"></a>1.1.4 Shell中单引号和双引号区别</h3><p>1）在/home/atguigu/bin创建一个test.sh文件</p>
<p>[atguigu@hadoop102 bin]$ vim test.sh</p>
<p>在文件中添加如下内容</p>
<p>#!/bin/bash</p>
<p>do_date=$1</p>
<p>echo ‘$do_date’</p>
<p>echo “$do_date”</p>
<p>echo “‘$do_date’”</p>
<p>echo ‘“$do_date”‘</p>
<p>echo `date`</p>
<p>2）查看执行结果</p>
<p>[atguigu@hadoop102 bin]$ test.sh 2019-02-10</p>
<p>$do_date</p>
<p>2019-02-10</p>
<p>‘2019-02-10’</p>
<p>“$do_date”</p>
<p>2019年 05月 02日 星期四 21:02:08 CST</p>
<p>3）总结：</p>
<p>（1）单引号不取变量值</p>
<p>（2）双引号取变量值</p>
<p>（3）反引号`，执行引号中命令</p>
<p>（4）双引号内部嵌套单引号，取出变量值</p>
<p>（5）单引号内部嵌套双引号，不取出变量值</p>
<h2 id="1-2-Hadoop"><a href="#1-2-Hadoop" class="headerlink" title="1.2 Hadoop"></a>1.2 Hadoop</h2><h3 id="1-2-1-Hadoop常用端口号"><a href="#1-2-1-Hadoop常用端口号" class="headerlink" title="1.2.1 Hadoop常用端口号"></a>1.2.1 Hadoop常用端口号</h3><table>
<thead>
<tr>
<th></th>
<th>hadoop2.x</th>
<th>Hadoop3.x</th>
</tr>
</thead>
<tbody><tr>
<td>访问HDFS端口</td>
<td>50070</td>
<td>9870</td>
</tr>
<tr>
<td>访问MR执行情况端口</td>
<td>8088</td>
<td>8088</td>
</tr>
<tr>
<td>历史服务器</td>
<td>19888</td>
<td>19888</td>
</tr>
<tr>
<td>客户端访问集群端口</td>
<td>9000</td>
<td>8020</td>
</tr>
</tbody></table>
<h3 id="1-2-2-Hadoop配置文件以及简单的Hadoop集群搭建"><a href="#1-2-2-Hadoop配置文件以及简单的Hadoop集群搭建" class="headerlink" title="1.2.2 Hadoop配置文件以及简单的Hadoop集群搭建"></a>1.2.2 Hadoop配置文件以及简单的Hadoop集群搭建</h3><p>（1）配置文件：</p>
<blockquote>
<p>  Hadoop2.x core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml<br>  slaves</p>
</blockquote>
<blockquote>
<p>  Hadoop3.x core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml<br>  workers</p>
</blockquote>
<p>（2）简单的集群搭建过程：</p>
<p>JDK安装</p>
<blockquote>
<p>  配置SSH免密登录</p>
</blockquote>
<blockquote>
<p>  配置hadoop核心文件:</p>
</blockquote>
<blockquote>
<p>  格式化namenode</p>
</blockquote>
<h3 id="1-2-3-HDFS读流程和写流程"><a href="#1-2-3-HDFS读流程和写流程" class="headerlink" title="1.2.3 HDFS读流程和写流程"></a>1.2.3 HDFS读流程和写流程</h3><h3 id="1-2-4-HDFS小文件处理"><a href="#1-2-4-HDFS小文件处理" class="headerlink" title="1.2.4 HDFS小文件处理"></a>1.2.4 HDFS小文件处理</h3><p>1）会有什么影响</p>
<p>（1）1个文件块，占用namenode多大内存150字节</p>
<blockquote>
<p>  1亿个小文件*150字节</p>
</blockquote>
<blockquote>
<p>  1 个文件块 * 150字节</p>
</blockquote>
<blockquote>
<p>  128G能存储多少文件块？ 128 * 1024*1024*1024byte/150字节 = 9亿文件块</p>
</blockquote>
<p>2）怎么解决</p>
<p>（1）采用har归档方式，将小文件归档</p>
<p>（2）采用CombineTextInputFormat</p>
<p>（3）有小文件场景开启JVM重用；如果没有小文件，不要开启JVM重用，因为会一直占用使用到的task卡槽，直到任务完成才释放。</p>
<p>JVM重用可以使得JVM实例在同一个job中重新使用N次，N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间</p>
<blockquote>
<p>  &lt;property&gt;</p>
</blockquote>
<blockquote>
<p>  &lt;name&gt;mapreduce.job.jvm.numtasks&lt;/name&gt;</p>
</blockquote>
<blockquote>
<p>  &lt;value&gt;10&lt;/value&gt;</p>
</blockquote>
<blockquote>
<p>  &lt;description&gt;How many tasks to run per jvm,if set to -1 ,there is no<br>  limit&lt;/description&gt;</p>
</blockquote>
<blockquote>
<p>  &lt;/property&gt;</p>
</blockquote>
<h3 id="1-2-5-Shuffle及优化"><a href="#1-2-5-Shuffle及优化" class="headerlink" title="1.2.5 Shuffle及优化"></a>1.2.5 Shuffle及优化</h3><p><strong>1、Shuffle过程</strong></p>
<p><strong>2、优化</strong></p>
<p>1）Map阶段</p>
<p>（1）增大环形缓冲区大小。由100m扩大到200m</p>
<p>（2）增大环形缓冲区溢写的比例。由80%扩大到90%</p>
<p>（3）减少对溢写文件的merge次数。（10个文件，一次20个merge）</p>
<p>（4）不影响实际业务的前提下，采用Combiner提前合并，减少 I/O。</p>
<p>2）Reduce阶段</p>
<p>（1）合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致<br>Map、Reduce任务间竞争资源，造成处理超时等错误。</p>
<p>（2）设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。</p>
<p>（3）规避使用Reduce，因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。</p>
<p>（4）增加每个Reduce去Map中拿数据的并行数</p>
<p>（5）集群性能可以的前提下，增大Reduce端存储数据内存的大小。</p>
<p>3）IO传输</p>
<p>采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZOP压缩编码器。</p>
<p>压缩：</p>
<p>（1）map输入端主要考虑数据量大小和切片，支持切片的有Bzip2、LZO。注意：LZO要想支持切片必须创建索引；</p>
<p>（2）map输出端主要考虑速度，速度快的snappy、LZO；</p>
<p>（3）reduce输出端主要看具体需求，例如作为下一个mr输入需要考虑切片，永久保存考虑压缩率比较大的gzip。</p>
<p>4）整体</p>
<p>（1）NodeManager默认内存8G，需要根据服务器实际配置灵活调整，例如128G内存，配置为100G内存左右，yarn.nodemanager.resource.memory-mb。</p>
<p>（2）单任务默认内存8G，需要根据该任务的数据量灵活调整，例如128m数据，配置1G内存，yarn.scheduler.maximum-allocation-mb。</p>
<p>（3）mapreduce.map.memory.mb<br>：控制分配给MapTask内存上限，如果超过会kill掉进程（报：Container is running<br>beyond physical memory limits. Current usage:565MB of512MB physical memory<br>used；Killing<br>Container）。默认内存大小为1G，如果数据量是128m，正常不需要调整内存；如果数据量大于128m，可以增加MapTask内存，最大可以增加到4-5g。</p>
<p>（4）mapreduce.reduce.memory.mb：控制分配给ReduceTask内存上限。默认内存大小为1G，如果数据量是128m，正常不需要调整内存；如果数据量大于128m，可以增加ReduceTask内存大小为4-5g。</p>
<p>（5）mapreduce.map.java.opts：控制MapTask堆内存大小。（如果内存不够，报：java.lang.OutOfMemoryError）</p>
<p>（6）mapreduce.reduce.java.opts：控制ReduceTask堆内存大小。（如果内存不够，报：java.lang.OutOfMemoryError）</p>
<p>（7）可以增加MapTask的CPU核数，增加ReduceTask的CPU核数</p>
<p>（8）增加每个Container的CPU核数和内存大小</p>
<p>（9）在hdfs-site.xml文件中配置多目录（多磁盘）</p>
<p>（10）NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。dfs.namenode.handler.count=</p>
<p><img src="media/04d6f8637383ea0dc5752b70e2f93622.png"></p>
<p>，，比如集群规模为8台时，此参数设置为41。可通过简单的python代码计算该值，代码如下。</p>
<blockquote>
<p>  [atguigu@hadoop102 ~]$ python</p>
</blockquote>
<blockquote>
<p>  Python 2.7.5 (default, Apr 11 2018, 07:36:10)</p>
</blockquote>
<blockquote>
<p>  [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2</p>
</blockquote>
<blockquote>
<p>  Type “help”, “copyright”, “credits” or “license” for more information.</p>
</blockquote>
<blockquote>
<p>  &gt;&gt;&gt; import math</p>
</blockquote>
<blockquote>
<p>  &gt;&gt;&gt; print int(20*math.log(8))</p>
</blockquote>
<blockquote>
<p>  41</p>
</blockquote>
<blockquote>
<p>  &gt;&gt;&gt; quit()</p>
</blockquote>
<h3 id="1-2-6-Yarn工作机制"><a href="#1-2-6-Yarn工作机制" class="headerlink" title="1.2.6 Yarn工作机制"></a>1.2.6 Yarn工作机制</h3><h3 id="1-2-7-Yarn调度器"><a href="#1-2-7-Yarn调度器" class="headerlink" title="1.2.7 Yarn调度器"></a>1.2.7 Yarn调度器</h3><p>1）Hadoop调度器重要分为三类：</p>
<blockquote>
<p>  FIFO 、Capacity Scheduler（容量调度器）和Fair Sceduler（公平调度器）。</p>
</blockquote>
<blockquote>
<p>  Apache默认的资源调度器是容量调度器；</p>
</blockquote>
<blockquote>
<p>  CDH默认的资源调度器是公平调度器。</p>
</blockquote>
<p>2）区别：</p>
<p>FIFO调度器：支持单队列 、先进先出 生产环境不会用。</p>
<p>容量调度器：支持多队列，保证先进入的任务优先执行。</p>
<p>公平调度器：支持多队列，保证每个任务公平享有队列资源。</p>
<p>3）在生产环境下怎么选择？</p>
<p>大厂：如果对并发度要求比较高，选择公平，要求服务器性能必须OK；</p>
<p>中小公司，集群服务器资源不太充裕选择容量。</p>
<p>4）在生产环境怎么创建队列？</p>
<p>（1）调度器默认就1个default队列，不能满足生产要求。</p>
<p>（2）按照框架：hive /spark/ flink<br>每个框架的任务放入指定的队列（企业用的不是特别多）</p>
<p>（3）按照业务模块：登录注册、购物车、下单、业务部门1、业务部门2</p>
<p>5）创建多队列的好处？</p>
<p>（1）因为担心员工不小心，写递归死循环代码，把所有资源全部耗尽。</p>
<p>（2）实现任务的降级使用，特殊时期保证重要的任务队列资源充足。</p>
<p>业务部门1（重要）=》业务部门2（比较重要）=》下单（一般）=》购物车（一般）=》登录注册（次要）</p>
<h3 id="1-2-8-项目经验之基准测试"><a href="#1-2-8-项目经验之基准测试" class="headerlink" title="1.2.8 项目经验之基准测试"></a>1.2.8 项目经验之基准测试</h3><p>搭建完Hadoop集群后需要对HDFS读写性能和MR计算能力测试。测试jar包在hadoop的share文件夹下。</p>
<h3 id="1-2-9-Hadoop宕机"><a href="#1-2-9-Hadoop宕机" class="headerlink" title="1.2.9 Hadoop宕机"></a>1.2.9 Hadoop宕机</h3><p>1）如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是8192MB）</p>
<p>2）如果写入文件过快造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。例如，可以调整Flume每批次拉取数据量的大小参数batchsize。</p>
<h3 id="1-2-10-Hadoop解决数据倾斜方法"><a href="#1-2-10-Hadoop解决数据倾斜方法" class="headerlink" title="1.2.10 Hadoop解决数据倾斜方法"></a>1.2.10 Hadoop解决数据倾斜方法</h3><p><strong>1）提前在map进行combine，减少传输的数据量</strong></p>
<p>在Mapper加上combiner相当于提前进行reduce，即把一个Mapper中的相同key进行了聚合，减少shuffle过程中传输的数据量，以及Reducer端的计算量。</p>
<p>如果导致数据倾斜的key大量分布在不同的mapper的时候，这种方法就不是很有效了。</p>
<p><strong>2）导致数据倾斜的key 大量分布在不同的mapper</strong></p>
<p>（1）局部聚合加全局聚合。</p>
<p>第一次在map阶段对那些导致了数据倾斜的key 加上1到n的随机前缀，这样本来相同的key<br>也会被分到多个Reducer中进行局部聚合，数量就会大大降低。</p>
<p>第二次mapreduce，去掉key的随机前缀，进行全局聚合。</p>
<p>思想：二次mr，第一次将key随机散列到不同reducer进行处理达到负载均衡目的。第二次再根据去掉key的随机前缀，按原key进行reduce处理。</p>
<p>这个方法进行两次mapreduce，性能稍差。</p>
<blockquote>
<p>  （2）增加Reducer，提升并行度<br>  JobConf.setNumReduceTasks(int)</p>
</blockquote>
<blockquote>
<p>  （3）实现自定义分区</p>
</blockquote>
<blockquote>
<p>  根据数据分布情况，自定义散列函数，将key均匀分配到不同Reducer</p>
</blockquote>
<h3 id="1-2-11-集群资源分配参数（项目中遇到的问题）"><a href="#1-2-11-集群资源分配参数（项目中遇到的问题）" class="headerlink" title="1.2.11 集群资源分配参数（项目中遇到的问题）"></a>1.2.11 集群资源分配参数（项目中遇到的问题）</h3><p>集群有30台机器，跑MR任务的时候发现5个map任务全都分配到了同一台机器上，这个可能是由于什么原因导致的吗？</p>
<p>解决方案：yarn.scheduler.fair.assignmultiple 这个参数，默认是关闭的。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/leone911/article/details/51605172">https://blog.csdn.net/leone911/article/details/51605172</a></p>
<h2 id="1-3-Zookeeper"><a href="#1-3-Zookeeper" class="headerlink" title="1.3 Zookeeper"></a>1.3 Zookeeper</h2><h3 id="1-3-1-选举机制"><a href="#1-3-1-选举机制" class="headerlink" title="1.3.1 选举机制"></a>1.3.1 选举机制</h3><p>半数机制：2n+1，安装奇数台</p>
<blockquote>
<p>  10台服务器：3台</p>
</blockquote>
<blockquote>
<p>  20台服务器：5台</p>
</blockquote>
<blockquote>
<p>  100台服务器：11台</p>
</blockquote>
<blockquote>
<p>  台数多，好处：提高可靠性；坏处：影响通信延时</p>
</blockquote>
<h3 id="1-3-2-常用命令"><a href="#1-3-2-常用命令" class="headerlink" title="1.3.2 常用命令"></a>1.3.2 常用命令</h3><p>ls、get、create</p>
<h3 id="1-3-3-Paxos算法（扩展）"><a href="#1-3-3-Paxos算法（扩展）" class="headerlink" title="1.3.3 Paxos算法（扩展）"></a>1.3.3 Paxos算法（扩展）</h3><blockquote>
<p>  注意：暂时先不用看。如果后期准备面今日头条，需要认真准备，其他公司几乎都不问。</p>
</blockquote>
<blockquote>
<p>  Paxos算法一种基于消息传递且具有高度容错特性的一致性算法。</p>
</blockquote>
<p>分布式系统中的节点通信存在两种模型：共享内存（Shared<br>memory）和消息传递（Messages<br>passing）。基于消息传递通信模型的分布式系统，不可避免的会发生以下错误：进程可能会慢、被杀死或者重启，消息可能会延迟、丢失、重复，在基础Paxos场景中，先不考虑可能出现消息篡改即拜占庭错误的情况。Paxos算法解决的问题是在一个可能发生上述异常的分布式系统中如何就某个值达成一致，保证不论发生以上任何异常，都不会破坏决议的一致性。</p>
<h3 id="1-3-4-讲一讲什么是CAP法则？Zookeeper符合了这个法则的哪两个？（扩展）"><a href="#1-3-4-讲一讲什么是CAP法则？Zookeeper符合了这个法则的哪两个？（扩展）" class="headerlink" title="1.3.4 讲一讲什么是CAP法则？Zookeeper符合了这个法则的哪两个？（扩展）"></a>1.3.4 讲一讲什么是CAP法则？Zookeeper符合了这个法则的哪两个？（扩展）</h3><p>CAP法则：强一致性、高可用性、分区容错性；</p>
<p>Zookeeper符合强一致性、高可用性！</p>
<h2 id="1-4-Flume"><a href="#1-4-Flume" class="headerlink" title="1.4 Flume"></a>1.4 Flume</h2><h3 id="1-4-1-Flume组成，Put事务，Take事务"><a href="#1-4-1-Flume组成，Put事务，Take事务" class="headerlink" title="1.4.1 Flume组成，Put事务，Take事务"></a>1.4.1 Flume组成，Put事务，Take事务</h3><p>1）taildir source</p>
<p>（1）断点续传、多目录</p>
<p>（2）哪个flume版本产生的？Apache1.7、CDH1.6</p>
<p>（3）没有断点续传功能时怎么做的？ 自定义</p>
<p>（4）taildir挂了怎么办？</p>
<p>不会丢数：断点续传</p>
<p>重复数据：</p>
<p>（5）怎么处理重复数据？</p>
<p>不处理：生产环境通常不处理，因为会影响传输效率</p>
<p>处理</p>
<p>自身：在taildirsource里面增加自定义事务</p>
<p>找兄弟：下一级处理（hive dwd sparkstreaming<br>flink布隆）、去重手段（groupby、开窗取窗口第一条、redis）</p>
<p>（6）taildir source 是否支持递归遍历文件夹读取文件？</p>
<p>不支持。 自定义 递归遍历文件夹 +读取文件</p>
<p>2）file channel /memory channel/kafka channel</p>
<p>（1）file channel</p>
<p>数据存储于磁盘，优势：可靠性高；劣势：传输速度低</p>
<p>默认容量：100万event</p>
<p>注意：FileChannel可以通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。</p>
<p>（2）memory channel</p>
<p>数据存储于内存，优势：传输速度快；劣势：可靠性差</p>
<p>默认容量：100个event</p>
<p>（3）kafka channel</p>
<p>数据存储于Kafka，基于磁盘；</p>
<p>优势：可靠性高；</p>
<p>传输速度快 kafka channel》memory channel+kafka sink 原因省去了sink阶段</p>
<p>（4）kafka channel哪个版本产生的？</p>
<p>flume1.6 版本产生=》并没有火；因为有bug</p>
<p>topic-start 数据内容</p>
<p>topic-event 数据内容 ture 和false 很遗憾，都不起作用。</p>
<p>增加了额外清洗的工作量。</p>
<p>flume1.7解决了这个问题，开始火了。</p>
<p>（5）生产环境如何选择</p>
<p>如果下一级是kafka，优先选择kafka channel</p>
<p>如果是金融、对钱要求准确的公司，选择file channel</p>
<p>如果就是普通的日志，通常可以选择memory channel</p>
<p>每天丢几百万数据 pb级 亿万富翁，掉1块钱会捡？</p>
<p>3）HDFS sink</p>
<p>（1）时间(1小时-2小时) or 大小128m、event个数（0禁止）</p>
<p>具体参数：hdfs.rollInterval=3600，hdfs.rollSize=134217728，hdfs.rollCount =0</p>
<p>4）事务</p>
<p>Source到Channel是Put事务</p>
<p>Channel到Sink是Take事务</p>
<h3 id="1-4-2-Flume拦截器"><a href="#1-4-2-Flume拦截器" class="headerlink" title="1.4.2 Flume拦截器"></a>1.4.2 Flume拦截器</h3><p>1）拦截器注意事项</p>
<p>项目中自定义了：ETL拦截器。</p>
<p>采用两个拦截器的优缺点：优点，模块化开发和可移植性；缺点，性能会低一些</p>
<p>2）自定义拦截器步骤</p>
<p>（1）实现 Interceptor</p>
<p>（2）重写四个方法</p>
<ul>
<li><p>  initialize 初始化</p>
</li>
<li><p>  public Event intercept(Event event) 处理单个Event</p>
</li>
<li><p>public List&lt;Event&gt; intercept(List&lt;Event&gt; events)<br>  处理多个Event，在这个方法中调用Event intercept(Event event)</p>
</li>
<li><p>  close 方法</p>
</li>
</ul>
<p>（3）静态内部类，实现Interceptor.Builder</p>
<p>3）拦截器可以不用吗？</p>
<p>可以不用；需要在下一级hive的dwd层和sparksteaming里面处理</p>
<p>优势：只处理一次，轻度处理；劣势：影响性能，不适合做实时推荐这种对实时要求比较高的场景。</p>
<h3 id="1-4-3-Flume-Channel选择器"><a href="#1-4-3-Flume-Channel选择器" class="headerlink" title="1.4.3 Flume Channel选择器"></a>1.4.3 Flume Channel选择器</h3><h3 id="1-4-4-Flume监控器"><a href="#1-4-4-Flume监控器" class="headerlink" title="1.4.4 Flume监控器"></a>1.4.4 Flume监控器</h3><p>1）采用Ganglia监控器，监控到flume尝试提交的次数远远大于最终成功的次数，说明flume运行比较差。</p>
<p>2）解决办法？</p>
<p>（1）自身：增加内存flume-env.sh 4-6g</p>
<p>-Xmx与-Xms最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。</p>
<p>（2）找朋友：增加服务器台数</p>
<p>搞活动 618 =》增加服务器=》用完在退出</p>
<p>日志服务器配置：8-16g内存、磁盘8T</p>
<h3 id="1-4-5-Flume采集数据会丢失吗-（防止数据丢失的机制）"><a href="#1-4-5-Flume采集数据会丢失吗-（防止数据丢失的机制）" class="headerlink" title="1.4.5 Flume采集数据会丢失吗?（防止数据丢失的机制）"></a>1.4.5 Flume采集数据会丢失吗?（防止数据丢失的机制）</h3><p>如果是FileChannel不会，Channel存储可以存储在File中，数据传输自身有事务。</p>
<p>如果是MemoryChannel有可能丢。</p>
<h2 id="1-5-Kafka"><a href="#1-5-Kafka" class="headerlink" title="1.5 Kafka"></a>1.5 Kafka</h2><h3 id="1-5-1-Kafka架构"><a href="#1-5-1-Kafka架构" class="headerlink" title="1.5.1 Kafka架构"></a>1.5.1 Kafka架构</h3><p>生产者、Broker、消费者、ZK；</p>
<p>注意：Zookeeper中保存Broker id和消费者offsets等信息，但是没有生产者信息。</p>
<p><img src="media/4c9a71cc86e2271661cd8013ad01e0d8.png"></p>
<h3 id="1-5-2-Kafka的机器数量"><a href="#1-5-2-Kafka的机器数量" class="headerlink" title="1.5.2 Kafka的机器数量"></a>1.5.2 Kafka的机器数量</h3><p>Kafka机器数量=2*（峰值生产速度*副本数/100）+ 1</p>
<h3 id="1-5-3-副本数设定"><a href="#1-5-3-副本数设定" class="headerlink" title="1.5.3 副本数设定"></a>1.5.3 副本数设定</h3><p>一般我们设置成2个或3个，很多企业设置为2个。</p>
<p>副本的优势：提高可靠性；副本劣势：增加了网络IO传输</p>
<h3 id="1-5-4-Kafka压测"><a href="#1-5-4-Kafka压测" class="headerlink" title="1.5.4 Kafka压测"></a>1.5.4 Kafka压测</h3><p>Kafka官方自带压力测试脚本（kafka-consumer-perf-test.sh、kafka-producer-perf-test.sh）。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。</p>
<h3 id="1-5-5-Kafka日志保存时间"><a href="#1-5-5-Kafka日志保存时间" class="headerlink" title="1.5.5 Kafka日志保存时间"></a>1.5.5 Kafka日志保存时间</h3><p>默认保存7天；生产环境建议3天</p>
<h3 id="1-5-6-Kafka中数据量计算"><a href="#1-5-6-Kafka中数据量计算" class="headerlink" title="1.5.6 Kafka中数据量计算"></a>1.5.6 Kafka中数据量计算</h3><p>每天总数据量100g，每天产生1亿条日志， 10000万/24/60/60=1150条/每秒钟</p>
<p>平均每秒钟：1150条</p>
<p>低谷每秒钟：50条</p>
<p>高峰每秒钟：1150条*（2-20倍）=2300条-23000条</p>
<p>每条日志大小：0.5k-2k（取1k）</p>
<p>每秒多少数据量：2.0M-20MB</p>
<h3 id="1-5-7-Kafka的硬盘大小"><a href="#1-5-7-Kafka的硬盘大小" class="headerlink" title="1.5.7 Kafka的硬盘大小"></a>1.5.7 Kafka的硬盘大小</h3><p>每天的数据量100g*2个副本*3天/70%</p>
<h3 id="1-5-8-Kafka监控"><a href="#1-5-8-Kafka监控" class="headerlink" title="1.5.8 Kafka监控"></a>1.5.8 Kafka监控</h3><p>公司自己开发的监控器；</p>
<p>开源的监控器：KafkaManager、KafkaMonitor、KafkaEagle</p>
<h3 id="1-5-9-Kakfa分区数"><a href="#1-5-9-Kakfa分区数" class="headerlink" title="1.5.9 Kakfa分区数"></a>1.5.9 Kakfa分区数</h3><p>1）创建一个只有1个分区的topic</p>
<p>2）测试这个topic的producer吞吐量和consumer吞吐量。</p>
<p>3）假设他们的值分别是Tp和Tc，单位可以是MB/s。</p>
<p>4）然后假设总的目标吞吐量是Tt，那么分区数=Tt / min（Tp，Tc）</p>
<p>例如：producer吞吐量=20m/s；consumer吞吐量=50m/s，期望吞吐量100m/s；</p>
<p>分区数=100 / 20 =5分区</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42641909/article/details/89294698">https://blog.csdn.net/weixin_42641909/article/details/89294698</a></p>
<p>分区数一般设置为：3-10个</p>
<h3 id="1-5-10-多少个Topic"><a href="#1-5-10-多少个Topic" class="headerlink" title="1.5.10 多少个Topic"></a>1.5.10 多少个Topic</h3><p>通常情况：多少个日志类型就多少个Topic。也有对日志类型进行合并的。</p>
<h3 id="1-5-11-Kafka的ISR副本同步队列"><a href="#1-5-11-Kafka的ISR副本同步队列" class="headerlink" title="1.5.11 Kafka的ISR副本同步队列"></a>1.5.11 Kafka的ISR副本同步队列</h3><p>ISR（In-Sync<br>Replicas），副本同步队列。ISR中包括Leader和Follower。如果Leader进程挂掉，会在ISR队列中选择一个服务作为新的Leader。有replica.lag.max.messages（延迟条数）和replica.lag.time.max.ms（延迟时间）两个参数决定一台服务是否可以加入ISR副本队列，在0.10版本移除了replica.lag.max.messages参数，防止服务频繁的进去队列。</p>
<p>任意一个维度超过阈值都会把Follower剔除出ISR，存入OSR（Outof-Sync<br>Replicas）列表，新加入的Follower也会先存放在OSR中。</p>
<h3 id="1-5-12-Kafka分区分配策略"><a href="#1-5-12-Kafka分区分配策略" class="headerlink" title="1.5.12 Kafka分区分配策略"></a>1.5.12 Kafka分区分配策略</h3><p>在 Kafka内部存在两种默认的分区分配策略：Range和 RoundRobin。</p>
<p>Range是默认策略。Range是对每个Topic而言的（即一个Topic一个Topic分），首先对同一个Topic里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后用Partitions分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。</p>
<p>例如：我们有10个分区，两个消费者（C1，C2），3个消费者线程，10 / 3 =<br>3而且除不尽。</p>
<p>C1-0 将消费 0, 1, 2, 3 分区</p>
<p>C2-0 将消费 4, 5, 6 分区</p>
<p>C2-1 将消费 7, 8, 9 分区</p>
<p>第一步：将所有主题分区组成TopicAndPartition列表，然后对TopicAndPartition列表按照hashCode进行排序，最后按照轮询的方式发给每一个消费线程。</p>
<h3 id="1-5-13-Kafka挂掉"><a href="#1-5-13-Kafka挂掉" class="headerlink" title="1.5.13 Kafka挂掉"></a>1.5.13 Kafka挂掉</h3><blockquote>
<p>  1）Flume记录</p>
</blockquote>
<blockquote>
<p>  2）日志有记录</p>
</blockquote>
<blockquote>
<p>  3）短期没事</p>
</blockquote>
<h3 id="1-5-14-Kafka丢不丢数据"><a href="#1-5-14-Kafka丢不丢数据" class="headerlink" title="1.5.14 Kafka丢不丢数据"></a>1.5.14 Kafka丢不丢数据</h3><p>Ack=0，相当于异步发送，消息发送完毕即offset增加，继续生产。</p>
<p>Ack=1，leader收到leader replica 对一个消息的接受ack才增加offset，然后继续生产。</p>
<p>Ack=-1，leader收到所有replica 对一个消息的接受ack才增加offset，然后继续生产。</p>
<h3 id="1-5-15-Kafka数据重复"><a href="#1-5-15-Kafka数据重复" class="headerlink" title="1.5.15 Kafka数据重复"></a>1.5.15 Kafka数据重复</h3><p>幂等性 + ack-1 + 事务</p>
<p>Kafka数据重复，可以再下一级：SparkStreaming、redis或者hive中dwd层去重，去重的手段：分组、按照id开窗只取第一个值；</p>
<h3 id="1-5-16-Kafka消息数据积压，Kafka消费能力不足怎么处理？"><a href="#1-5-16-Kafka消息数据积压，Kafka消费能力不足怎么处理？" class="headerlink" title="1.5.16 Kafka消息数据积压，Kafka消费能力不足怎么处理？"></a>1.5.16 Kafka消息数据积压，Kafka消费能力不足怎么处理？</h3><p>1）如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）</p>
<p>2）如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间&lt;生产速度），使处理的数据小于生产的数据，也会造成数据积压。</p>
<h3 id="1-5-17-Kafka参数优化"><a href="#1-5-17-Kafka参数优化" class="headerlink" title="1.5.17 Kafka参数优化"></a>1.5.17 Kafka参数优化</h3><p><strong>1）Broker参数配置（server.properties）</strong></p>
<blockquote>
<p>  1、日志保留策略配置</p>
</blockquote>
<blockquote>
<p>  # 保留三天，也可以更短 （log.cleaner.delete.retention.ms）</p>
</blockquote>
<blockquote>
<p>  log.retention.hours=72</p>
</blockquote>
<blockquote>
<p>  2、Replica相关配置</p>
</blockquote>
<blockquote>
<p>  default.replication.factor:1 默认副本1个</p>
</blockquote>
<blockquote>
<p>  3、网络通信延时</p>
</blockquote>
<blockquote>
<p>  replica.socket.timeout.ms:30000 #当集群之间网络不稳定时,调大该参数</p>
</blockquote>
<blockquote>
<p>  replica.lag.time.max.ms= 600000#<br>  如果网络不好,或者kafka集群压力较大,会出现副本丢失,然后会频繁复制副本,导致集群压力更大,此时可以调大该参数</p>
</blockquote>
<p><strong>2）Producer优化（producer.properties）</strong></p>
<blockquote>
<p>  compression.type:none gzip snappy lz4</p>
</blockquote>
<blockquote>
<p>  #默认发送不进行压缩，推荐配置一种适合的压缩算法，可以大幅度的减缓网络压力和Broker的存储压力。</p>
</blockquote>
<p><strong>3）Kafka内存调整（</strong>kafka-server-start.sh<strong>）</strong></p>
<p>默认内存1个G，生产环境尽量不要超过6个G。</p>
<blockquote>
<p>  export KAFKA_HEAP_OPTS=”-Xms4g -Xmx4g”</p>
</blockquote>
<h3 id="1-5-18-Kafka高效读写数据"><a href="#1-5-18-Kafka高效读写数据" class="headerlink" title="1.5.18 Kafka高效读写数据"></a>1.5.18 Kafka高效读写数据</h3><p><strong>1）Kafka本身是分布式集群，同时采用分区技术，并发度高。</strong></p>
<p><strong>2）顺序写磁盘</strong></p>
<p>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。</p>
<p><strong>3）零复制技术</strong></p>
<h3 id="1-5-19-Kafka单条日志传输大小"><a href="#1-5-19-Kafka单条日志传输大小" class="headerlink" title="1.5.19 Kafka单条日志传输大小"></a>1.5.19 Kafka单条日志传输大小</h3><p>kafka对于消息体的大小默认为单条最大值是1M但是在我们应用场景中,<br>常常会出现一条消息大于1M，如果不对kafka进行配置。则会出现生产者无法将消息推送到kafka或消费者无法去消费kafka里面的数据,<br>这时我们就要对kafka进行以下配置：server.properties</p>
<blockquote>
<p>  replica.fetch.max.bytes: 1048576 broker可复制的消息的最大字节数, 默认为1M</p>
</blockquote>
<blockquote>
<p>  message.max.bytes: 1000012 kafka 会接收单个消息size的最大限制， 默认为1M左右</p>
</blockquote>
<p>注意：message.max.bytes必须小于等于replica.fetch.max.bytes，否则就会导致replica之间数据同步失败。</p>
<h3 id="1-5-20-Kafka过期数据清理"><a href="#1-5-20-Kafka过期数据清理" class="headerlink" title="1.5.20 Kafka过期数据清理"></a>1.5.20 Kafka过期数据清理</h3><p>保证数据没有被引用（没人消费他）</p>
<p>日志清理保存的策略只有delete和compact两种</p>
<p>log.cleanup.policy=delete启用删除策略</p>
<p>log.cleanup.policy=compact启用压缩策略</p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/fa6adeae8eb5">https://www.jianshu.com/p/fa6adeae8eb5</a></p>
<h3 id="1-5-21-Kafka可以按照时间消费数据"><a href="#1-5-21-Kafka可以按照时间消费数据" class="headerlink" title="1.5.21 Kafka可以按照时间消费数据"></a>1.5.21 Kafka可以按照时间消费数据</h3><p>Map&lt;TopicPartition, OffsetAndTimestamp&gt; startOffsetMap =<br>KafkaUtil.fetchOffsetsWithTimestamp(topic, sTime, kafkaProp);</p>
<h3 id="1-5-22-Kafka消费者角度考虑是拉取数据还是推送数据"><a href="#1-5-22-Kafka消费者角度考虑是拉取数据还是推送数据" class="headerlink" title="1.5.22 Kafka消费者角度考虑是拉取数据还是推送数据"></a>1.5.22 Kafka消费者角度考虑是拉取数据还是推送数据</h3><p>拉取数据</p>
<h3 id="1-5-23-Kafka中的数据是有序的吗"><a href="#1-5-23-Kafka中的数据是有序的吗" class="headerlink" title="1.5.23 Kafka中的数据是有序的吗"></a>1.5.23 Kafka中的数据是有序的吗</h3><p>单分区内有序；多分区，分区与分区间无序；</p>
<h2 id="1-6-Hive"><a href="#1-6-Hive" class="headerlink" title="1.6 Hive"></a>1.6 Hive</h2><h3 id="1-6-1-Hive的架构"><a href="#1-6-1-Hive的架构" class="headerlink" title="1.6.1 Hive的架构"></a>1.6.1 Hive的架构</h3><p><img src="media/f8c18c9c9fa3df8aa30d1297184e7406.png"></p>
<blockquote>
<p>  图片</p>
</blockquote>
<h3 id="1-6-2-Hive和数据库比较"><a href="#1-6-2-Hive和数据库比较" class="headerlink" title="1.6.2 Hive和数据库比较"></a>1.6.2 Hive和数据库比较</h3><p>Hive 和数据库除了拥有类似的查询语言，再无类似之处。</p>
<p>1）数据存储位置</p>
<p>Hive 存储在 HDFS 。数据库将数据保存在块设备或者本地文件系统中。</p>
<p>2）数据更新</p>
<p>Hive中不建议对数据的改写。而数据库中的数据通常是需要经常进行修改的，</p>
<p>3）执行延迟</p>
<p>Hive<br>执行延迟较高。数据库的执行延迟较低。当然，这个是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p>
<p>4）数据规模</p>
<p>Hive支持很大规模的数据计算；数据库可以支持的数据规模较小。</p>
<h3 id="1-6-3-内部表和外部表"><a href="#1-6-3-内部表和外部表" class="headerlink" title="1.6.3 内部表和外部表"></a>1.6.3 内部表和外部表</h3><p>元数据、原始数据</p>
<p>1）删除数据时：</p>
<p>内部表：元数据、原始数据，全删除</p>
<p>外部表：元数据 只删除</p>
<p>2）在公司生产环境下，什么时候创建内部表，什么时候创建外部表？</p>
<p>在公司中绝大多数场景都是外部表。</p>
<p>自己使用的临时表，才会创建内部表；</p>
<h3 id="1-6-4-4个By区别"><a href="#1-6-4-4个By区别" class="headerlink" title="1.6.4 4个By区别"></a>1.6.4 4个By区别</h3><p>1）Order By：全局排序，只有一个Reducer；</p>
<p>2）Sort By：分区内有序；</p>
<p>3）Distrbute By：类似MR中Partition，进行分区，结合sort by使用。</p>
<p>4） Cluster By：当Distribute by和Sorts by字段相同时，可以使用Cluster<br>by方式。Cluster by除了具有Distribute by的功能外还兼具Sort<br>by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p>
<p>在生产环境中Order By用的比较少，容易导致OOM。</p>
<p>在生产环境中Sort By+ Distrbute By用的多。</p>
<h3 id="1-6-5-系统函数"><a href="#1-6-5-系统函数" class="headerlink" title="1.6.5 系统函数"></a>1.6.5 系统函数</h3><p>1）date_add、date_sub函数（加减日期）</p>
<p>2）next_day函数（周指标相关）</p>
<p>3）date_format函数（根据格式整理日期）</p>
<p>4）last_day函数（求当月最后一天日期）</p>
<p>5）collect_set函数</p>
<p>6）get_json_object解析json函数</p>
<p>7）NVL（表达式1，表达式2）</p>
<p>如果表达式1为空值，NVL返回值为表达式2的值，否则返回表达式1的值。</p>
<h3 id="1-6-6-自定义UDF、UDTF函数"><a href="#1-6-6-自定义UDF、UDTF函数" class="headerlink" title="1.6.6 自定义UDF、UDTF函数"></a>1.6.6 自定义UDF、UDTF函数</h3><p>1）在项目中是否自定义过UDF、UDTF函数，以及用他们处理了什么问题，及自定义步骤？</p>
<p>（1）用UDF函数解析公共字段；用UDTF函数解析事件字段。</p>
<p>（2）自定义UDF：继承UDF，重写evaluate方法</p>
<p>（3）自定义UDTF：继承自GenericUDTF，重写3个方法：initialize(自定义输出的列名和类型)，process（将结果返回forward(result)），close</p>
<p>2）为什么要自定义UDF/UDTF？</p>
<p>因为自定义函数，可以自己埋点Log打印日志，出错或者数据异常，方便调试。</p>
<h3 id="1-6-7-窗口函数"><a href="#1-6-7-窗口函数" class="headerlink" title="1.6.7 窗口函数"></a>1.6.7 窗口函数</h3><p>1）Rank</p>
<blockquote>
<p>  （1）RANK() 排序相同时会重复，总数不会变</p>
</blockquote>
<blockquote>
<p>  （2）DENSE_RANK() 排序相同时会重复，总数会减少</p>
</blockquote>
<blockquote>
<p>  （3）ROW_NUMBER() 会根据顺序计算</p>
</blockquote>
<p>2）<br>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化</p>
<p>（1）CURRENT ROW：当前行</p>
<p>（2）n PRECEDING：往前n行数据</p>
<p>（3） n FOLLOWING：往后n行数据</p>
<p>（4）UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED<br>FOLLOWING表示到后面的终点</p>
<p>（5） LAG(col,n)：往前第n行数据</p>
<p>（6）LEAD(col,n)：往后第n行数据</p>
<p>（7）<br>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。</p>
<p>3）手写TopN</p>
<h3 id="1-6-8-Hive优化"><a href="#1-6-8-Hive优化" class="headerlink" title="1.6.8 Hive优化"></a>1.6.8 Hive优化</h3><p><strong>1）MapJoin</strong></p>
<p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common<br>Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。</p>
<p><strong>2）行列过滤</strong></p>
<p>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。</p>
<p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。</p>
<p><strong>3）列式存储</strong></p>
<p><strong>4）采用分区技术</strong></p>
<p><strong>5）合理设置Map数</strong></p>
<p>mapred.min.split.size: 指的是数据的最小分割单元大小；min的默认值是1B</p>
<p>mapred.max.split.size: 指的是数据的最大分割单元大小；max的默认值是256MB</p>
<p>通过调整max可以起到调整map数的作用，减小max可以增加map数，增大max可以减少map数。</p>
<p>需要提醒的是，直接调整mapred.map.tasks这个参数是没有效果的。</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/swordfall/p/11037539.html">https://www.cnblogs.com/swordfall/p/11037539.html</a></p>
<p><strong>6）合理设置Reduce数</strong></p>
<p>Reduce个数并不是越多越好</p>
<p>（1）过多的启动和初始化Reduce也会消耗时间和资源；</p>
<p>（2）另外，有多少个Reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p>
<p>在设置Reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的Reduce数；使单个Reduce任务处理数据量大小要合适；</p>
<p><strong>7）小文件如何产生的？</strong></p>
<p>（1）动态分区插入数据，产生大量的小文件，从而导致map数量剧增；</p>
<p>（2）reduce数量越多，小文件也越多（reduce的个数和输出文件是对应的）；</p>
<p>（3）数据源本身就包含大量的小文件。</p>
<p><strong>8）小文件解决方案</strong></p>
<p>（1）在Map执行前合并小文件，减少Map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p>
<blockquote>
<p>  <strong>（2）merge</strong></p>
</blockquote>
<blockquote>
<p>  // 输出合并小文件</p>
</blockquote>
<p>SET hive.merge.mapfiles = true; – 默认true，在map-only任务结束时合并小文件</p>
<p>SET hive.merge.mapredfiles = true; –<br>默认false，在map-reduce任务结束时合并小文件</p>
<p>SET hive.merge.size.per.task = 268435456; – 默认256M</p>
<p>SET hive.merge.smallfiles.avgsize = 16777216; –<br>当输出文件的平均大小小于16m该值时，启动一个独立的map-reduce任务进行文件merge</p>
<blockquote>
<p>  <strong>（3）开启JVM重用</strong></p>
</blockquote>
<p>set mapreduce.job.jvm.numtasks=10</p>
<p><strong>9）开启map端combiner（不影响最终业务逻辑）</strong></p>
<p>set hive.map.aggr=true；</p>
<p><strong>10）压缩（选择快的）</strong></p>
<p>设置map端输出、中间结果压缩。（不完全是解决数据倾斜的问题，但是减少了IO读写和网络传输，能提高很多效率）</p>
<p>set hive.exec.compress.intermediate=true –启用中间数据压缩</p>
<p>set mapreduce.map.output.compress=true –启用最终数据压缩</p>
<p>set mapreduce.map.outout.compress.codec=…; –设置压缩方式</p>
<p><strong>11）采用tez引擎或者spark引擎</strong></p>
<h3 id="1-6-9-Hive解决数据倾斜方法"><a href="#1-6-9-Hive解决数据倾斜方法" class="headerlink" title="1.6.9 Hive解决数据倾斜方法"></a>1.6.9 Hive解决数据倾斜方法</h3><p><strong>1）数据倾斜长啥样？</strong></p>
<p><img src="media/22038bef2feb5c0ece38a55c53d9943a.jpeg"></p>
<p><img src="media/9047befedb8470e10af55e1a1bf15038.jpeg"></p>
<p><strong>2）怎么产生的数据倾斜？</strong></p>
<p><strong>（1）不同数据类型关联产生数据倾斜</strong></p>
<p>情形：比如用户表中user_id字段为int，log表中user_id字段string类型。当按照user_id进行两个表的Join操作时。</p>
<p>解决方式：把数字类型转换成字符串类型</p>
<p>select * from users a</p>
<p>left outer join logs b</p>
<p>on a.usr_id = cast(b.user_id as string)</p>
<p>bug记录：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/2181e00d74dc">https://www.jianshu.com/p/2181e00d74dc</a></p>
<p><strong>（2）控制空值分布</strong></p>
<p>在生产环境经常会用大量空值数据进入到一个reduce中去，导致数据倾斜。</p>
<p>解决办法：</p>
<p>自定义分区，将为空的key转变为字符串加随机数或纯随机数，将因空值而造成倾斜的数据分不到多个Reducer。</p>
<p>注意：对于异常值如果不需要的话，最好是提前在where条件里过滤掉，这样可以使计算量大大减少</p>
<p><strong>3）解决数据倾斜的方法？</strong></p>
<p><strong>（1）group by</strong></p>
<p>注：group by 优于distinct group</p>
<p>解决方式：采用sum() group by的方式来替换count(distinct)完成计算。</p>
<p><strong>（2）mapjoin</strong></p>
<p><strong>（3）开启数据倾斜时负载均衡</strong></p>
<blockquote>
<p>  set hive.groupby.skewindata=true;</p>
</blockquote>
<p>思想：就是先随机分发并处理，再按照key group by来分发处理。</p>
<p>操作：当选项设定为true，生成的查询计划会有两个MRJob。</p>
<p>第一个MRJob中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupBy<br>Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；</p>
<p>第二个MRJob再根据预处理的数据结果按照GroupBy<br>Key分布到Reduce中（这个过程可以保证相同的原始GroupBy<br>Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p>
<p>点评：它使计算变成了两个mapreduce，先在第一个中在shuffle过程partition时随机给<br>key打标记，使每个key随机均匀分布到各个reduce上计算，但是这样只能完成部分计算，因为相同key没有分配到相同reduce上。</p>
<p>所以需要第二次的mapreduce，这次就回归正常shuffle，但是数据分布不均匀的问题在第一次mapreduce已经有了很大的改善，因此基本解决数据倾斜。因为大量计算已经在第一次mr中随机分布到各个节点完成。</p>
<p><strong>（4）设置多个reduce个数</strong></p>
<h3 id="1-6-10-Hive里边字段的分隔符用的什么？为什么用-t？有遇到过字段里边有-t的情况吗，怎么处理的？"><a href="#1-6-10-Hive里边字段的分隔符用的什么？为什么用-t？有遇到过字段里边有-t的情况吗，怎么处理的？" class="headerlink" title="1.6.10 Hive里边字段的分隔符用的什么？为什么用\t？有遇到过字段里边有\t的情况吗，怎么处理的？"></a>1.6.10 Hive里边字段的分隔符用的什么？为什么用\t？有遇到过字段里边有\t的情况吗，怎么处理的？</h3><p>hive 默认的字段分隔符为ascii码的控制符\001（^A）,建表的时候用fields terminated<br>by<br>‘\001’。注意：如果采用\t或者\001等为分隔符，需要要求前端埋点和javaEE后台传递过来的数据必须不能出现该分隔符，通过代码规范约束。一旦传输过来的数据含有分隔符，需要在前一级数据中转义或者替换（ETL）。</p>
<h3 id="1-6-11-Tez引擎优点？"><a href="#1-6-11-Tez引擎优点？" class="headerlink" title="1.6.11 Tez引擎优点？"></a>1.6.11 Tez引擎优点？</h3><p>Tez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。</p>
<p>Mr/tez/spark区别：</p>
<p>Mr引擎：多job串联，基于磁盘，落盘的地方比较多。虽然慢，但一定能跑出结果。一般处理，周、月、年指标。</p>
<p>Spark引擎：虽然在Shuffle过程中也落盘，但是并不是所有算子都需要Shuffle，尤其是多算子过程，中间过程不落盘<br>DAG有向无环图。 兼顾了可靠性和效率。一般处理天指标。</p>
<p>Tez引擎：完全基于内存。<br>注意：如果数据量特别大，慎重使用。容易OOM。一般用于快速出结果，数据量比较小的场景。</p>
<h3 id="1-6-12-MySQL元数据备份"><a href="#1-6-12-MySQL元数据备份" class="headerlink" title="1.6.12 MySQL元数据备份"></a>1.6.12 MySQL元数据备份</h3><p><strong>1）MySQL之元数据备份（项目中遇到的问题）</strong></p>
<p>元数据备份（重点，如数据损坏，可能整个集群无法运行，至少要保证每日零点之后备份到其它服务器两个复本）</p>
<p>Keepalived或者用mycat</p>
<p>2）MySQL utf8超过字节数问题</p>
<p>MySQL的utf8编码最多存储3个字节，当数据中存在表情号、特色符号时会占用超过3个字节数的字节，那么会出现错误<br>Incorrect string value: ‘\xF0\x9F\x91\x91\xE5\xB0…’</p>
<p>解决办法：将utf8修改为utf8mb4</p>
<p>首先修改库的基字符集和数据库排序规则</p>
<p><img src="media/21c1691a0e2547efd21261d03fb87175.png" alt="e46516d3561eea6523d980c7af9eadd"></p>
<p>再使用 SHOW VARIABLES LIKE ‘%char%’; 命令查看参数</p>
<p><img src="media/c3e2091b70b6d0ab6817ab7528e547bd.png"></p>
<p>确保这几个参数的value值为utf8mb4 如果不是使用set命令修改</p>
<p>如：set character_set_server = utf8mb4;</p>
<h3 id="1-6-13-Union与Union-all区别"><a href="#1-6-13-Union与Union-all区别" class="headerlink" title="1.6.13 Union与Union all区别"></a>1.6.13 Union与Union all区别</h3><p>1）union会将联合的结果集去重，效率较union all差</p>
<p>2）union all不会对结果集去重，所以效率高</p>
<h2 id="1-7-Sqoop"><a href="#1-7-Sqoop" class="headerlink" title="1.7 Sqoop"></a>1.7 Sqoop</h2><h3 id="1-7-1-Sqoop参数"><a href="#1-7-1-Sqoop参数" class="headerlink" title="1.7.1 Sqoop参数"></a>1.7.1 Sqoop参数</h3><p>/opt/module/sqoop/bin/sqoop import \</p>
<p>--connect \</p>
<p>--username \</p>
<p>--password \</p>
<p>--target-dir \</p>
<p>--delete-target-dir \</p>
<p>--num-mappers \</p>
<p>--fields-terminated-by \</p>
<p>--query “$2” ‘ and $CONDITIONS;’</p>
<h3 id="1-7-2-Sqoop导入导出Null存储一致性问题"><a href="#1-7-2-Sqoop导入导出Null存储一致性问题" class="headerlink" title="1.7.2 Sqoop导入导出Null存储一致性问题"></a>1.7.2 Sqoop导入导出Null存储一致性问题</h3><p>Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用–input-null-string和–input-null-non-string两个参数。导入数据时采用–null-string和–null-non-string。</p>
<h3 id="1-7-3-Sqoop数据导出一致性问题"><a href="#1-7-3-Sqoop数据导出一致性问题" class="headerlink" title="1.7.3 Sqoop数据导出一致性问题"></a>1.7.3 Sqoop数据导出一致性问题</h3><p>场景1：如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。</p>
<p>官网：<a target="_blank" rel="noopener" href="http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html">http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html</a></p>
<p>Since Sqoop breaks down export process into multiple transactions, it is<br>possible that a failed export job may result in partial data being committed to<br>the database. This can further lead to subsequent jobs failing due to insert<br>collisions in some cases, or lead to duplicated data in others. You can overcome<br>this problem by specifying a staging table via the –staging-table option which<br>acts as an auxiliary table that is used to stage exported data. The staged data<br>is finally moved to the destination table in a single transaction.</p>
<p>–staging-table方式</p>
<p>sqoop export –connect jdbc:mysql://192.168.137.10:3306/user_behavior –username<br>root –password 123456 –table app_cource_study_report –columns<br>watch_video_cnt,complete_video_cnt,dt –fields-terminated-by “\t” –export-dir<br>“/user/hive/warehouse/tmp.db/app_cource_study_analysis_${day}” –staging-table<br>app_cource_study_report_tmp –clear-staging-table –input-null-string ‘\N’</p>
<h3 id="1-7-4-Sqoop底层运行的任务是什么"><a href="#1-7-4-Sqoop底层运行的任务是什么" class="headerlink" title="1.7.4 Sqoop底层运行的任务是什么"></a>1.7.4 Sqoop底层运行的任务是什么</h3><p>只有Map阶段，没有Reduce阶段的任务。默认是4个MapTask。</p>
<h3 id="1-7-5-Sqoop一天导入多少数据"><a href="#1-7-5-Sqoop一天导入多少数据" class="headerlink" title="1.7.5 Sqoop一天导入多少数据"></a>1.7.5 Sqoop一天导入多少数据</h3><p>100万日活=》10万订单，1人10条，每天1g左右业务数据</p>
<p>Sqoop每天将1G的数据量导入到数仓。</p>
<h3 id="1-7-6-Sqoop数据导出的时候一次执行多长时间"><a href="#1-7-6-Sqoop数据导出的时候一次执行多长时间" class="headerlink" title="1.7.6 Sqoop数据导出的时候一次执行多长时间"></a>1.7.6 Sqoop数据导出的时候一次执行多长时间</h3><p>每天晚上00:30开始执行，Sqoop任务一般情况40<br>-50分钟的都有。取决于数据量（11:11，6:18等活动在1个小时左右）。</p>
<h3 id="1-7-7-Sqoop在导入数据的时候数据倾斜"><a href="#1-7-7-Sqoop在导入数据的时候数据倾斜" class="headerlink" title="1.7.7 Sqoop在导入数据的时候数据倾斜"></a>1.7.7 Sqoop在导入数据的时候数据倾斜</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lizhiguo18/article/details/103969906">https://blog.csdn.net/lizhiguo18/article/details/103969906</a></p>
<p>Sqoop 抽数的并行化主要涉及到两个参数：通过ROWNUM()<br>生成一个严格均匀分布的字段，然后指定为分割字段;<br>split-by：按照某一列来切分表的工作单元。</p>
<p>num-mappers：启动N个map来并行导入数据，默认4个；</p>
<h3 id="1-7-8-Sqoop数据导出Parquet（项目中遇到的问题）"><a href="#1-7-8-Sqoop数据导出Parquet（项目中遇到的问题）" class="headerlink" title="1.7.8 Sqoop数据导出Parquet（项目中遇到的问题）"></a>1.7.8 Sqoop数据导出Parquet（项目中遇到的问题）</h3><p>Ads层数据用Sqoop往MySql中导入数据的时候，如果用了orc（Parquet）不能导入，需转化成text格式</p>
<p>（1）创建临时表，把Parquet中表数据导入到临时表，把临时表导出到目标表用于可视化</p>
<p>（2）ads层建表的时候就不要建Parquet表</p>
<h2 id="1-8-Azkaban"><a href="#1-8-Azkaban" class="headerlink" title="1.8 Azkaban"></a>1.8 Azkaban</h2><h3 id="1-8-1-每天集群运行多少指标"><a href="#1-8-1-每天集群运行多少指标" class="headerlink" title="1.8.1 每天集群运行多少指标?"></a>1.8.1 每天集群运行多少指标?</h3><p>每天跑100多个指标，有活动时跑200个左右。</p>
<h3 id="1-8-2-任务挂了怎么办？"><a href="#1-8-2-任务挂了怎么办？" class="headerlink" title="1.8.2 任务挂了怎么办？"></a>1.8.2 任务挂了怎么办？</h3><blockquote>
<p>  1）运行成功或者失败都会发邮件、发钉钉、集成自动打电话<strong>（项目中遇到的问题）</strong></p>
</blockquote>
<blockquote>
<p>  2）最主要的解决方案就是重新跑。</p>
</blockquote>
<blockquote>
<p>  3）报警网站<a target="_blank" rel="noopener" href="http://www.onealert.com/">http://www.onealert.com/</a></p>
</blockquote>
<h2 id="1-9-HBase"><a href="#1-9-HBase" class="headerlink" title="1.9 HBase"></a>1.9 HBase</h2><h3 id="1-9-1-HBase存储结构"><a href="#1-9-1-HBase存储结构" class="headerlink" title="1.9.1 HBase存储结构"></a>1.9.1 HBase存储结构</h3><h3 id="1-9-2-RowKey设计原则"><a href="#1-9-2-RowKey设计原则" class="headerlink" title="1.9.2 RowKey设计原则"></a>1.9.2 RowKey设计原则</h3><p>1）rowkey长度原则</p>
<p>2）rowkey散列原则</p>
<p>3）rowkey唯一原则</p>
<h3 id="1-9-3-RowKey如何设计"><a href="#1-9-3-RowKey如何设计" class="headerlink" title="1.9.3 RowKey如何设计"></a>1.9.3 RowKey如何设计</h3><blockquote>
<p>  1）生成随机数、hash、散列值</p>
</blockquote>
<blockquote>
<p>  2）字符串反转</p>
</blockquote>
<h3 id="1-9-4-Phoenix二级索引（讲原理）"><a href="#1-9-4-Phoenix二级索引（讲原理）" class="headerlink" title="1.9.4 Phoenix二级索引（讲原理）"></a>1.9.4 Phoenix二级索引（讲原理）</h3><h2 id="1-10-Scala"><a href="#1-10-Scala" class="headerlink" title="1.10 Scala"></a>1.10 Scala</h2><h3 id="1-10-1-开发环境"><a href="#1-10-1-开发环境" class="headerlink" title="1.10.1 开发环境"></a>1.10.1 开发环境</h3><p>要求掌握必要的scala开发环境搭建技能。</p>
<h3 id="1-10-2-变量和数据类型"><a href="#1-10-2-变量和数据类型" class="headerlink" title="1.10.2 变量和数据类型"></a>1.10.2 变量和数据类型</h3><p>掌握var和val的区别</p>
<p>掌握数值类型（Byte、Short、Int、Long、Float、Double、Char）之间的转换关系</p>
<h3 id="1-10-3-流程控制"><a href="#1-10-3-流程控制" class="headerlink" title="1.10.3 流程控制"></a>1.10.3 流程控制</h3><p>掌握if-else、for、while等必要的流程控制结构，掌握如何实现break、continue的功能。</p>
<h3 id="1-10-4-函数式编程"><a href="#1-10-4-函数式编程" class="headerlink" title="1.10.4 函数式编程"></a>1.10.4 函数式编程</h3><p>掌握高阶函数、匿名函数、函数柯里化、函数参数以及函数至简原则。</p>
<h3 id="1-10-5-面向对象"><a href="#1-10-5-面向对象" class="headerlink" title="1.10.5 面向对象"></a>1.10.5 面向对象</h3><p>掌握Scala与Java继承方面的区别、单例对象（伴生对象）、特质的用法及功能。</p>
<h3 id="1-10-6-集合"><a href="#1-10-6-集合" class="headerlink" title="1.10.6 集合"></a>1.10.6 集合</h3><p>掌握常用集合的使用、集合常用的计算函数。</p>
<h3 id="1-10-7-模式匹配"><a href="#1-10-7-模式匹配" class="headerlink" title="1.10.7 模式匹配"></a>1.10.7 模式匹配</h3><p>掌握模式匹配的用法</p>
<h3 id="1-10-8-异常"><a href="#1-10-8-异常" class="headerlink" title="1.10.8 异常"></a>1.10.8 异常</h3><p>掌握异常常用操作即可</p>
<h3 id="1-10-9-隐式转换"><a href="#1-10-9-隐式转换" class="headerlink" title="1.10.9 隐式转换"></a>1.10.9 隐式转换</h3><p>掌握隐式方法、隐式参数、隐式类，以及隐式解析机制</p>
<h3 id="1-10-10-泛型"><a href="#1-10-10-泛型" class="headerlink" title="1.10.10 泛型"></a>1.10.10 泛型</h3><p>掌握泛型语法</p>
<h2 id="1-11-Spark-Core-amp-SQL"><a href="#1-11-Spark-Core-amp-SQL" class="headerlink" title="1.11 Spark Core &amp; SQL"></a>1.11 Spark Core &amp; SQL</h2><h3 id="1-11-1-Spark有几种部署方式？请分别简要论述"><a href="#1-11-1-Spark有几种部署方式？请分别简要论述" class="headerlink" title="1.11.1 Spark有几种部署方式？请分别简要论述"></a>1.11.1 Spark有几种部署方式？请分别简要论述</h3><p>1）Local:运行在一台机器上，通常是练手或者测试环境。</p>
<p>2）Standalone:构建一个基于Mster+Slaves的资源调度集群，Spark任务提交给Master运行。是Spark自身的一个调度系统。</p>
<p>3）Yarn:<br>Spark客户端直接连接Yarn，不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。</p>
<p>4）Mesos：国内大环境比较少用。</p>
<h3 id="1-11-2-Spark任务使用什么进行提交，JavaEE界面还是脚本"><a href="#1-11-2-Spark任务使用什么进行提交，JavaEE界面还是脚本" class="headerlink" title="1.11.2 Spark任务使用什么进行提交，JavaEE界面还是脚本"></a>1.11.2 Spark任务使用什么进行提交，JavaEE界面还是脚本</h3><p>Shell 脚本。</p>
<h3 id="1-11-3-Spark提交作业参数（重点）"><a href="#1-11-3-Spark提交作业参数（重点）" class="headerlink" title="1.11.3 Spark提交作业参数（重点）"></a>1.11.3 Spark提交作业参数（重点）</h3><p>参考答案：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/gamer_gyt/article/details/79135118">https://blog.csdn.net/gamer_gyt/article/details/79135118</a></p>
<p>1）在提交任务时的几个重要参数</p>
<blockquote>
<p>  executor-cores ——<br>  每个executor使用的内核数，默认为1，官方建议2-5个，我们企业是4个</p>
</blockquote>
<blockquote>
<p>  num-executors —— 启动executors的数量，默认为2</p>
</blockquote>
<blockquote>
<p>  executor-memory —— executor内存大小，默认1G</p>
</blockquote>
<blockquote>
<p>  driver-cores —— driver使用内核数，默认为1</p>
</blockquote>
<blockquote>
<p>  driver-memory —— driver内存大小，默认512M</p>
</blockquote>
<p>2）边给一个提交任务的样式</p>
<blockquote>
<p>  spark-submit \</p>
</blockquote>
<blockquote>
<p>  –master local[5] \</p>
</blockquote>
<blockquote>
<p>  –driver-cores 2 \</p>
</blockquote>
<blockquote>
<p>  –driver-memory 8g \</p>
</blockquote>
<blockquote>
<p>  –executor-cores 4 \</p>
</blockquote>
<blockquote>
<p>  –num-executors 10 \</p>
</blockquote>
<blockquote>
<p>  –executor-memory 8g \</p>
</blockquote>
<blockquote>
<p>  –class PackageName.ClassName XXXX.jar \</p>
</blockquote>
<blockquote>
<p>  –name “Spark Job Name” \</p>
</blockquote>
<blockquote>
<p>  InputPath \</p>
</blockquote>
<blockquote>
<p>  OutputPath</p>
</blockquote>
<h3 id="1-11-4-简述Spark的架构与作业提交流程（画图讲解，注明各个部分的作用）（重点）"><a href="#1-11-4-简述Spark的架构与作业提交流程（画图讲解，注明各个部分的作用）（重点）" class="headerlink" title="1.11.4 简述Spark的架构与作业提交流程（画图讲解，注明各个部分的作用）（重点）"></a>1.11.4 简述Spark的架构与作业提交流程（画图讲解，注明各个部分的作用）（重点）</h3><h3 id="1-11-5-如何理解Spark中的血统概念（RDD）（笔试重点）"><a href="#1-11-5-如何理解Spark中的血统概念（RDD）（笔试重点）" class="headerlink" title="1.11.5 如何理解Spark中的血统概念（RDD）（笔试重点）"></a>1.11.5 如何理解Spark中的血统概念（RDD）（笔试重点）</h3><p>RDD在Lineage依赖方面分为两种Narrow Dependencies与Wide<br>Dependencies用来解决数据容错时的高效性以及划分任务时候起到重要作用。</p>
<h3 id="1-11-6-简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage又根据什么决定task个数-（笔试重点）"><a href="#1-11-6-简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage又根据什么决定task个数-（笔试重点）" class="headerlink" title="1.11.6 简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage又根据什么决定task个数? （笔试重点）"></a>1.11.6 简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage又根据什么决定task个数? （笔试重点）</h3><p>Stage：根据RDD之间的依赖关系的不同将Job划分成不同的Stage，遇到一个宽依赖则划分一个Stage。</p>
<p>Task：Stage是一个TaskSet，将Stage根据分区数划分成一个个的Task。</p>
<h3 id="1-11-7-请列举Spark的transformation算子（不少于8个），并简述功能（重点）"><a href="#1-11-7-请列举Spark的transformation算子（不少于8个），并简述功能（重点）" class="headerlink" title="1.11.7 请列举Spark的transformation算子（不少于8个），并简述功能（重点）"></a>1.11.7 请列举Spark的transformation算子（不少于8个），并简述功能（重点）</h3><p>1）map（func）：返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成.</p>
<p>2）mapPartitions(func)：类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RD上运行时，func的函数类型必须是Iterator[T]<br>=&gt;<br>Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。</p>
<p>3）reduceByKey（func，[numTask]）：在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</p>
<p>4）aggregateByKey (zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt;<br>U,combOp: (U, U) =&gt; U:<br>在kv对的RDD中，，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。</p>
<p>5）combineByKey(createCombiner: V=&gt;C, mergeValue: (C, V) =&gt;C, mergeCombiners:<br>(C, C) =&gt;C):</p>
<p>对相同K，把V合并成一个集合。</p>
<p>1.createCombiner: combineByKey()<br>会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素,combineByKey()会使用一个叫作createCombiner()的函数来创建那个键对应的累加器的初始值</p>
<p>2.mergeValue:<br>如果这是一个在处理当前分区之前已经遇到的键，它会使用mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并</p>
<p>3.mergeCombiners: 由于每个分区都是独立处理的，<br>因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，<br>就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。</p>
<p>…</p>
<p>根据自身情况选择比较熟悉的算子加以介绍。</p>
<h3 id="1-11-8-请列举Spark的action算子（不少于6个），并简述功能（重点）"><a href="#1-11-8-请列举Spark的action算子（不少于6个），并简述功能（重点）" class="headerlink" title="1.11.8 请列举Spark的action算子（不少于6个），并简述功能（重点）"></a>1.11.8 请列举Spark的action算子（不少于6个），并简述功能（重点）</h3><p>1）reduce：</p>
<p>2）collect:</p>
<p>3）first：</p>
<p>4）take：</p>
<p>5）aggregate：</p>
<p>6）countByKey：</p>
<p>7）foreach：</p>
<p>8）saveAsTextFile：</p>
<h3 id="1-11-9-请列举会引起Shuffle过程的Spark算子，并简述功能。"><a href="#1-11-9-请列举会引起Shuffle过程的Spark算子，并简述功能。" class="headerlink" title="1.11.9 请列举会引起Shuffle过程的Spark算子，并简述功能。"></a>1.11.9 请列举会引起Shuffle过程的Spark算子，并简述功能。</h3><p>reduceBykey：</p>
<p>groupByKey：</p>
<p>…ByKey:</p>
<h3 id="1-11-10-简述Spark的两种核心Shuffle（HashShuffle与SortShuffle）的工作流程（包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle）（重点）"><a href="#1-11-10-简述Spark的两种核心Shuffle（HashShuffle与SortShuffle）的工作流程（包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle）（重点）" class="headerlink" title="1.11.10 简述Spark的两种核心Shuffle（HashShuffle与SortShuffle）的工作流程（包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle）（重点）"></a>1.11.10 简述Spark的两种核心Shuffle（HashShuffle与SortShuffle）的工作流程（包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle）（重点）</h3><p>未经优化的HashShuffle：</p>
<p>优化后的Shuffle：</p>
<p>普通的SortShuffle：</p>
<p>当 shuffle read task 的 数 量 小 于 等 于 spark.shuffle.sort。</p>
<p>bypassMergeThreshold 参数的值时（默认为 200），就会启用 bypass 机制。</p>
<p><img src="media/c6bf5aab850855e1110b69a39e3914ca.png" alt="图片"></p>
<h3 id="1-11-11-Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势？（重点）"><a href="#1-11-11-Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势？（重点）" class="headerlink" title="1.11.11 Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势？（重点）"></a>1.11.11 Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势？（重点）</h3><p>reduceByKey：按照key进行聚合，在shuffle之前有combine（预聚合）操作，返回结果是RDD[k,v]。</p>
<p>groupByKey：按照key进行分组，直接进行shuffle。</p>
<p>开发指导：reduceByKey比groupByKey，建议使用。但是需要注意是否会影响业务逻辑。</p>
<h3 id="1-11-12-Repartition和Coalesce关系与区别"><a href="#1-11-12-Repartition和Coalesce关系与区别" class="headerlink" title="1.11.12 Repartition和Coalesce关系与区别"></a>1.11.12 Repartition和Coalesce关系与区别</h3><p>1）关系：</p>
<p>两者都是用来改变RDD的partition数量的，repartition底层调用的就是coalesce方法：coalesce(numPartitions,<br>shuffle = true)</p>
<p>2）区别：</p>
<p>repartition一定会发生shuffle，coalesce根据传入的参数来判断是否发生shuffle</p>
<p>一般情况下增大rdd的partition数量使用repartition，减少partition数量时使用coalesce</p>
<h3 id="1-11-13-分别简述Spark中的缓存机制（cache和persist）与checkpoint机制，并指出两者的区别与联系"><a href="#1-11-13-分别简述Spark中的缓存机制（cache和persist）与checkpoint机制，并指出两者的区别与联系" class="headerlink" title="1.11.13 分别简述Spark中的缓存机制（cache和persist）与checkpoint机制，并指出两者的区别与联系"></a>1.11.13 分别简述Spark中的缓存机制（cache和persist）与checkpoint机制，并指出两者的区别与联系</h3><p>都是做RDD持久化的</p>
<p>cache:内存，不会截断血缘关系，使用计算过程中的数据缓存。</p>
<p>checkpoint：磁盘，截断血缘关系，在ck之前必须没有任何任务提交才会生效，ck过程会额外提交一次任务。</p>
<h3 id="1-11-14-简述Spark中共享变量（广播变量和累加器）的基本原理与用途。（重点）"><a href="#1-11-14-简述Spark中共享变量（广播变量和累加器）的基本原理与用途。（重点）" class="headerlink" title="1.11.14 简述Spark中共享变量（广播变量和累加器）的基本原理与用途。（重点）"></a>1.11.14 简述Spark中共享变量（广播变量和累加器）的基本原理与用途。（重点）</h3><p>累加器（accumulator）是Spark中提供的一种分布式的变量机制，其原理类似于mapreduce，即分布式的改变，然后聚合这些改变。累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。而广播变量用来高效分发较大的对象。</p>
<p>共享变量出现的原因：</p>
<p>通常在向 Spark 传递函数时，比如使用 map() 函数或者用 filter()<br>传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。</p>
<p>Spark的两个共享变量，累加器与广播变量，分别为结果聚合与广播这两种常见的通信模式突破了这一限制。</p>
<h3 id="1-11-15-当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？"><a href="#1-11-15-当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？" class="headerlink" title="1.11.15 当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？"></a>1.11.15 当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？</h3><p>使用foreachPartition代替foreach，在foreachPartition内获取数据库的连接。</p>
<h3 id="1-11-16-如何使用Spark实现TopN的获取（描述思路或使用伪代码）（重点）"><a href="#1-11-16-如何使用Spark实现TopN的获取（描述思路或使用伪代码）（重点）" class="headerlink" title="1.11.16 如何使用Spark实现TopN的获取（描述思路或使用伪代码）（重点）"></a>1.11.16 如何使用Spark实现TopN的获取（描述思路或使用伪代码）（重点）</h3><p>方法1：</p>
<p>（1）按照key对数据进行聚合（groupByKey）</p>
<p>（2）将value转换为数组，利用scala的sortBy或者sortWith进行排序（mapValues）数据量太大，会OOM。</p>
<p>方法2：</p>
<p>（1）取出所有的key</p>
<p>（2）对key进行迭代，每次取出一个key利用spark的排序算子进行排序</p>
<p>方法3：</p>
<p>（1）自定义分区器，按照key进行分区，使不同的key进到不同的分区</p>
<p>（2）对每个分区运用spark的排序算子进行排序</p>
<h3 id="1-11-17-京东：调优之前与调优之后性能的详细对比（例如调整map个数，map个数之前多少、之后多少，有什么提升）"><a href="#1-11-17-京东：调优之前与调优之后性能的详细对比（例如调整map个数，map个数之前多少、之后多少，有什么提升）" class="headerlink" title="1.11.17 京东：调优之前与调优之后性能的详细对比（例如调整map个数，map个数之前多少、之后多少，有什么提升）"></a>1.11.17 京东：调优之前与调优之后性能的详细对比（例如调整map个数，map个数之前多少、之后多少，有什么提升）</h3><p>这里举个例子。比如我们有几百个文件，会有几百个map出现，读取之后进行join操作，会非常的慢。这个时候我们可以进行coalesce操作，比如240个map，我们合成60个map，也就是窄依赖。这样再shuffle，过程产生的文件数会大大减少。提高join的时间性能。</p>
<h3 id="1-11-18-简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系-（笔试重点）"><a href="#1-11-18-简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系-（笔试重点）" class="headerlink" title="1.11.18 简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系? （笔试重点）"></a>1.11.18 简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系? （笔试重点）</h3><p><strong>1）RDD</strong></p>
<p>优点:</p>
<p>编译时类型安全</p>
<p>编译时就能检查出类型错误</p>
<p>面向对象的编程风格</p>
<p>直接通过类名点的方式来操作数据</p>
<p>缺点:</p>
<p>序列化和反序列化的性能开销</p>
<p>无论是集群间的通信, 还是IO操作都需要对对象的结构和数据进行序列化和反序列化。</p>
<p>GC的性能开销，频繁的创建和销毁对象, 势必会增加GC</p>
<p><strong>2）DataFrame</strong></p>
<p>DataFrame引入了schema和off-heap</p>
<p>schema : RDD每一行的数据, 结构都是一样的，这个结构就存储在schema中。<br>Spark通过schema就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据,<br>而结构的部分就可以省略了。</p>
<p><strong>3）DataSet</strong></p>
<p>DataSet结合了RDD和DataFrame的优点，并带来的一个新的概念Encoder。</p>
<p>当序列化数据时，Encoder产生字节码与off-heap进行交互，能够达到按需访问数据的效果，而不用反序列化整个对象。Spark还没有提供自定义Encoder的API，但是未来会加入。</p>
<p>三者之间的转换：</p>
<h3 id="1-11-19-append和overwrite的区别"><a href="#1-11-19-append和overwrite的区别" class="headerlink" title="1.11.19 append和overwrite的区别"></a>1.11.19 append和overwrite的区别</h3><p>append在原有分区上进行追加，overwrite在原有分区上进行全量刷新</p>
<h3 id="1-11-20-coalesce和repartition的区别"><a href="#1-11-20-coalesce和repartition的区别" class="headerlink" title="1.11.20 coalesce和repartition的区别"></a>1.11.20 coalesce和repartition的区别</h3><p>coalesce和repartition都用于改变分区，coalesce用于缩小分区且不会进行shuffle，repartition用于增大分区（提供并行度）会进行shuffle,在spark中减少文件个数会使用coalesce来减少分区来到这个目的。但是如果数据量过大，分区数过少会出现OOM所以coalesce缩小分区个数也需合理</p>
<h3 id="1-11-21-cache缓存级别"><a href="#1-11-21-cache缓存级别" class="headerlink" title="1.11.21 cache缓存级别"></a>1.11.21 cache缓存级别</h3><p>DataFrame的cache默认采用 MEMORY_AND_DISK 这和RDD 的默认方式不一样RDD cache<br>默认采用MEMORY_ONLY</p>
<h3 id="1-11-22-释放缓存和缓存"><a href="#1-11-22-释放缓存和缓存" class="headerlink" title="1.11.22 释放缓存和缓存"></a>1.11.22 释放缓存和缓存</h3><p>缓存：(1)dataFrame.cache (2)sparkSession.catalog.cacheTable(“tableName”)</p>
<p>释放缓存：(1)dataFrame.unpersist<br>(2)sparkSession.catalog.uncacheTable(“tableName”)</p>
<h3 id="1-11-23-Spark-Shuffle默认并行度"><a href="#1-11-23-Spark-Shuffle默认并行度" class="headerlink" title="1.11.23 Spark Shuffle默认并行度"></a>1.11.23 Spark Shuffle默认并行度</h3><p>参数spark.sql.shuffle.partitions 决定 默认并行度200</p>
<h3 id="1-11-24-kryo序列化"><a href="#1-11-24-kryo序列化" class="headerlink" title="1.11.24 kryo序列化"></a>1.11.24 kryo序列化</h3><p>kryo序列化比java序列化更快更紧凑，但spark默认的序列化是java序列化并不是spark序列化，因为spark并不支持所有序列化类型，而且每次使用都必须进行注册。注册只针对于RDD。在DataFrames和DataSet当中自动实现了kryo序列化。</p>
<h3 id="1-11-25-创建临时表和全局临时表"><a href="#1-11-25-创建临时表和全局临时表" class="headerlink" title="1.11.25 创建临时表和全局临时表"></a>1.11.25 创建临时表和全局临时表</h3><p>DataFrame.createTempView() 创建普通临时表</p>
<p>DataFrame.createGlobalTempView() DataFrame.createOrReplaceTempView()<br>创建全局临时表</p>
<h3 id="1-11-26-BroadCast-join-广播join"><a href="#1-11-26-BroadCast-join-广播join" class="headerlink" title="1.11.26 BroadCast join 广播join"></a>1.11.26 BroadCast join 广播join</h3><p>原理：先将小表数据查询出来聚合到driver端，再广播到各个executor端，使表与表join时</p>
<p>进行本地join，避免进行网络传输产生shuffle。</p>
<p>使用场景：大表join小表 只能广播小表</p>
<h3 id="1-11-27-控制Spark-reduce缓存-调优shuffle"><a href="#1-11-27-控制Spark-reduce缓存-调优shuffle" class="headerlink" title="1.11.27 控制Spark reduce缓存 调优shuffle"></a>1.11.27 控制Spark reduce缓存 调优shuffle</h3><p>spark.reducer.maxSizeInFilght 此参数为reduce<br>task能够拉取多少数据量的一个参数默认48MB，当集群资源足够时，增大此参数可减少reduce拉取数据量的次数，从而达到优化shuffle的效果，一般调大为96MB,资源够大可继续往上跳。</p>
<p>spark.shuffle.file.buffer<br>此参数为每个shuffle文件输出流的内存缓冲区大小，调大此参数可以减少在创建shuffle文件时进行磁盘搜索和系统调用的次数，默认参数为32k<br>一般调大为64k。</p>
<h3 id="1-11-28-注册UDF函数"><a href="#1-11-28-注册UDF函数" class="headerlink" title="1.11.28 注册UDF函数"></a>1.11.28 注册UDF函数</h3><p>SparkSession.udf.register 方法进行注册</p>
<h3 id="1-11-29-SparkSQL中join操作与left-join操作的区别？"><a href="#1-11-29-SparkSQL中join操作与left-join操作的区别？" class="headerlink" title="1.11.29 SparkSQL中join操作与left join操作的区别？"></a>1.11.29 SparkSQL中join操作与left join操作的区别？</h3><p>join和sql中的inner<br>join操作很相似，返回结果是前面一个集合和后面一个集合中匹配成功的，过滤掉关联不上的。</p>
<p>leftJoin类似于SQL中的左外关联left outer<br>join，返回结果以第一个RDD为主，关联不上的记录为空。</p>
<p>部分场景下可以使用left semi join替代left join：</p>
<p>因为 left semi join 是 in(keySet)<br>的关系，<strong>遇到右表重复记录，左表会跳过,性能更高</strong>，而 left join<br>则会一直遍历。但是left semi join 中最后 select<br>的结果中只许出现左表中的列名，因为右表只有 join key 参与关联计算了</p>
<h2 id="1-12-Spark-Streaming"><a href="#1-12-Spark-Streaming" class="headerlink" title="1.12 Spark Streaming"></a>1.12 Spark Streaming</h2><h3 id="1-12-1-Spark-Streaming第一次运行不丢失数据"><a href="#1-12-1-Spark-Streaming第一次运行不丢失数据" class="headerlink" title="1.12.1 Spark Streaming第一次运行不丢失数据"></a>1.12.1 Spark Streaming第一次运行不丢失数据</h3><p>kafka参数 auto.offset.reset 参数设置成earliest 从最初始偏移量开始消费数据</p>
<h3 id="1-12-2-Spark-Streaming精准一次消费"><a href="#1-12-2-Spark-Streaming精准一次消费" class="headerlink" title="1.12.2 Spark Streaming精准一次消费"></a>1.12.2 Spark Streaming精准一次消费</h3><ol>
<li><p> 手动维护偏移量</p>
</li>
<li><p> 处理完业务数据后，再进行提交偏移量操作</p>
</li>
</ol>
<p>极端情况下，如在提交偏移量时断网或停电会造成spark程序第二次启动时重复消费问题，所以在涉及到金额或精确性非常高的场景会使用事物保证精准一次消费</p>
<h3 id="1-12-3-Spark-Streaming控制每秒消费数据的速度"><a href="#1-12-3-Spark-Streaming控制每秒消费数据的速度" class="headerlink" title="1.12.3 Spark Streaming控制每秒消费数据的速度"></a>1.12.3 Spark Streaming控制每秒消费数据的速度</h3><p>通过spark.streaming.kafka.maxRatePerPartition参数来设置Spark<br>Streaming从kafka分区每秒拉取的条数</p>
<h3 id="1-12-4-Spark-Streaming背压机制"><a href="#1-12-4-Spark-Streaming背压机制" class="headerlink" title="1.12.4 Spark Streaming背压机制"></a>1.12.4 Spark Streaming背压机制</h3><p>把spark.streaming.backpressure.enabled 参数设置为ture,开启背压机制后Spark<br>Streaming会根据延迟动态去kafka消费数据,上限由spark.streaming.kafka.maxRatePerPartition参数控制，所以两个参数一般会一起使用</p>
<h3 id="1-12-5-Spark-Streaming-一个stage耗时"><a href="#1-12-5-Spark-Streaming-一个stage耗时" class="headerlink" title="1.12.5 Spark Streaming 一个stage耗时"></a>1.12.5 Spark Streaming 一个stage耗时</h3><p>Spark Streaming<br>stage耗时由最慢的task决定,所以数据倾斜时某个task运行慢会导致整个Spark<br>Streaming都运行非常慢。</p>
<h3 id="1-12-6-Spark-Streaming-优雅关闭"><a href="#1-12-6-Spark-Streaming-优雅关闭" class="headerlink" title="1.12.6 Spark Streaming 优雅关闭"></a>1.12.6 Spark Streaming 优雅关闭</h3><p>把spark.streaming.stopGracefullyOnShutdown参数设置成ture,Spark会在JVM关闭时正常关闭StreamingContext,而不是立马关闭</p>
<p>Kill 命令：yarn application -kill 后面跟 applicationid</p>
<h3 id="1-12-7-Spark-Streaming-默认分区个数"><a href="#1-12-7-Spark-Streaming-默认分区个数" class="headerlink" title="1.12.7 Spark Streaming 默认分区个数"></a>1.12.7 Spark Streaming 默认分区个数</h3><p>Spark Streaming默认分区个数与所对接的kafka topic分区个数一致，Spark<br>Streaming里一般不会使用repartition算子增大分区，因为repartition会进行shuffle增加耗时</p>
<h3 id="1-12-8-SparkStreaming有哪几种方式消费Kafka中的数据，它们之间的区别是什么？"><a href="#1-12-8-SparkStreaming有哪几种方式消费Kafka中的数据，它们之间的区别是什么？" class="headerlink" title="1.12.8 SparkStreaming有哪几种方式消费Kafka中的数据，它们之间的区别是什么？"></a>1.12.8 SparkStreaming有哪几种方式消费Kafka中的数据，它们之间的区别是什么？</h3><p><strong>一、基于Receiver的方式</strong></p>
<p>这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer<br>API来实现的。receiver从Kafka中获取的数据都是存储在Spark<br>Executor的内存中的（如果突然数据暴增，大量batch堆积，很容易出现内存溢出的问题），然后Spark<br>Streaming启动的job会去处理那些数据。</p>
<p>然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark<br>Streaming的预写日志机制（Write Ahead<br>Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。</p>
<p><strong>二、基于Direct的方式</strong></p>
<p>这种新的不基于Receiver的直接方式，是在Spark<br>1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer<br>api来获取Kafka指定offset范围的数据。</p>
<p><strong>优点如下：</strong></p>
<p><strong>简化并行读取：</strong>如果要读取多个partition，不需要创建多个输入DStream然后对它们进行union操作。Spark会创建跟Kafka<br>partition一样多的RDD partition，并且会并行从Kafka中读取数据。所以在Kafka<br>partition和RDD partition之间，有一个一对一的映射关系。</p>
<p><strong>高性能：</strong>如果要保证零数据丢失，在基于receiver的方式中，需要开启WAL机制。这种方式其实效率低下，因为数据实际上被复制了两份，Kafka自己本身就有高可靠的机制，会对数据复制一份，而这里又会复制一份到WAL中。而基于direct的方式，不依赖Receiver，不需要开启WAL机制，只要Kafka中作了数据的复制，那么就可以通过Kafka的副本进行恢复。</p>
<p><strong>一次且仅一次的事务机制</strong>。</p>
<p><strong>三、对比：</strong></p>
<p>基于receiver的方式，是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的。这是消费Kafka数据的传统方式。这种方式配合着WAL机制可以保证数据零丢失的高可靠性，但是却无法保证数据被处理一次且仅一次，可能会处理两次。因为Spark和ZooKeeper之间可能是不同步的。</p>
<p>基于direct的方式，使用kafka的简单api，Spark<br>Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。</p>
<p>在实际生产环境中大都用Direct方式</p>
<h3 id="1-12-9-简述SparkStreaming窗口函数的原理（重点）"><a href="#1-12-9-简述SparkStreaming窗口函数的原理（重点）" class="headerlink" title="1.12.9 简述SparkStreaming窗口函数的原理（重点）"></a>1.12.9 简述SparkStreaming窗口函数的原理（重点）</h3><p>窗口函数就是在原来定义的SparkStreaming计算批次大小的基础上再次进行封装，每次计算多个批次的数据，同时还需要传递一个滑动步长的参数，用来设置当次计算任务完成之后下一次从什么地方开始计算。</p>
<p>图中time1就是SparkStreaming计算批次大小，虚线框以及实线大框就是窗口的大小，必须为批次的整数倍。虚线框到大实线框的距离（相隔多少批次），就是滑动步长。</p>
<h2 id="1-13-数据倾斜"><a href="#1-13-数据倾斜" class="headerlink" title="1.13 数据倾斜"></a>1.13 数据倾斜</h2><p>公司一：总用户量1000万，5台64G内存的服务器。</p>
<p>公司二：总用户量10亿，1000台64G内存的服务器。</p>
<p>1.公司一的数据分析师在做join的时候发生了数据倾斜，会导致有几百万用户的相关数据集中到了一台服务器上，几百万的用户数据，说大也不大，正常字段量的数据的话64G还是能轻松处理掉的。</p>
<p>2.公司二的数据分析师在做join的时候也发生了数据倾斜，可能会有1个亿的用户相关数据集中到了一台机器上了（相信我，这很常见）。这时候一台机器就很难搞定了，最后会很难算出结果。</p>
<h3 id="1-13-1-数据倾斜表现"><a href="#1-13-1-数据倾斜表现" class="headerlink" title="1.13.1 数据倾斜表现"></a>1.13.1 数据倾斜表现</h3><p>1）hadoop中的数据倾斜表现：</p>
<ul>
<li><p>  有一个多几个Reduce卡住，卡在99.99%，一直不能结束。</p>
</li>
<li><p>  各种container报错OOM</p>
</li>
<li><p>  异常的Reducer读写的数据量极大，至少远远超过其它正常的Reducer</p>
</li>
<li><p>  伴随着数据倾斜，会出现任务被kill等各种诡异的表现。</p>
</li>
</ul>
<p>2）hive中数据倾斜</p>
<p>一般都发生在Sql中group by和join on上，而且和数据逻辑绑定比较深。</p>
<p>3）Spark中的数据倾斜</p>
<p>Spark中的数据倾斜，包括Spark Streaming和Spark Sql，表现主要有下面几种：</p>
<ul>
<li><p>  Executor lost，OOM，Shuffle过程出错；</p>
</li>
<li><p>  Driver OOM；</p>
</li>
<li><p>  单个Executor执行时间特别久，整体任务卡在某个阶段不能结束；</p>
</li>
<li><p>  正常运行的任务突然失败；</p>
</li>
</ul>
<h3 id="1-13-2-数据倾斜产生原因"><a href="#1-13-2-数据倾斜产生原因" class="headerlink" title="1.13.2 数据倾斜产生原因"></a>1.13.2 数据倾斜产生原因</h3><p>我们以Spark和Hive的使用场景为例。</p>
<p>他们在做数据运算的时候会涉及到，count distinct、group by、join<br>on等操作，这些都会触发Shuffle动作。一旦触发Shuffle，所有相同key的值就会被拉到一个或几个Reducer节点上，容易发生单点计算问题，导致数据倾斜。</p>
<p>一般来说，数据倾斜原因有以下几方面：</p>
<p><strong>1）key分布不均匀；</strong></p>
<p><img src="media/e4f6302e7d33be29c0c23a12004868bf.png"></p>
<p><strong>2）建表时考虑不周</strong></p>
<p>我们举一个例子，就说数据默认值的设计吧，假设我们有两张表：</p>
<p>user（用户信息表）：userid，register_ip</p>
<p>ip（IP表）：ip，register_user_cnt</p>
<p>这可能是两个不同的人开发的数据表。如果我们的数据规范不太完善的话，会出现一种情况：</p>
<p>user表中的register_ip字段，如果获取不到这个信息，我们默认为null；</p>
<p>但是在ip表中，我们在统计这个值的时候，为了方便，我们把获取不到ip的用户，统一认为他们的ip为0。</p>
<p>两边其实都没有错的，但是一旦我们做关联了，这个任务会在做关联的阶段，也就是sql的on的阶段卡死。</p>
<p><strong>3）业务数据激增</strong></p>
<p>比如订单场景，我们在某一天在北京和上海两个城市多了强力的推广，结果可能是这两个城市的订单量增长了10000%，其余城市的数据量不变。</p>
<p>然后我们要统计不同城市的订单情况，这样，一做group操作，可能直接就数据倾斜了。</p>
<h3 id="1-13-3-解决数据倾斜思路"><a href="#1-13-3-解决数据倾斜思路" class="headerlink" title="1.13.3 解决数据倾斜思路"></a>1.13.3 解决数据倾斜思路</h3><p>很多数据倾斜的问题，都可以用和平台无关的方式解决，比如更好的<strong>数据预处理</strong>，<strong>异常值的过滤</strong>等。因此，解决数据倾斜的重点在于对数据设计和业务的理解，这两个搞清楚了，数据倾斜就解决了大部分了。</p>
<p><strong>1）业务逻辑</strong></p>
<p>我们从业务逻辑的层面上来优化数据倾斜，比如上面的两个城市做推广活动导致那两个城市数据量激增的例子，我们可以单独对这两个城市来做count，单独做时可用两次MR，第一次打散计算，第二次再最终聚合计算。完成后和其它城市做整合。</p>
<p><strong>2）程序层面</strong></p>
<p>比如说在Hive中，经常遇到count(distinct)操作，这样会导致最终只有一个Reduce任务。</p>
<p>我们可以先group<br>by，再在外面包一层count，就可以了。比如计算按用户名去重后的总用户量：</p>
<p>（1）优化前 只有一个reduce，先去重再count负担比较大：</p>
<p>select name,count(distinct name)from user;</p>
<p>（2）优化后</p>
<p>// 设置该任务的每个job的reducer个数为3个。Hive默认-1，自动推断。</p>
<p>set mapred.reduce.tasks=3;</p>
<p>// 启动两个job，一个负责子查询(可以有多个reduce)，另一个负责count(1)：</p>
<p>select count(1) from (select name from user group by name) tmp;</p>
<p><strong>3）调参方面</strong></p>
<p>Hadoop和Spark都自带了很多的参数和机制来调节数据倾斜，合理利用它们就能解决大部分问题。</p>
<p><strong>4）从业务和数据上解决数据倾斜</strong></p>
<p>很多数据倾斜都是在数据的使用上造成的。我们举几个场景，并分别给出它们的解决方案。</p>
<ul>
<li><p>  有损的方法：找到异常数据，比如ip为0的数据，过滤掉</p>
</li>
<li><p>  无损的方法：对分布不均匀的数据，单独计算</p>
</li>
<li><p>  先对key做一层hash，先将数据随机打散让它的并行度变大，再汇集</p>
</li>
<li><p>  数据预处理</p>
</li>
</ul>
<h3 id="1-13-4-定位导致数据倾斜代码"><a href="#1-13-4-定位导致数据倾斜代码" class="headerlink" title="1.13.4 定位导致数据倾斜代码"></a>1.13.4 定位导致数据倾斜代码</h3><p>Spark数据倾斜只会发生在shuffle过程中。</p>
<p>这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。</p>
<p>出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p>
<h4 id="1-13-4-1-某个task执行特别慢的情况"><a href="#1-13-4-1-某个task执行特别慢的情况" class="headerlink" title="1.13.4.1 某个task执行特别慢的情况"></a>1.13.4.1 某个task执行特别慢的情况</h4><p>首先要看的，就是数据倾斜发生在第几个stage中：</p>
<p>如果是用yarn-client模式提交，那么在提交的机器本地是直接可以看到log，可以在log中找到当前运行到了第几个stage；</p>
<p>如果是用yarn-cluster模式提交，则可以通过Spark Web<br>UI来查看当前运行到了第几个stage。</p>
<p>此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web<br>UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p>
<p>看task运行时间和数据量</p>
<p>task运行时间</p>
<p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。</p>
<p>task数据量</p>
<p>此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p>
<p>推断倾斜代码</p>
<p>知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。</p>
<p>精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark<br>SQL的SQL语句中出现了会导致shuffle的语句（比如group<br>by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p>
<p>这里我们就以如下单词计数来举例。</p>
<p>val conf = new SparkConf()val sc = new SparkContext(conf)val lines =<br>sc.textFile(“hdfs://…”)val words = lines.flatMap(<em>.split(“ “))val pairs =<br>words.map((</em>, 1))val wordCounts = pairs.reduceByKey(_ +<br>_)wordCounts.collect().foreach(println(_))</p>
<p>在整个代码中只有一个reduceByKey是会发生shuffle的算子，也就是说这个算子为界限划分出了前后两个stage：</p>
<p>stage0，主要是执行从textFile到map操作，以及shuffle write操作（对pairs<br>RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内）。</p>
<p>stage1，主要是执行从reduceByKey到collect操作，以及stage1的各个task一开始运行，就会首先执行shuffle<br>read操作（会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加）</p>
<p>stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts<br>RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</p>
<p>123456789</p>
<p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。</p>
<p>比如我们在Spark Web<br>UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中，定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是是该算子导致了数据倾斜问题。</p>
<p>此时，如果某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p>
<h4 id="1-13-4-2-某个task莫名其妙内存溢出的情况"><a href="#1-13-4-2-某个task莫名其妙内存溢出的情况" class="headerlink" title="1.13.4.2 某个task莫名其妙内存溢出的情况"></a>1.13.4.2 某个task莫名其妙内存溢出的情况</h4><p>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p>
<p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark<br>Web<br>UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p>
<h3 id="1-13-5-查看导致数据倾斜的key分布情况"><a href="#1-13-5-查看导致数据倾斜的key分布情况" class="headerlink" title="1.13.5 查看导致数据倾斜的key分布情况"></a>1.13.5 查看导致数据倾斜的key分布情况</h3><p>先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p>
<p>val sampledPairs = pairs.sample(false, 0.1)</p>
<p>val sampledWordCounts = sampledPairs.countByKey()</p>
<p>sampledWordCounts.foreach(println(_))</p>
<h3 id="1-13-6-Spark-数据倾斜的解决方案"><a href="#1-13-6-Spark-数据倾斜的解决方案" class="headerlink" title="1.13.6 Spark 数据倾斜的解决方案"></a>1.13.6 Spark 数据倾斜的解决方案</h3><h4 id="1-13-6-1-使用Hive-ETL预处理数据"><a href="#1-13-6-1-使用Hive-ETL预处理数据" class="headerlink" title="1.13.6.1 使用Hive ETL预处理数据"></a>1.13.6.1 使用Hive ETL预处理数据</h4><h5 id="1-13-6-1-1-适用场景"><a href="#1-13-6-1-1-适用场景" class="headerlink" title="1.13.6.1.1 适用场景"></a>1.13.6.1.1 适用场景</h5><p>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p>
<h5 id="1-13-6-1-2-实现思路"><a href="#1-13-6-1-2-实现思路" class="headerlink" title="1.13.6.1.2 实现思路"></a>1.13.6.1.2 实现思路</h5><p>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive<br>ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p>
<h5 id="1-13-6-1-3-方案实现原理"><a href="#1-13-6-1-3-方案实现原理" class="headerlink" title="1.13.6.1.3 方案实现原理"></a>1.13.6.1.3 方案实现原理</h5><p>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive<br>ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive<br>ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive<br>ETL中，避免Spark程序发生数据倾斜而已。</p>
<h5 id="1-13-6-1-4-方案优缺点"><a href="#1-13-6-1-4-方案优缺点" class="headerlink" title="1.13.6.1.4 方案优缺点"></a>1.13.6.1.4 方案优缺点</h5><p>优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p>
<p>缺点：治标不治本，Hive ETL中还是会发生数据倾斜。</p>
<h5 id="1-13-6-1-5-方案实践经验"><a href="#1-13-6-1-5-方案实践经验" class="headerlink" title="1.13.6.1.5 方案实践经验"></a>1.13.6.1.5 方案实践经验</h5><p>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive<br>ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p>
<h5 id="1-13-6-1-6-项目实践经验"><a href="#1-13-6-1-6-项目实践经验" class="headerlink" title="1.13.6.1.6 项目实践经验"></a>1.13.6.1.6 项目实践经验</h5><p>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java<br>Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive<br>ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p>
<h4 id="1-13-6-2-过滤少数导致倾斜的key"><a href="#1-13-6-2-过滤少数导致倾斜的key" class="headerlink" title="1.13.6.2 过滤少数导致倾斜的key"></a>1.13.6.2 过滤少数导致倾斜的key</h4><h5 id="1-13-6-2-1-方案适用场景"><a href="#1-13-6-2-1-方案适用场景" class="headerlink" title="1.13.6.2.1 方案适用场景"></a>1.13.6.2.1 方案适用场景</h5><p>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p>
<h5 id="1-13-6-2-2-方案实现思路"><a href="#1-13-6-2-2-方案实现思路" class="headerlink" title="1.13.6.2.2 方案实现思路"></a>1.13.6.2.2 方案实现思路</h5><p>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。</p>
<p>比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark<br>Core中对RDD执行filter算子过滤掉这些key。</p>
<p>如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p>
<h5 id="1-13-6-2-3-方案实现原理"><a href="#1-13-6-2-3-方案实现原理" class="headerlink" title="1.13.6.2.3 方案实现原理"></a>1.13.6.2.3 方案实现原理</h5><p>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p>
<h5 id="1-13-6-2-4-方案优缺点"><a href="#1-13-6-2-4-方案优缺点" class="headerlink" title="1.13.6.2.4 方案优缺点"></a>1.13.6.2.4 方案优缺点</h5><p>优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p>
<p>缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p>
<h5 id="1-13-6-2-5-方案实践经验"><a href="#1-13-6-2-5-方案实践经验" class="headerlink" title="1.13.6.2.5 方案实践经验"></a>1.13.6.2.5 方案实践经验</h5><p>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p>
<h4 id="1-13-6-3-提高shuffle操作的并行度"><a href="#1-13-6-3-提高shuffle操作的并行度" class="headerlink" title="1.13.6.3 提高shuffle操作的并行度"></a>1.13.6.3 提高shuffle操作的并行度</h4><h5 id="1-13-6-3-1-方案适用场景"><a href="#1-13-6-3-1-方案适用场景" class="headerlink" title="1.13.6.3.1 方案适用场景"></a>1.13.6.3.1 方案适用场景</h5><p>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p>
<h5 id="1-13-6-3-2-方案实现思路"><a href="#1-13-6-3-2-方案实现思路" class="headerlink" title="1.13.6.3.2 方案实现思路"></a>1.13.6.3.2 方案实现思路</h5><p>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle<br>read task的数量，即spark.sql.shuffle.partitions，该参数代表了shuffle read<br>task的并行度，默认是200，对于很多场景来说都有点过小。</p>
<h5 id="1-13-6-3-3-方案实现原理"><a href="#1-13-6-3-3-方案实现原理" class="headerlink" title="1.13.6.3.3 方案实现原理"></a>1.13.6.3.3 方案实现原理</h5><p>增加shuffle read<br>task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。</p>
<p>而增加了shuffle read<br>task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p>
<p><img src="media/66bb55ccd97c524fa13fbf888b94255d.png"></p>
<h5 id="1-13-6-3-4-方案优缺点"><a href="#1-13-6-3-4-方案优缺点" class="headerlink" title="1.13.6.3.4 方案优缺点"></a>1.13.6.3.4 方案优缺点</h5><p>优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p>
<p>缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p>
<h5 id="1-13-6-3-5-方案实践经验"><a href="#1-13-6-3-5-方案实践经验" class="headerlink" title="1.13.6.3.5 方案实践经验"></a>1.13.6.3.5 方案实践经验</h5><p>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p>
<h4 id="1-13-6-4-两阶段聚合（局部聚合-全局聚合）"><a href="#1-13-6-4-两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="1.13.6.4 两阶段聚合（局部聚合+全局聚合）"></a>1.13.6.4 两阶段聚合（局部聚合+全局聚合）</h4><h5 id="1-13-6-4-1-方案适用场景"><a href="#1-13-6-4-1-方案适用场景" class="headerlink" title="1.13.6.4.1 方案适用场景"></a>1.13.6.4.1 方案适用场景</h5><p>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group<br>by语句进行分组聚合时，比较适用这种方案。</p>
<h5 id="1-13-6-4-2-方案实现思路"><a href="#1-13-6-4-2-方案实现思路" class="headerlink" title="1.13.6.4.2 方案实现思路"></a>1.13.6.4.2 方案实现思路</h5><p>这个方案的核心实现思路就是进行两阶段聚合：</p>
<p>第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello,</p>
<ol>
<li>(hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello,</li>
<li>(2_hello, 1)。</li>
</ol>
<p>接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello,<br>2) (2_hello, 2)。</p>
<p>然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello,<br>4)。</p>
<blockquote>
<p>  示例代码如下：</p>
</blockquote>
<p>// 第一步，给RDD中的每个key都打上一个随机前缀。<br>JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(<br>new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple)<br>throws Exception {<br>Random random = new Random();<br>int prefix = random.nextInt(10);<br>return new Tuple2&lt;String, Long&gt;(prefix + “_” + tuple._1, tuple._2);<br>}<br>});  </p>
<p>// 第二步，对打上随机前缀的key进行局部聚合。<br>JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(<br>new Function2&lt;Long, Long, Long&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Long call(Long v1, Long v2) throws Exception {<br>return v1 + v2;<br>}<br>});  </p>
<p>// 第三步，去除RDD中每个key的随机前缀。<br>JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(<br>new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple)<br>throws Exception {<br>long originalKey = Long.valueOf(tuple.<em>1.split(“</em>“)[1]);<br>return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2);<br>}<br>});  </p>
<p>// 第四步，对去除了随机前缀的RDD进行全局聚合。<br>JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(<br>new Function2&lt;Long, Long, Long&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Long call(Long v1, Long v2) throws Exception {<br>return v1 + v2;<br>}<br>});</p>
<h5 id="1-13-6-4-3-方案实现原理"><a href="#1-13-6-4-3-方案实现原理" class="headerlink" title="1.13.6.4.3 方案实现原理"></a>1.13.6.4.3 方案实现原理</h5><p>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p>
<p><img src="media/0413e3961ff5780673df09d46ebc1679.png"></p>
<h5 id="1-13-6-4-4-方案优缺点"><a href="#1-13-6-4-4-方案优缺点" class="headerlink" title="1.13.6.4.4 方案优缺点"></a>1.13.6.4.4 方案优缺点</h5><p>优点<br>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p>
<p>缺点<br>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p>
<h4 id="1-13-6-5-将reduce-join转为map-join"><a href="#1-13-6-5-将reduce-join转为map-join" class="headerlink" title="1.13.6.5 将reduce join转为map join"></a>1.13.6.5 将reduce join转为map join</h4><h5 id="1-13-6-5-1-方案适用场景"><a href="#1-13-6-5-1-方案适用场景" class="headerlink" title="1.13.6.5.1 方案适用场景"></a>1.13.6.5.1 方案适用场景</h5><p>在对RDD使用join类操作，或者是在Spark<br>SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p>
<h5 id="1-13-6-5-2-方案实现思路"><a href="#1-13-6-5-2-方案实现思路" class="headerlink" title="1.13.6.5.2 方案实现思路"></a>1.13.6.5.2 方案实现思路</h5><p>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量，广播给其他Executor节点；</p>
<p>接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p>
<p>示例如下：</p>
<p>// 首先将数据量比较小的RDD的数据，collect到Driver中来。<br>List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()<br>//<br>然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。<br>// 可以尽可能节省内存空间，并且减少网络传输性能开销。<br>final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast =<br>sc.broadcast(rdd1Data);  </p>
<p>// 对另外一个RDD执行map类操作，而不再是join类操作。<br>JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(<br>new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt;<br>tuple)<br>throws Exception {<br>// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。<br>List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();<br>// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。<br>Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;();<br>for(Tuple2&lt;Long, Row&gt; data : rdd1Data) {<br>rdd1DataMap.put(data._1, data._2);<br>}<br>// 获取当前RDD数据的key以及value。<br>String key = tuple._1;<br>String value = tuple._2;<br>// 从rdd1数据Map中，根据key获取到可以join到的数据。<br>Row rdd1Value = rdd1DataMap.get(key);<br>return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value,<br>rdd1Value));<br>}<br>});  </p>
<p>// 这里得提示一下。<br>// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。<br>//<br>如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。<br>// rdd2中每条数据都可能会返回多条join后的数据。</p>
<h5 id="1-13-6-5-3-方案实现原理"><a href="#1-13-6-5-3-方案实现原理" class="headerlink" title="1.13.6.5.3 方案实现原理"></a>1.13.6.5.3 方案实现原理</h5><p>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle<br>read task中再进行join，此时就是reduce join。</p>
<p>但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map<br>join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p>
<p><img src="media/9010c275399ee03d0de99aaae438f409.png"></p>
<h5 id="1-13-6-5-4-方案优缺点"><a href="#1-13-6-5-4-方案优缺点" class="headerlink" title="1.13.6.5.4 方案优缺点"></a>1.13.6.5.4 方案优缺点</h5><p>优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p>
<p>缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p>
<h4 id="1-13-6-6-采样倾斜key并分拆join操作"><a href="#1-13-6-6-采样倾斜key并分拆join操作" class="headerlink" title="1.13.6.6 采样倾斜key并分拆join操作"></a>1.13.6.6 采样倾斜key并分拆join操作</h4><h5 id="1-13-6-6-1-方案适用场景"><a href="#1-13-6-6-1-方案适用场景" class="headerlink" title="1.13.6.6.1 方案适用场景"></a>1.13.6.6.1 方案适用场景</h5><p>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。</p>
<p>如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p>
<h5 id="1-13-6-6-2-方案实现思路"><a href="#1-13-6-6-2-方案实现思路" class="headerlink" title="1.13.6.6.2 方案实现思路"></a>1.13.6.6.2 方案实现思路</h5><p>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</p>
<p>然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀；</p>
<p>而不会导致倾斜的大部分key形成另外一个RDD。</p>
<p>接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀；</p>
<p>不会导致倾斜的大部分key也形成另外一个RDD。</p>
<p>再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。</p>
<p>而另外两个普通的RDD就照常join即可。</p>
<p>最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</p>
<p>示例如下：</p>
<p>// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。<br>JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(false, 0.1);  </p>
<p>// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。<br>// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。<br>// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。  </p>
<p>// 每行数据变为&lt;key,1&gt;<br>JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair(<br>new PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, String&gt; tuple)<br>throws Exception {<br>return new Tuple2&lt;Long, Long&gt;(tuple._1, 1L);<br>}<br>});  </p>
<p>// 按key累加行数<br>JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(<br>new Function2&lt;Long, Long, Long&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Long call(Long v1, Long v2) throws Exception {<br>return v1 + v2;<br>}<br>});  </p>
<p>// 反转key和value,变为&lt;value,key&gt;<br>JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair(<br>new PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple)<br>throws Exception {<br>return new Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1);<br>}<br>});  </p>
<p>// 以行数排序key，取最多行数的key<br>final Long skewedUserid = reversedSampledRDD.sortByKey(false).take(1).get(0)._2;  </p>
<p>// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。<br>JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter(<br>new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception {<br>return tuple._1.equals(skewedUserid);<br>}<br>});  </p>
<p>// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。<br>JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter(<br>new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception {<br>return !tuple._1.equals(skewedUserid);<br>}<br>});  </p>
<p>// rdd2，就是那个所有key的分布相对较为均匀的rdd。<br>//<br>这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。<br>// 对扩容的每条数据，都打上0～100的前缀。<br>JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter(<br>new Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Boolean call(Tuple2&lt;Long, Row&gt; tuple) throws Exception {<br>return tuple.<em>1.equals(skewedUserid);<br>}<br>}).flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(<br>Tuple2&lt;Long, Row&gt; tuple) throws Exception {<br>Random random = new Random();<br>List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();<br>for(int i = 0; i &lt; 100; i++) {<br>list.add(new Tuple2&lt;String, Row&gt;(i + “</em>“ + tuple._1, tuple._2));<br>}<br>return list;<br>}  </p>
<p>});  </p>
<p>// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。<br>// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。<br>JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(<br>new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple)<br>throws Exception {<br>Random random = new Random();<br>int prefix = random.nextInt(100);<br>return new Tuple2&lt;String, String&gt;(prefix + “_” + tuple.<em>1, tuple._2);<br>}<br>})<br>.join(skewedUserid2infoRDD)<br>.mapToPair(new PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long,<br>Tuple2&lt;String, Row&gt;&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call(<br>Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple)<br>throws Exception {<br>long key = Long.valueOf(tuple._1.split(“</em>“)[1]);<br>return new Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2);<br>}<br>});  </p>
<p>// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。<br>JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);  </p>
<p>// 将倾斜key join后的结果与普通key join后的结果，uinon起来。<br>// 就是最终的join结果。<br>JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD =<br>joinedRDD1.union(joinedRDD2);</p>
<h5 id="1-13-6-6-3-方案实现原理"><a href="#1-13-6-6-3-方案实现原理" class="headerlink" title="1.13.6.6.3 方案实现原理"></a>1.13.6.6.3 方案实现原理</h5><p>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p>
<p><img src="media/37dade51eedfda4a2b3f0f776e2017b9.png"></p>
<h5 id="1-13-6-6-4-方案优缺点"><a href="#1-13-6-6-4-方案优缺点" class="headerlink" title="1.13.6.6.4 方案优缺点"></a>1.13.6.6.4 方案优缺点</h5><p>优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p>
<p>缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p>
<h4 id="1-13-6-7-使用随机前缀和扩容RDD进行join"><a href="#1-13-6-7-使用随机前缀和扩容RDD进行join" class="headerlink" title="1.13.6.7 使用随机前缀和扩容RDD进行join"></a>1.13.6.7 使用随机前缀和扩容RDD进行join</h4><h5 id="1-13-6-7-1-方案适用场景"><a href="#1-13-6-7-1-方案适用场景" class="headerlink" title="1.13.6.7.1 方案适用场景"></a>1.13.6.7.1 方案适用场景</h5><p>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p>
<h5 id="1-13-6-7-2-方案实现思路"><a href="#1-13-6-7-2-方案实现思路" class="headerlink" title="1.13.6.7.2 方案实现思路"></a>1.13.6.7.2 方案实现思路</h5><p>该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。</p>
<p>然后将该RDD的每条数据都打上一个n以内的随机前缀。</p>
<p>同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。</p>
<p>最后将两个处理后的RDD进行join即可。</p>
<p>示例代码如下：</p>
<p>// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。<br>JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(<br>new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple)<br>throws Exception {<br>List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();<br>for(int i = 0; i &lt; 100; i++) {<br>list.add(new Tuple2&lt;String, Row&gt;(0 + “_” + tuple._1, tuple._2));<br>}<br>return list;<br>}<br>});  </p>
<p>// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。<br>JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(<br>new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() {<br>private static final long serialVersionUID = 1L;<br>@Override<br>public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple)<br>throws Exception {<br>Random random = new Random();<br>int prefix = random.nextInt(100);<br>return new Tuple2&lt;String, String&gt;(prefix + “_” + tuple._1, tuple._2);<br>}<br>});  </p>
<p>// 将两个处理后的RDD进行join即可。<br>JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD =<br>mappedRDD.join(expandedRDD);</p>
<h5 id="1-13-6-7-3-方案实现原理"><a href="#1-13-6-7-3-方案实现原理" class="headerlink" title="1.13.6.7.3 方案实现原理"></a>1.13.6.7.3 方案实现原理</h5><p>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。</p>
<p>该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；</p>
<p>而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p>
<h5 id="1-13-6-7-4-方案优缺点"><a href="#1-13-6-7-4-方案优缺点" class="headerlink" title="1.13.6.7.4 方案优缺点"></a>1.13.6.7.4 方案优缺点</h5><p>优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p>
<p>缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p>
<h5 id="1-13-6-7-5-方案实践经验"><a href="#1-13-6-7-5-方案实践经验" class="headerlink" title="1.13.6.7.5 方案实践经验"></a>1.13.6.7.5 方案实践经验</h5><p>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p>
<h4 id="1-13-6-8-多种方案组合使用"><a href="#1-13-6-8-多种方案组合使用" class="headerlink" title="1.13.6.8 多种方案组合使用"></a>1.13.6.8 多种方案组合使用</h4><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。</p>
<p>比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一HiveETL预处理和过滤少数导致倾斜的k，预处理一部分数据，并过滤一部分数据来缓解；</p>
<p>其次可以对某些shuffle操作提升并行度，优化其性能；</p>
<p>最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。</p>
<p>大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p>
<h3 id="1-13-7-Spark数据倾斜处理小结"><a href="#1-13-7-Spark数据倾斜处理小结" class="headerlink" title="1.13.7 Spark数据倾斜处理小结"></a>1.13.7 Spark数据倾斜处理小结</h3><p><img src="media/7f8113d89efd447800e3120aaa6d39e0.png"></p>
<h2 id="1-14-Flink"><a href="#1-14-Flink" class="headerlink" title="1.14 Flink"></a>1.14 Flink</h2><h3 id="1-14-1-简单介绍一下-Flink"><a href="#1-14-1-简单介绍一下-Flink" class="headerlink" title="1.14.1 简单介绍一下 Flink"></a>1.14.1 简单介绍一下 Flink</h3><p>Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。并且<br>Flink<br>提供了数据分布、容错机制以及资源管理等核心功能。Flink提供了诸多高抽象层的API以便用户编写分布式任务：</p>
<p>DataSet API，<br>对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。</p>
<p>DataStream<br>API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala。</p>
<p>Table<br>API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala。</p>
<p>此外，Flink 还针对特定的应用领域提供了领域库，例如： Flink ML，Flink<br>的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法。 Gelly，Flink<br>的图计算库，提供了图计算的相关API及多种图计算算法实现。</p>
<h3 id="1-14-2-Flink跟Spark-Streaming的区别"><a href="#1-14-2-Flink跟Spark-Streaming的区别" class="headerlink" title="1.14.2 Flink跟Spark Streaming的区别"></a>1.14.2 Flink跟Spark Streaming的区别</h3><p>这个问题是一个非常宏观的问题，因为两个框架的不同点非常之多。但是在面试时有非常重要的一点一定要回答出来：<strong>Flink<br>是标准的实时处理引擎，基于事件驱动。而 Spark Streaming<br>是微批（Micro-Batch）的模型。</strong></p>
<p>下面我们就分几个方面介绍两个框架的主要区别：</p>
<p>1）架构模型Spark Streaming<br>在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink<br>在运行时主要包含：Jobmanager、Taskmanager和Slot。</p>
<p>2）任务调度Spark Streaming<br>连续不断的生成微小的数据批次，构建有向无环图DAG，Spark Streaming 会依次创建<br>DStreamGraph、JobGenerator、JobScheduler。Flink 根据用户提交的代码生成<br>StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager<br>会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink<br>调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。</p>
<p>3）时间机制Spark Streaming 支持的时间机制有限，只支持处理时间。 Flink<br>支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持<br>watermark 机制来处理滞后数据。</p>
<p>4）容错机制对于 Spark Streaming 任务，我们可以设置<br>checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint<br>之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰好一次处理语义。Flink<br>则使用两阶段提交协议来解决这个问题。</p>
<h3 id="1-14-3-Flink集群有哪些角色？各自有什么作用？"><a href="#1-14-3-Flink集群有哪些角色？各自有什么作用？" class="headerlink" title="1.14.3 Flink集群有哪些角色？各自有什么作用？"></a>1.14.3 Flink集群有哪些角色？各自有什么作用？</h3><p><img src="media/db329173f1e4081c8e06383fdf064e32.png"></p>
<p>Flink 程序在运行时主要有 TaskManager，JobManager，Client三种角色。</p>
<p>JobManager扮演着集群中的管理者Master的角色，它是整个集群的协调者，负责接收Flink<br>Job，协调检查点，Failover 故障恢复等，同时管理Flink集群中从节点TaskManager。</p>
<p>TaskManager是实际负责执行计算的Worker，在其上执行Flink<br>Job的一组Task，每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。</p>
<p>Client是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink<br>Job提交给JobManager。</p>
<h3 id="1-14-4-公司怎么提交的实时任务，有多少-Job-Manager？"><a href="#1-14-4-公司怎么提交的实时任务，有多少-Job-Manager？" class="headerlink" title="1.14.4 公司怎么提交的实时任务，有多少 Job Manager？"></a>1.14.4 公司怎么提交的实时任务，有多少 Job Manager？</h3><p>1）我们使用yarn session模式提交任务；另一种方式是每次提交都会创建一个新的 Flink<br>集群，为每一个 job<br>提供资源，任务之间互相独立，互不影响，方便管理。任务执行完成之后创建的集群也会消失。线上命令脚本如下：</p>
<p>bin/yarn-session.sh -n 7 -s 8 -jm 3072 -tm 32768 -qu root.*.* -nm *-* -d</p>
<p>其中申请 7 个 taskManager，每个 8 核，每个 taskmanager 有 32768M 内存。</p>
<p>2）集群默认只有一个 Job<br>Manager。但为了防止单点故障，我们配置了高可用。对于standlone模式，我们公司一般配置一个主<br>Job Manager，两个备用 Job Manager，然后结合 ZooKeeper<br>的使用，来达到高可用；对于yarn模式，yarn在Job<br>Mananger故障会自动进行重启，所以只需要一个，我们配置的最大重启次数是10次。</p>
<h3 id="1-14-5-Flink的并行度了解吗？Flink的并行度设置是怎样的？"><a href="#1-14-5-Flink的并行度了解吗？Flink的并行度设置是怎样的？" class="headerlink" title="1.14.5 Flink的并行度了解吗？Flink的并行度设置是怎样的？"></a>1.14.5 Flink的并行度了解吗？Flink的并行度设置是怎样的？</h3><p>Flink中的任务被分为多个并行任务来执行，其中每个并行的实例处理一部分数据。这些并行实例的数量被称为并行度。我们在实际生产环境中可以从四个不同层面设置并行度：</p>
<p>操作算子层面(Operator Level)</p>
<p>执行环境层面(Execution Environment Level)</p>
<p>客户端层面(Client Level)</p>
<p>系统层面(System Level)</p>
<p>需要注意的优先级：算子层面&gt;环境层面&gt;客户端层面&gt;系统层面。</p>
<h3 id="1-14-6-Flink的Checkpoint-存在哪里"><a href="#1-14-6-Flink的Checkpoint-存在哪里" class="headerlink" title="1.14.6 Flink的Checkpoint 存在哪里"></a>1.14.6 Flink的Checkpoint 存在哪里</h3><p>可以是内存，文件系统，或者 RocksDB。</p>
<h3 id="1-14-7-Flink的三种时间语义"><a href="#1-14-7-Flink的三种时间语义" class="headerlink" title="1.14.7 Flink的三种时间语义"></a>1.14.7 Flink的三种时间语义</h3><p>Event<br>Time：是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink通过时间戳分配器访问事件时间戳。</p>
<p>Ingestion Time：是数据进入Flink的时间。</p>
<p>Processing<br>Time：是每一个执行基于时间操作的算子的本地系统时间，与机器相关，默认的时间属性就是Processing<br>Time。</p>
<h3 id="1-14-8-说说Flink中的窗口"><a href="#1-14-8-说说Flink中的窗口" class="headerlink" title="1.14.8 说说Flink中的窗口"></a>1.14.8 说说Flink中的窗口</h3><p>来一张官网经典的图：</p>
<p><img src="media/d604970d4df0a3b53ee3176700184a3a.png"></p>
<p>Flink<br>支持两种划分窗口的方式，按照time和count。如果根据时间划分窗口，那么它就是一个time-window<br>如果根据数据划分窗口，那么它就是一个count-window。flink支持窗口的两个重要属性（size和interval）如果size=interval,那么就会形成tumbling-window(无重叠数据)<br>如果size&gt;interval,那么就会形成sliding-window(有重叠数据) 如果size&lt; interval,<br>那么这种窗口将会丢失数据。比如每5秒钟，统计过去3秒的通过路口汽车的数据，将会漏掉2秒钟的数据。通过组合可以得出四种基本窗口：</p>
<p>time-tumbling-window<br>无重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5))</p>
<p>time-sliding-window<br>有重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5), Time.seconds(3))</p>
<p>count-tumbling-window无重叠数据的数量窗口，设置方式举例：countWindow(5)</p>
<p>count-sliding-window 有重叠数据的数量窗口，设置方式举例：countWindow(5,3)</p>
<h3 id="1-14-9-Exactly-Once的保证"><a href="#1-14-9-Exactly-Once的保证" class="headerlink" title="1.14.9 Exactly-Once的保证"></a>1.14.9 Exactly-Once的保证</h3><p>下级存储支持事务：Flink可以通过实现两阶段提交和状态保存来实现端到端的一致性语义。<br>分为以下几个步骤：</p>
<p>1）开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面</p>
<p>2）预提交（preCommit）将内存中缓存的数据写入文件并关闭</p>
<p>3）正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟</p>
<p>4）丢弃（abort）丢弃临时文件</p>
<p>5）若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。</p>
<p>下级存储不支持事务：</p>
<p>具体实现是幂等写入，需要下级存储具有幂等性写入特性。</p>
<h3 id="1-14-10-说一下Flink状态机制"><a href="#1-14-10-说一下Flink状态机制" class="headerlink" title="1.14.10 说一下Flink状态机制"></a>1.14.10 说一下Flink状态机制</h3><p>Flink在做计算的过程中经常需要存储中间状态，来避免数据丢失和状态恢复。选择的状态存储策略不同，会影响状态持久化如何和<br>checkpoint 交互。</p>
<p>Flink提供了三种状态存储方式：MemoryStateBackend、FsStateBackend、RocksDBStateBackend。</p>
<h3 id="1-14-11-Flink-中的Watermark机制"><a href="#1-14-11-Flink-中的Watermark机制" class="headerlink" title="1.14.11 Flink 中的Watermark机制"></a>1.14.11 Flink 中的Watermark机制</h3><p>Watermark 是一种衡量 Event Time 进展的机制，可以设定延迟触发</p>
<p>Watermark 是用于处理乱序事件的，而正确的处理乱序事件，通常用Watermark 机制结合<br>window 来实现；</p>
<p>数据流中的 Watermark 用于表示 timestamp 小于 Watermark<br>的数据，都已经到达了，因此，window 的执行也是由 Watermark 触发的。</p>
<h3 id="1-14-12-Flink分布式快照的原理是什么"><a href="#1-14-12-Flink分布式快照的原理是什么" class="headerlink" title="1.14.12 Flink分布式快照的原理是什么"></a>1.14.12 Flink分布式快照的原理是什么</h3><p>Flink的容错机制的核心部分是制作分布式数据流和操作算子状态的一致性快照。<br>这些快照充当一致性checkpoint，系统可以在发生故障时回滚。<br>Flink用于制作这些快照的机制在“分布式数据流的轻量级异步快照”中进行了描述。<br>它受到分布式快照的标准Chandy-Lamport算法的启发，专门针对Flink的执行模型而定制。</p>
<p><img src="media/fe245e67357ede3ce9710cbb8bbd6574.png"></p>
<p>barriers在数据流源处被注入并行数据流中。快照n的barriers被插入的位置（我们称之为Sn）是快照所包含的数据在数据源中最大位置。</p>
<p>例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。<br>将该位置Sn报告给checkpoint协调器（Flink的JobManager）。</p>
<p>然后barriers向下游流动。当一个中间操作算子从其所有输入流中收到快照n的barriers时，它会为快照n发出barriers进入其所有输出流中。</p>
<p>一旦sink操作算子（流式DAG的末端）从其所有输入流接收到barriers<br>n，它就向checkpoint协调器确认快照n完成。</p>
<p>在所有sink确认快照后，意味快照着已完成。一旦完成快照n，job将永远不再向数据源请求Sn之前的记录，因为此时这些记录（及其后续记录）将已经通过整个数据流拓扑，也即是已经被处理结束。</p>
<h3 id="1-14-13-介绍一下Flink的CEP机制"><a href="#1-14-13-介绍一下Flink的CEP机制" class="headerlink" title="1.14.13 介绍一下Flink的CEP机制"></a>1.14.13 介绍一下Flink的CEP机制</h3><p>CEP全称为Complex Event Processing，复杂事件处理</p>
<p>Flink CEP是在 Flink 中实现的复杂事件处理（CEP）库</p>
<p>CEP 允许在无休止的事件流中检测事件模式，让我们有机会掌握数据中重要的部分</p>
<p>一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据 ——<br>满足规则的复杂事件</p>
<h3 id="1-14-14-Flink-CEP-编程中当状态没有到达的时候会将数据保存在哪里？"><a href="#1-14-14-Flink-CEP-编程中当状态没有到达的时候会将数据保存在哪里？" class="headerlink" title="1.14.14 Flink CEP 编程中当状态没有到达的时候会将数据保存在哪里？"></a>1.14.14 Flink CEP 编程中当状态没有到达的时候会将数据保存在哪里？</h3><p>在流式处理中，CEP 当然是要支持 EventTime<br>的，那么相对应的也要支持数据的迟到现象，也就是watermark的处理逻辑。CEP对未匹配成功的事件序列的处理，和迟到数据是类似的。在<br>Flink<br>CEP的处理逻辑中，状态没有满足的和迟到的数据，都会存储在一个Map数据结构中，也就是说，如果我们限定判断事件序列的时长为5分钟，那么内存中就会存储5分钟的数据，这在我看来，也是对内存的极大损伤之一。</p>
<h1 id="第2章-项目架构"><a href="#第2章-项目架构" class="headerlink" title="第2章 项目架构"></a>第2章 项目架构</h1><h2 id="2-1-提高自信"><a href="#2-1-提高自信" class="headerlink" title="2.1 提高自信"></a>2.1 提高自信</h2><p>云上数据仓库解决方案：<a target="_blank" rel="noopener" href="https://www.aliyun.com/solution/datavexpo/datawarehouse">https://www.aliyun.com/solution/datavexpo/datawarehouse</a></p>
<p><img src="media/0fb6dc419edbde5867e3171c4baa6c4e.png"></p>
<p><img src="media/61dee637e5674e3b670909df7b51d907.png"></p>
<h2 id="2-2-数仓概念"><a href="#2-2-数仓概念" class="headerlink" title="2.2 数仓概念"></a>2.2 数仓概念</h2><p><strong>数据仓库的输入数据源和输出系统分别是什么？</strong></p>
<p>输入系统：埋点产生的用户行为数据、JavaEE后台产生的业务数据、个别公司有爬虫数据。</p>
<p>输出系统：报表系统、用户画像系统、推荐系统</p>
<h2 id="2-3-系统数据流程设计"><a href="#2-3-系统数据流程设计" class="headerlink" title="2.3 系统数据流程设计"></a>2.3 系统数据流程设计</h2><h2 id="2-4-框架版本选型"><a href="#2-4-框架版本选型" class="headerlink" title="2.4 框架版本选型"></a>2.4 框架版本选型</h2><p>1）Apache：运维麻烦，组件间兼容性需要自己调研。（一般大厂使用，技术实力雄厚，有专业的运维人员）</p>
<p>2）CDH6.3.2：国内使用最多的版本，但<br>CM不开源，但其实对中、小公司使用来说没有影响（建议使用）10000美金一个节点 CDP7.0</p>
<p>3）HDP2.7：开源，可以进行二次开发，但是没有CDH稳定，国内使用较少</p>
<h2 id="2-5-服务器选型"><a href="#2-5-服务器选型" class="headerlink" title="2.5 服务器选型"></a>2.5 服务器选型</h2><p>服务器使用物理机还是云主机？</p>
<p>1）机器成本考虑：</p>
<p>（1）物理机：以128G内存，20核物理CPU，40线程，8THDD和2TSSD硬盘，单台报价4W出头，惠普品牌。一般物理机寿命5年左右。</p>
<p>（2）云主机，以阿里云为例，差不多相同配置，每年5W</p>
<p>2）运维成本考虑：</p>
<p>（1）物理机：需要有专业的运维人员（1万*13个月）、电费（商业用户）、安装空调</p>
<p>（2）云主机：很多运维工作都由阿里云已经完成，运维相对较轻松</p>
<p>3）企业选择</p>
<p>（1）金融有钱公司和阿里没有直接冲突的公司选择阿里云（上海）</p>
<p>（2）中小公司、为了融资上市，选择阿里云，拉倒融资后买物理机。</p>
<p>（3）有长期打算，资金比较足，选择物理机。</p>
<h2 id="2-6-集群规模"><a href="#2-6-集群规模" class="headerlink" title="2.6 集群规模"></a>2.6 集群规模</h2><p>20核物理CPU 40线程 * 7 = 280线程</p>
<p>内存128g * 7台 = 896g （计算任务内存700g，其他安装框架需要内存）</p>
<p>128m =》1g内存</p>
<p>=》</p>
<p>87g数据 、700g内存</p>
<p>根据数据规模大家集群</p>
<table>
<thead>
<tr>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody><tr>
<td>nn</td>
<td>nn</td>
<td>dn</td>
<td>dn</td>
<td>dn</td>
<td>dn</td>
<td>dn</td>
<td>dn</td>
<td>dn</td>
<td>dn</td>
</tr>
<tr>
<td></td>
<td></td>
<td>rm</td>
<td>rm</td>
<td>nm</td>
<td>nm</td>
<td>nm</td>
<td>nm</td>
<td>nm</td>
<td>nm</td>
</tr>
<tr>
<td></td>
<td></td>
<td>nm</td>
<td>nm</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>zk</td>
<td>zk</td>
<td>zk</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>kafka</td>
<td>kafka</td>
<td>kafka</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Flume</td>
<td>Flume</td>
<td>flume</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Hbase</td>
<td>Hbase</td>
<td>Hbase</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>hive</td>
<td>hive</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>mysql</td>
<td>mysql</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>spark</td>
<td>spark</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>ES</td>
<td>ES</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>1）消耗内存的分开；</p>
<p>2）kafka 、zk 、flume 传输数据比较紧密的放在一起；</p>
<p>3）客户端尽量放在一到两台服务器上，方便外部访问；</p>
<h2 id="2-7-人员配置参考"><a href="#2-7-人员配置参考" class="headerlink" title="2.7 人员配置参考"></a>2.7 人员配置参考</h2><h3 id="2-7-1-整体架构"><a href="#2-7-1-整体架构" class="headerlink" title="2.7.1 整体架构"></a>2.7.1 整体架构</h3><p>属于<strong>研发部</strong>/技术部/数据部，我们属于<strong>大数据组</strong>，其他还有后端项目组，前端组、测试组、UI组等。其他的还有产品部、运营部、人事部、财务部、行政部等。</p>
<p>大数据开发工程师=&gt;大数据组组长=》项目经理=&gt;部门经理=》技术总监CTO</p>
<h3 id="2-7-2-你们部门的职级等级，晋升规则"><a href="#2-7-2-你们部门的职级等级，晋升规则" class="headerlink" title="2.7.2 你们部门的职级等级，晋升规则"></a>2.7.2 你们部门的职级等级，晋升规则</h3><p>职级就分初级，中级，高级。晋升规则不一定，看公司效益和职位空缺。</p>
<p>京东：T1、T2应届生；T3 14k左右 T4 18K左右 T5 24k-28k左右</p>
<p>阿里：p5、p6、p7、p8</p>
<h3 id="2-7-3-人员配置参考"><a href="#2-7-3-人员配置参考" class="headerlink" title="2.7.3 人员配置参考"></a>2.7.3 人员配置参考</h3><p>小型公司（3人左右）：组长1人，剩余组员无明确分工，并且可能兼顾javaEE和前端。</p>
<p>中小型公司（3~6人左右）：组长1人，离线2人左右，实时1人左右（离线一般多于实时），组长兼顾和javaEE、前端。</p>
<p>中型公司（5~10人左右）：组长1人，离线3~5人左右（离线处理、数仓），实时2人左右，组长和技术大牛兼顾和javaEE、前端。</p>
<p>中大型公司（10~20人左右）：组长1人，离线5~10人（离线处理、数仓），实时5人左右，JavaEE1人左右（负责对接JavaEE业务），前端1人（有或者没有人单独负责前端）。（发展比较良好的中大型公司可能大数据部门已经细化拆分，分成多个大数据组，分别负责不同业务）</p>
<p>上面只是参考配置，因为公司之间差异很大，例如ofo大数据部门只有5个人左右，因此根据所选公司规模确定一个合理范围，在面试前必须将这个人员配置考虑清楚，回答时要非常确定。</p>
<p>IOS多少人 安卓多少人 前端多少人 JavaEE多少人 测试多少人</p>
<p>（IOS、安卓） 1-2个人 前端1-3个人；<br>JavaEE一般是大数据的1-1.5倍，测试：有的有，有的没有。1个左右。<br>产品经理1个、产品助理1-2个，运营1-3个</p>
<p>公司划分：</p>
<blockquote>
<p>  0-50 小公司</p>
</blockquote>
<blockquote>
<p>  50-500 中等</p>
</blockquote>
<blockquote>
<p>  500-1000 大公司</p>
</blockquote>
<blockquote>
<p>  1000以上 大厂 领军的存在</p>
</blockquote>
<h1 id="第3章-数仓分层"><a href="#第3章-数仓分层" class="headerlink" title="第3章 数仓分层"></a>第3章 数仓分层</h1><h2 id="3-1-数据仓库建模（绝对重点）"><a href="#3-1-数据仓库建模（绝对重点）" class="headerlink" title="3.1 数据仓库建模（绝对重点）"></a>3.1 数据仓库建模（绝对重点）</h2><h3 id="3-1-1-建模工具是什么？"><a href="#3-1-1-建模工具是什么？" class="headerlink" title="3.1.1 建模工具是什么？"></a>3.1.1 建模工具是什么？</h3><p>PowerDesigner/SQLYog/EZDML</p>
<h3 id="3-1-2-ODS层"><a href="#3-1-2-ODS层" class="headerlink" title="3.1.2 ODS层"></a>3.1.2 ODS层</h3><p>（1）保持数据原貌不做任何修改，起到备份数据的作用。</p>
<p>（2）数据采用压缩，减少磁盘存储空间（例如：原始数据100G，可以压缩到10G左右）</p>
<p>（3）创建分区表，防止后续的全表扫描</p>
<h3 id="3-1-3-DWD层"><a href="#3-1-3-DWD层" class="headerlink" title="3.1.3 DWD层"></a>3.1.3 DWD层</h3><p>DWD层需构建维度模型，一般采用星型模型，呈现的状态一般为星座模型。</p>
<p>维度建模一般按照以下四个步骤：</p>
<p><strong>选择业务过程→声明粒度→确认维度→确认事实</strong></p>
<p><strong>（1）选择业务过程</strong></p>
<p>在业务系统中，如果业务表过多，挑选我们感兴趣的业务线，比如下单业务，支付业务，退款业务，物流业务，一条业务线对应一张事实表。如果小公司业务表比较少，建议选择所有业务线。</p>
<p><strong>（2）声明粒度</strong></p>
<p>数据粒度指数据仓库的数据中保存数据的细化程度或综合程度的级别。</p>
<p>声明粒度意味着精确定义事实表中的一行数据表示什么，应该尽可能选择最小粒度，以此来应各种各样的需求。</p>
<p><strong>典型的粒度声明如下：</strong></p>
<p>订单当中的每个商品项作为下单事实表中的一行，粒度为每次</p>
<p>每周的订单次数作为一行，粒度为每周。</p>
<p>每月的订单次数作为一行，粒度为每月。</p>
<p>如果在DWD层粒度就是每周或者每月，那么后续就没有办法统计细粒度的指标了。所有建议采用最小粒度。</p>
<p><strong>（3）确定维度</strong></p>
<p>维度的主要作用是描述业务是事实，主要表示的是“谁，何处，何时”等信息。例如：时间维度、用户维度、地区维度等常见维度。</p>
<p><strong>（4）确定事实</strong></p>
<p>此处的“事实”一词，指的是业务中的度量值，例如订单金额、下单次数等。</p>
<p>在DWD层，以<strong>业务过程</strong>为建模驱动，基于每个具体业务过程的特点，构建<strong>最细粒度</strong>的明细层事实表。事实表可做适当的宽表化处理。</p>
<p>通过以上步骤，结合本数仓的业务事实，得出业务总线矩阵表如下表所示。业务总线矩阵的原则，主要是根据维度表和事实表之间的关系，如果两者有关联则使用√标记。</p>
<p>表 业务总线矩阵表</p>
<table>
<thead>
<tr>
<th></th>
<th>时间</th>
<th>用户</th>
<th>地区</th>
<th>商品</th>
<th>优惠券</th>
<th>活动</th>
<th>编码</th>
<th>度量值</th>
</tr>
</thead>
<tbody><tr>
<td>订单</td>
<td>√</td>
<td>√</td>
<td>√</td>
<td></td>
<td></td>
<td>√</td>
<td></td>
<td>件数/金额</td>
</tr>
<tr>
<td>订单详情</td>
<td>√</td>
<td></td>
<td>√</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
<td>件数/金额</td>
</tr>
<tr>
<td>支付</td>
<td>√</td>
<td></td>
<td>√</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>次数/金额</td>
</tr>
<tr>
<td>加购</td>
<td>√</td>
<td>√</td>
<td></td>
<td>√</td>
<td></td>
<td></td>
<td></td>
<td>件数/金额</td>
</tr>
<tr>
<td>收藏</td>
<td>√</td>
<td>√</td>
<td></td>
<td>√</td>
<td></td>
<td></td>
<td></td>
<td>个数</td>
</tr>
<tr>
<td>评价</td>
<td>√</td>
<td>√</td>
<td></td>
<td>√</td>
<td></td>
<td></td>
<td></td>
<td>个数</td>
</tr>
<tr>
<td>退款</td>
<td>√</td>
<td>√</td>
<td></td>
<td>√</td>
<td></td>
<td></td>
<td></td>
<td>件数/金额</td>
</tr>
<tr>
<td>优惠券领用</td>
<td>√</td>
<td>√</td>
<td></td>
<td></td>
<td>√</td>
<td></td>
<td></td>
<td>个数</td>
</tr>
</tbody></table>
<p>根据维度建模中的星型模型思想，将维度进行退化。例如下图所示：地区表和省份表退化为地区维度表，商品表、品类表、spu表、商品三级分类、商品二级分类、商品一级分类表退化为商品维度表，活动信息表和活动规则表退化为活动维度表。</p>
<p><img src="media/8fe184a75c53c34a7930305d76d2bb27.png"></p>
<p>至此，数仓的维度建模已经完毕，DWS、DWT和ADS和维度建模已经没有关系了。</p>
<p>DWS和DWT都是建宽表，宽表都是按照主题去建。主题相当于观察问题的角度。对应着维度表。</p>
<h3 id="3-1-4-DWS层"><a href="#3-1-4-DWS层" class="headerlink" title="3.1.4 DWS层"></a>3.1.4 DWS层</h3><p>DWS层统计各个主题对象的当天行为，服务于DWT层的主题宽表。如图所示，DWS层的宽表字段，是站在不同维度的视角去看事实表，重点关注事实表的度量值，通过与之关联的事实表，获得不同的事实表的度量值。</p>
<p><img src="media/254f0eb40a8434abd9e2a7f1dd62e1f0.png"></p>
<h3 id="3-1-5-DWT层"><a href="#3-1-5-DWT层" class="headerlink" title="3.1.5 DWT层"></a>3.1.5 DWT层</h3><p>以分析的<strong>主题对象</strong>为建模驱动，基于上层的应用和产品的指标需求，构建主题对象的全量宽表。</p>
<p>DWT层主题宽表都记录什么字段？</p>
<p>如图所示，每个维度关联的不同事实表度量值以及首次、末次时间、累积至今的度量值、累积某个时间段的度量值。</p>
<p><img src="media/79b7ad6ae2caacb327e961afbfd86088.png"></p>
<h3 id="3-1-6-ADS层"><a href="#3-1-6-ADS层" class="headerlink" title="3.1.6 ADS层"></a>3.1.6 ADS层</h3><p>分别对设备主题、会员主题、商品主题和营销主题进行指标分析，其中营销主题是用户主题和商品主题的跨主题分析案例</p>
<h2 id="3-2-ODS层做了哪些事？"><a href="#3-2-ODS层做了哪些事？" class="headerlink" title="3.2 ODS层做了哪些事？"></a>3.2 ODS层做了哪些事？</h2><p>1）保持数据原貌，不做任何修改</p>
<p>2）压缩采用LZO，压缩比是100g数据压缩完10g左右。</p>
<p>3）创建分区表</p>
<h2 id="3-3-DWD层做了哪些事？"><a href="#3-3-DWD层做了哪些事？" class="headerlink" title="3.3 DWD层做了哪些事？"></a>3.3 DWD层做了哪些事？</h2><h3 id="3-3-1-数据清洗"><a href="#3-3-1-数据清洗" class="headerlink" title="3.3.1 数据清洗"></a>3.3.1 数据清洗</h3><blockquote>
<p>  （1）空值去除</p>
</blockquote>
<blockquote>
<p>  （2）过滤核心字段无意义的数据，比如订单表中订单id为null，支付表中支付id为空</p>
</blockquote>
<blockquote>
<p>  （3）将用户行为宽表和业务表进行数据一致性处理</p>
</blockquote>
<blockquote>
<p>  select case when a is null then b else a end as JZR,</p>
</blockquote>
<blockquote>
<p>  …</p>
</blockquote>
<blockquote>
<p>  from A</p>
</blockquote>
<h3 id="3-3-2-清洗的手段"><a href="#3-3-2-清洗的手段" class="headerlink" title="3.3.2 清洗的手段"></a>3.3.2 清洗的手段</h3><p>Sql、mr、rdd、kettle、Python（项目中采用sql进行清除）</p>
<h3 id="3-3-3-清洗掉多少数据算合理"><a href="#3-3-3-清洗掉多少数据算合理" class="headerlink" title="3.3.3 清洗掉多少数据算合理"></a>3.3.3 清洗掉多少数据算合理</h3><p>1万条数据清洗掉1条。</p>
<h3 id="3-3-4-脱敏"><a href="#3-3-4-脱敏" class="headerlink" title="3.3.4 脱敏"></a>3.3.4 脱敏</h3><blockquote>
<p>  对手机号、身份证号等敏感数据脱敏</p>
</blockquote>
<h3 id="3-3-5-维度退化"><a href="#3-3-5-维度退化" class="headerlink" title="3.3.5 维度退化"></a>3.3.5 维度退化</h3><blockquote>
<p>  对业务数据传过来的表进行维度退化和降维。（商品一级二级三级、省市县、年月日）</p>
</blockquote>
<h3 id="3-3-6-压缩LZO"><a href="#3-3-6-压缩LZO" class="headerlink" title="3.3.6 压缩LZO"></a>3.3.6 压缩LZO</h3><h3 id="3-3-7-列式存储parquet"><a href="#3-3-7-列式存储parquet" class="headerlink" title="3.3.7 列式存储parquet"></a>3.3.7 列式存储parquet</h3><h2 id="3-4-DWS层做了哪些事？"><a href="#3-4-DWS层做了哪些事？" class="headerlink" title="3.4 DWS层做了哪些事？"></a>3.4 DWS层做了哪些事？</h2><h3 id="3-4-1-DWS层有3-5张宽表（处理100-200个指标-70-以上的需求）"><a href="#3-4-1-DWS层有3-5张宽表（处理100-200个指标-70-以上的需求）" class="headerlink" title="3.4.1 DWS层有3-5张宽表（处理100-200个指标 70%以上的需求）"></a>3.4.1 DWS层有3-5张宽表（处理100-200个指标 70%以上的需求）</h3><p>具体宽表名称：用户行为宽表，用户购买商品明细行为宽表，商品宽表，购物车宽表，物流宽表、登录注册、售后等。</p>
<h3 id="3-4-2-哪个宽表最宽？大概有多少个字段？"><a href="#3-4-2-哪个宽表最宽？大概有多少个字段？" class="headerlink" title="3.4.2 哪个宽表最宽？大概有多少个字段？"></a>3.4.2 哪个宽表最宽？大概有多少个字段？</h3><p>最宽的是用户行为宽表。大概有60-100个字段</p>
<h3 id="3-4-3-具体用户行为宽表字段名称"><a href="#3-4-3-具体用户行为宽表字段名称" class="headerlink" title="3.4.3 具体用户行为宽表字段名称"></a>3.4.3 具体用户行为宽表字段名称</h3><p>评论、打赏、收藏、关注–商品、关注–人、点赞、分享、好价爆料、文章发布、活跃、签到、补签卡、幸运屋、礼品、金币、电商点击、gmv</p>
<blockquote>
<p>  CREATE TABLE `app_usr_interact`(</p>
</blockquote>
<blockquote>
<p>  `stat_dt` date COMMENT ‘互动日期’,</p>
</blockquote>
<blockquote>
<p>  `user_id` string COMMENT ‘用户id’,</p>
</blockquote>
<blockquote>
<p>  `nickname` string COMMENT ‘用户昵称’,</p>
</blockquote>
<blockquote>
<p>  `register_date` string COMMENT ‘注册日期’,</p>
</blockquote>
<blockquote>
<p>  `register_from` string COMMENT ‘注册来源’,</p>
</blockquote>
<blockquote>
<p>  `remark` string COMMENT ‘细分渠道’,</p>
</blockquote>
<blockquote>
<p>  `province` string COMMENT ‘注册省份’,</p>
</blockquote>
<blockquote>
<p>  `pl_cnt` bigint COMMENT ‘评论次数’,</p>
</blockquote>
<blockquote>
<p>  `ds_cnt` bigint COMMENT ‘打赏次数’,</p>
</blockquote>
<blockquote>
<p>  `sc_add` bigint COMMENT ‘添加收藏’,</p>
</blockquote>
<blockquote>
<p>  `sc_cancel` bigint COMMENT ‘取消收藏’,</p>
</blockquote>
<blockquote>
<p>  `gzg_add` bigint COMMENT ‘关注商品’,</p>
</blockquote>
<blockquote>
<p>  `gzg_cancel` bigint COMMENT ‘取消关注商品’,</p>
</blockquote>
<blockquote>
<p>  `gzp_add` bigint COMMENT ‘关注人’,</p>
</blockquote>
<blockquote>
<p>  `gzp_cancel` bigint COMMENT ‘取消关注人’,</p>
</blockquote>
<blockquote>
<p>  `buzhi_cnt` bigint COMMENT ‘点不值次数’,</p>
</blockquote>
<blockquote>
<p>  `zhi_cnt` bigint COMMENT ‘点值次数’,</p>
</blockquote>
<blockquote>
<p>  `zan_cnt` bigint COMMENT ‘点赞次数’,</p>
</blockquote>
<blockquote>
<p>  `share_cnts` bigint COMMENT ‘分享次数’,</p>
</blockquote>
<blockquote>
<p>  `bl_cnt` bigint COMMENT ‘爆料数’,</p>
</blockquote>
<blockquote>
<p>  `fb_cnt` bigint COMMENT ‘好价发布数’,</p>
</blockquote>
<blockquote>
<p>  `online_cnt` bigint COMMENT ‘活跃次数’,</p>
</blockquote>
<blockquote>
<p>  `checkin_cnt` bigint COMMENT ‘签到次数’,</p>
</blockquote>
<blockquote>
<p>  `fix_checkin` bigint COMMENT ‘补签次数’,</p>
</blockquote>
<blockquote>
<p>  `house_point` bigint COMMENT ‘幸运屋金币抽奖次数’,</p>
</blockquote>
<blockquote>
<p>  `house_gold` bigint COMMENT ‘幸运屋积分抽奖次数’,</p>
</blockquote>
<blockquote>
<p>  `pack_cnt` bigint COMMENT ‘礼品兑换次数’,</p>
</blockquote>
<blockquote>
<p>  `gold_add` bigint COMMENT ‘获取金币’,</p>
</blockquote>
<blockquote>
<p>  `gold_cancel` bigint COMMENT ‘支出金币’,</p>
</blockquote>
<blockquote>
<p>  `surplus_gold` bigint COMMENT ‘剩余金币’,</p>
</blockquote>
<blockquote>
<p>  `event` bigint COMMENT ‘电商点击次数’,</p>
</blockquote>
<blockquote>
<p>  `gmv_amount` bigint COMMENT ‘gmv’,</p>
</blockquote>
<blockquote>
<p>  `gmv_sales` bigint COMMENT ‘订单数’)</p>
</blockquote>
<blockquote>
<p>  PARTITIONED BY ( `dt` string)</p>
</blockquote>
<h2 id="3-5-ADS层分析过哪些指标"><a href="#3-5-ADS层分析过哪些指标" class="headerlink" title="3.5 ADS层分析过哪些指标"></a>3.5 ADS层分析过哪些指标</h2><h3 id="3-5-1-分析过的指标（一分钟至少说出30个指标）"><a href="#3-5-1-分析过的指标（一分钟至少说出30个指标）" class="headerlink" title="3.5.1 分析过的指标（一分钟至少说出30个指标）"></a>3.5.1 分析过的指标（一分钟至少说出30个指标）</h3><p>日活、月活、周活、留存、留存率、新增（日、周、年）、转化率、流失、回流、七天内连续3天登录（点赞、收藏、评价、购买、加购、下单、活动）、连续3周（月）登录、GMV、复购率、复购率排行、点赞、评论、收藏、领优惠价人数、使用优惠价、沉默、值不值得买、退款人数、退款率<br>topn 热门商品</p>
<p>产品经理最关心的：留转G复活</p>
<p><img src="media/5679bb92e2b4f5ffac2449271c1ece9c.png"></p>
<h3 id="3-5-2-留转G复活指标"><a href="#3-5-2-留转G复活指标" class="headerlink" title="3.5.2 留转G复活指标"></a>3.5.2 留转G复活指标</h3><p>（1）活跃</p>
<blockquote>
<p>  日活：100万 ；月活：是日活的2-3倍 300万</p>
</blockquote>
<blockquote>
<p>  总注册的用户多少？1000万-3000万之间</p>
</blockquote>
<p>（2）GMV</p>
<blockquote>
<p>  GMV：每天 10万订单 （50 – 100元） 500万-1000万</p>
</blockquote>
<blockquote>
<p>  10%-20% 100万-200万（人员：程序员、人事、行政、财务、房租、收电费）</p>
</blockquote>
<p>（3）复购率</p>
<blockquote>
<p>  某日常商品复购；（手纸、面膜、牙膏）10%-20%</p>
</blockquote>
<p>电脑、显示器、手表 1%</p>
<p>（4）转化率</p>
<p>商品详情 =》 加购物车 =》下单 =》 支付</p>
<p>5%-10% 60-70% 90%-95%</p>
<p>（5）留存率</p>
<p>1/2/3、周留存、月留存</p>
<p>搞活动： 10-20%</p>
<h3 id="3-5-3-哪个商品卖的好？"><a href="#3-5-3-哪个商品卖的好？" class="headerlink" title="3.5.3 哪个商品卖的好？"></a>3.5.3 哪个商品卖的好？</h3><p>面膜、手纸，每天销售5000个</p>
<h2 id="3-6-ADS层手写指标"><a href="#3-6-ADS层手写指标" class="headerlink" title="3.6 ADS层手写指标"></a>3.6 ADS层手写指标</h2><h3 id="3-6-1-如何分析用户活跃？"><a href="#3-6-1-如何分析用户活跃？" class="headerlink" title="3.6.1 如何分析用户活跃？"></a>3.6.1 如何分析用户活跃？</h3><p>在启动日志中统计不同设备id出现次数。</p>
<h3 id="3-6-2-如何分析用户新增？vivo"><a href="#3-6-2-如何分析用户新增？vivo" class="headerlink" title="3.6.2 如何分析用户新增？vivo"></a>3.6.2 如何分析用户新增？vivo</h3><p>用活跃用户表 left join 用户新增表，用户新增表中mid为空的即为用户新增。</p>
<h3 id="3-6-3-如何分析用户1天留存？"><a href="#3-6-3-如何分析用户1天留存？" class="headerlink" title="3.6.3 如何分析用户1天留存？"></a>3.6.3 如何分析用户1天留存？</h3><p>留存用户=前一天新增 join 今天活跃</p>
<p>用户留存率=留存用户/前一天新增</p>
<h3 id="3-6-4-如何分析沉默用户？"><a href="#3-6-4-如何分析沉默用户？" class="headerlink" title="3.6.4 如何分析沉默用户？"></a>3.6.4 如何分析沉默用户？</h3><p>(登录时间为7天前,且只出现过一次)</p>
<p>按照设备id对日活表分组，登录次数为1，且是在一周前登录。</p>
<h3 id="3-6-5-如何分析本周回流用户？"><a href="#3-6-5-如何分析本周回流用户？" class="headerlink" title="3.6.5 如何分析本周回流用户？"></a>3.6.5 如何分析本周回流用户？</h3><p>本周活跃left join本周新增 left join上周活跃，且本周新增id和上周活跃id都为null</p>
<h3 id="3-6-6-如何分析流失用户？"><a href="#3-6-6-如何分析流失用户？" class="headerlink" title="3.6.6 如何分析流失用户？"></a>3.6.6 如何分析流失用户？</h3><p>(登录时间为7天前)</p>
<p>按照设备id对日活表分组，且七天内没有登录过。</p>
<h3 id="3-6-7-如何分析最近连续3周活跃用户数？"><a href="#3-6-7-如何分析最近连续3周活跃用户数？" class="headerlink" title="3.6.7 如何分析最近连续3周活跃用户数？"></a>3.6.7 如何分析最近连续3周活跃用户数？</h3><p>按照设备id对周活进行分组，统计次数大于3次。</p>
<h3 id="3-6-8-如何分析最近七天内连续三天活跃用户数？"><a href="#3-6-8-如何分析最近七天内连续三天活跃用户数？" class="headerlink" title="3.6.8 如何分析最近七天内连续三天活跃用户数？"></a>3.6.8 如何分析最近七天内连续三天活跃用户数？</h3><p>1）查询出最近7天的活跃用户，并对用户活跃日期进行排名</p>
<p>2）计算用户活跃日期及排名之间的差值</p>
<p>3）对同用户及差值分组，统计差值个数</p>
<p>4）将差值相同个数大于等于3的数据取出，然后去重(去的是什么重???)，即为连续3天及以上活跃的用户</p>
<p>7天连续收藏、点赞、购买、加购、付款、浏览、商品点击、退货</p>
<p>1个月连续7天</p>
<p>连续两周：</p>
<h2 id="3-7-分析过最难的指标"><a href="#3-7-分析过最难的指标" class="headerlink" title="3.7 分析过最难的指标"></a>3.7 分析过最难的指标</h2><h3 id="3-7-1-最近连续3周活跃用户"><a href="#3-7-1-最近连续3周活跃用户" class="headerlink" title="3.7.1 最近连续3周活跃用户"></a>3.7.1 最近连续3周活跃用户</h3><h3 id="3-7-2-最近7天连续3天活跃用户数"><a href="#3-7-2-最近7天连续3天活跃用户数" class="headerlink" title="3.7.2 最近7天连续3天活跃用户数"></a>3.7.2 最近7天连续3天活跃用户数</h3><h3 id="3-7-3-运费分摊"><a href="#3-7-3-运费分摊" class="headerlink" title="3.7.3 运费分摊"></a>3.7.3 运费分摊</h3><h1 id="第4章-生产经验—业务"><a href="#第4章-生产经验—业务" class="headerlink" title="第4章 生产经验—业务"></a>第4章 生产经验—业务</h1><h2 id="4-1-电商常识"><a href="#4-1-电商常识" class="headerlink" title="4.1 电商常识"></a>4.1 电商常识</h2><h3 id="4-1-1-SKU和SPU"><a href="#4-1-1-SKU和SPU" class="headerlink" title="4.1.1 SKU和SPU"></a>4.1.1 SKU和SPU</h3><blockquote>
<p>  SKU：一台银色、128G内存的、支持联通网络的iPhoneX</p>
</blockquote>
<blockquote>
<p>  SPU：iPhoneX</p>
</blockquote>
<blockquote>
<p>  Tm_id：品牌Id苹果，包括IPHONE，耳机，mac等</p>
</blockquote>
<h3 id="4-1-2-订单表跟订单详情表区别？"><a href="#4-1-2-订单表跟订单详情表区别？" class="headerlink" title="4.1.2 订单表跟订单详情表区别？"></a>4.1.2 订单表跟订单详情表区别？</h3><p>订单表的订单状态会变化，订单详情表不会，因为没有订单状态。</p>
<p>订单表记录user_id，订单id订单编号，订单的总金额order_status，支付方式，订单状态等。</p>
<p>订单详情表记录user_id，商品sku_id<br>,具体的商品信息（商品名称sku_name，价格order_price，数量sku_num）</p>
<h2 id="4-2-埋点行为数据基本格式-基本字段"><a href="#4-2-埋点行为数据基本格式-基本字段" class="headerlink" title="4.2 埋点行为数据基本格式(基本字段)"></a>4.2 埋点行为数据基本格式(基本字段)</h2><p>我们要收集和分析的数据主要包括<strong>页面数据</strong>、<strong>事件数据、曝光数据、启动数据和错误数据</strong>。</p>
<h3 id="4-2-1-页面"><a href="#4-2-1-页面" class="headerlink" title="4.2.1 页面"></a>4.2.1 页面</h3><p>页面数据主要记录一个页<br>面的用户访问情况，包括访问时间、停留时间、页面路径等信息。</p>
<p><img src="media/9a01f1b363d979c772fe9d2ecb2d308f.png"></p>
<p>所有页面id如下</p>
<blockquote>
<p>  home(“首页”),</p>
</blockquote>
<blockquote>
<p>  category(“分类页”),</p>
</blockquote>
<blockquote>
<p>  discovery(“发现页”),</p>
</blockquote>
<blockquote>
<p>  top_n(“热门排行”),</p>
</blockquote>
<blockquote>
<p>  favor(“收藏页”),</p>
</blockquote>
<blockquote>
<p>  search(“搜索页”),</p>
</blockquote>
<blockquote>
<p>  good_list(“商品列表页”),</p>
</blockquote>
<blockquote>
<p>  good_detail(“商品详情”),</p>
</blockquote>
<blockquote>
<p>  good_spec(“商品规格”),</p>
</blockquote>
<blockquote>
<p>  comment(“评价”),</p>
</blockquote>
<blockquote>
<p>  comment_done(“评价完成”),</p>
</blockquote>
<blockquote>
<p>  comment_list(“评价列表”),</p>
</blockquote>
<blockquote>
<p>  cart(“购物车”),</p>
</blockquote>
<blockquote>
<p>  trade(“下单结算”),</p>
</blockquote>
<blockquote>
<p>  payment(“支付页面”),</p>
</blockquote>
<blockquote>
<p>  payment_done(“支付完成”),</p>
</blockquote>
<blockquote>
<p>  orders_all(“全部订单”),</p>
</blockquote>
<blockquote>
<p>  orders_unpaid(“订单待支付”),</p>
</blockquote>
<blockquote>
<p>  orders_undelivered(“订单待发货”),</p>
</blockquote>
<blockquote>
<p>  orders_unreceipted(“订单待收货”),</p>
</blockquote>
<blockquote>
<p>  orders_wait_comment(“订单待评价”),</p>
</blockquote>
<blockquote>
<p>  mine(“我的”),</p>
</blockquote>
<blockquote>
<p>  activity(“活动”),</p>
</blockquote>
<blockquote>
<p>  login(“登录”),</p>
</blockquote>
<blockquote>
<p>  register(“注册”);</p>
</blockquote>
<p>所有页面对象类型如下：</p>
<blockquote>
<p>  sku_id(“商品skuId”),</p>
</blockquote>
<blockquote>
<p>  keyword(“搜索关键词”),</p>
</blockquote>
<blockquote>
<p>  sku_ids(“多个商品skuId”),</p>
</blockquote>
<blockquote>
<p>  activity_id(“活动id”),</p>
</blockquote>
<blockquote>
<p>  coupon_id(“购物券id”);</p>
</blockquote>
<p>所有来源类型如下：</p>
<blockquote>
<p>  promotion(“商品推广”),</p>
</blockquote>
<blockquote>
<p>  recommend(“算法推荐商品”),</p>
</blockquote>
<blockquote>
<p>  query(“查询结果商品”),</p>
</blockquote>
<blockquote>
<p>  activity(“促销活动”);</p>
</blockquote>
<h3 id="4-2-2-事件"><a href="#4-2-2-事件" class="headerlink" title="4.2.2 事件"></a>4.2.2 事件</h3><p>事件数据主要记录应用内一个具体操作行为，包括操作类型、操作对象、操作对象描述等信息。</p>
<p><img src="media/55e73089f1bd2706834d708e437e678c.png"></p>
<p>所有动作类型如下：</p>
<blockquote>
<p>  favor_add(“添加收藏”),</p>
</blockquote>
<blockquote>
<p>  favor_canel(“取消收藏”),</p>
</blockquote>
<blockquote>
<p>  cart_add(“添加购物车”),</p>
</blockquote>
<blockquote>
<p>  cart_remove(“删除购物车”),</p>
</blockquote>
<blockquote>
<p>  cart_add_num(“增加购物车商品数量”),</p>
</blockquote>
<blockquote>
<p>  cart_minus_num(“减少购物车商品数量”),</p>
</blockquote>
<blockquote>
<p>  trade_add_address(“增加收货地址”),</p>
</blockquote>
<blockquote>
<p>  get_coupon(“领取优惠券”);</p>
</blockquote>
<p>注：对于下单、支付等业务数据，可从业务数据库获取。</p>
<p>所有动作目标类型如下：</p>
<blockquote>
<p>  sku_id(“商品”),</p>
</blockquote>
<blockquote>
<p>  coupon_id(“购物券”);</p>
</blockquote>
<h3 id="4-2-3-曝光"><a href="#4-2-3-曝光" class="headerlink" title="4.2.3 曝光"></a>4.2.3 曝光</h3><p>曝光数据主要记录页面所曝光的内容，包括曝光对象，曝光类型等信息。</p>
<p><img src="media/4944ba187f3057d5e683b97dc0046994.png"></p>
<p>所有曝光类型如下：</p>
<blockquote>
<p>  promotion(“商品推广”),<br>  recommend(“算法推荐商品”),<br>  query(“查询结果商品”),<br>  activity(“促销活动”);</p>
</blockquote>
<p>所有曝光对象类型如下：</p>
<blockquote>
<p>  sku_id(“商品skuId”),</p>
</blockquote>
<blockquote>
<p>  activity_id(“活动id”);</p>
</blockquote>
<h3 id="4-2-4-启动"><a href="#4-2-4-启动" class="headerlink" title="4.2.4 启动"></a>4.2.4 启动</h3><p>启动数据记录应用的启动信息。</p>
<p><img src="media/51d3aa64fce010a017ab56e48b05e0fc.png"></p>
<p>所有启动入口类型如下：</p>
<blockquote>
<p>  icon(“图标”),<br>  notification(“通知”),<br>  install(“安装后启动”);</p>
</blockquote>
<h3 id="4-2-5-错误"><a href="#4-2-5-错误" class="headerlink" title="4.2.5 错误"></a>4.2.5 错误</h3><p>错误数据记录应用使用过程中的错误信息，包括错误编号及错误信息。</p>
<h3 id="4-2-6-埋点数据日志格式"><a href="#4-2-6-埋点数据日志格式" class="headerlink" title="4.2.6 埋点数据日志格式"></a>4.2.6 埋点数据日志格式</h3><p>我们的日志结构大致可分为两类，一是普通页面埋点日志，二是启动日志。</p>
<p>普通页面日志结构如下，每条日志包含了，当前页面的页面信息，所有事件（动作）、所有曝光信息以及错误信息。除此之外，还包含了一系列公共信息，包括设备信息，地理位置，应用信息等，即下边的common字段。</p>
<blockquote>
<p>  {</p>
</blockquote>
<blockquote>
<p>  “common”: { – 公共信息</p>
</blockquote>
<blockquote>
<p>  “ar”: “230000”, – 地区编码</p>
</blockquote>
<blockquote>
<p>  “ba”: “iPhone”, – 手机品牌</p>
</blockquote>
<blockquote>
<p>  “ch”: “Appstore”, – 渠道</p>
</blockquote>
<blockquote>
<p>  “md”: “iPhone 8”, – 手机型号</p>
</blockquote>
<blockquote>
<p>  “mid”: “YXfhjAYH6As2z9Iq”, – 设备id</p>
</blockquote>
<blockquote>
<p>  “os”: “iOS 13.2.9”, – 操作系统</p>
</blockquote>
<blockquote>
<p>  “uid”: “485”, – 会员id</p>
</blockquote>
<blockquote>
<p>  “vc”: “v2.1.134” – app版本号</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  “actions”: [ –动作(事件)</p>
</blockquote>
<blockquote>
<p>  {</p>
</blockquote>
<blockquote>
<p>  “action_id”: “favor_add”, –动作id</p>
</blockquote>
<blockquote>
<p>  “item”: “3”, –目标id</p>
</blockquote>
<blockquote>
<p>  “item_type”: “sku_id”, –目标类型</p>
</blockquote>
<blockquote>
<p>  “ts”: 1585744376605 –动作时间戳</p>
</blockquote>
<blockquote>
<p>  }</p>
</blockquote>
<blockquote>
<p>  ]，</p>
</blockquote>
<blockquote>
<p>  “displays”: [</p>
</blockquote>
<blockquote>
<p>  {</p>
</blockquote>
<blockquote>
<p>  “displayType”: “query”, – 曝光类型</p>
</blockquote>
<blockquote>
<p>  “item”: “3”, – 曝光对象id</p>
</blockquote>
<blockquote>
<p>  “item_type”: “sku_id”, – 曝光对象类型</p>
</blockquote>
<blockquote>
<p>  “order”: 1 –出现顺序</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  {</p>
</blockquote>
<blockquote>
<p>  “displayType”: “promotion”,</p>
</blockquote>
<blockquote>
<p>  “item”: “6”,</p>
</blockquote>
<blockquote>
<p>  “item_type”: “sku_id”,</p>
</blockquote>
<blockquote>
<p>  “order”: 2</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  {</p>
</blockquote>
<blockquote>
<p>  “displayType”: “promotion”,</p>
</blockquote>
<blockquote>
<p>  “item”: “9”,</p>
</blockquote>
<blockquote>
<p>  “item_type”: “sku_id”,</p>
</blockquote>
<blockquote>
<p>  “order”: 3</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  {</p>
</blockquote>
<blockquote>
<p>  “displayType”: “recommend”,</p>
</blockquote>
<blockquote>
<p>  “item”: “6”,</p>
</blockquote>
<blockquote>
<p>  “item_type”: “sku_id”,</p>
</blockquote>
<blockquote>
<p>  “order”: 4</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  {</p>
</blockquote>
<blockquote>
<p>  “displayType”: “query “,</p>
</blockquote>
<blockquote>
<p>  “item”: “6”,</p>
</blockquote>
<blockquote>
<p>  “item_type”: “sku_id”,</p>
</blockquote>
<blockquote>
<p>  “order”: 5</p>
</blockquote>
<blockquote>
<p>  }</p>
</blockquote>
<blockquote>
<p>  ],</p>
</blockquote>
<blockquote>
<p>  “page”: { –页面信息</p>
</blockquote>
<blockquote>
<p>  “during_time”: 7648, – 持续时间毫秒</p>
</blockquote>
<blockquote>
<p>  “item”: “3”, – 目标id</p>
</blockquote>
<blockquote>
<p>  “item_type”: “sku_id”, – 目标类型</p>
</blockquote>
<blockquote>
<p>  “last_page_id”: “login”, – 上页类型</p>
</blockquote>
<blockquote>
<p>  “page_id”: “good_detail”, – 页面ID</p>
</blockquote>
<blockquote>
<p>  “sourceType”: “promotion” – 来源类型</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  “err”:{ –错误</p>
</blockquote>
<blockquote>
<p>  “error_code”: “1234”, –错误码</p>
</blockquote>
<blockquote>
<p>  “msg”: “***********“ –错误信息</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  “ts”: 1585744374423 –跳入时间戳</p>
</blockquote>
<blockquote>
<p>  }</p>
</blockquote>
<p>启动日志结构相对简单，主要包含公共信息，启动信息和错误信息。</p>
<blockquote>
<p>  {</p>
</blockquote>
<blockquote>
<p>  “common”: {</p>
</blockquote>
<blockquote>
<p>  “ar”: “370000”,</p>
</blockquote>
<blockquote>
<p>  “ba”: “Honor”,</p>
</blockquote>
<blockquote>
<p>  “ch”: “wandoujia”,</p>
</blockquote>
<blockquote>
<p>  “md”: “Honor 20s”,</p>
</blockquote>
<blockquote>
<p>  “mid”: “eQF5boERMJFOujcp”,</p>
</blockquote>
<blockquote>
<p>  “os”: “Android 11.0”,</p>
</blockquote>
<blockquote>
<p>  “uid”: “76”,</p>
</blockquote>
<blockquote>
<p>  “vc”: “v2.1.134”</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  “start”: {</p>
</blockquote>
<blockquote>
<p>  “entry”: “icon”, –icon手机图标 notice 通知 install 安装后启动</p>
</blockquote>
<blockquote>
<p>  “loading_time”: 18803, –启动加载时间</p>
</blockquote>
<blockquote>
<p>  “open_ad_id”: 7, –广告页ID</p>
</blockquote>
<blockquote>
<p>  “open_ad_ms”: 3449, – 广告总共播放时间</p>
</blockquote>
<blockquote>
<p>  “open_ad_skip_ms”: 1989 – 用户跳过广告时点</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  “err”:{ –错误</p>
</blockquote>
<blockquote>
<p>  “error_code”: “1234”, –错误码</p>
</blockquote>
<blockquote>
<p>  “msg”: “***********“ –错误信息</p>
</blockquote>
<blockquote>
<p>  },</p>
</blockquote>
<blockquote>
<p>  “ts”: 1585744304000</p>
</blockquote>
<blockquote>
<p>  }</p>
</blockquote>
<h2 id="4-3-电商业务流程"><a href="#4-3-电商业务流程" class="headerlink" title="4.3 电商业务流程"></a>4.3 电商业务流程</h2><p>1）记住表与表之间的关系</p>
<p>2）每个表记住2-3个字段</p>
<p><img src="media/25d5316135ceba9902305941ace34c7d.png"></p>
<h2 id="4-4-维度表和事实表（重点）"><a href="#4-4-维度表和事实表（重点）" class="headerlink" title="4.4 维度表和事实表（重点）"></a>4.4 维度表和事实表（重点）</h2><h3 id="4-4-1-维度表"><a href="#4-4-1-维度表" class="headerlink" title="4.4.1 维度表"></a>4.4.1 维度表</h3><p><strong>维度表</strong>：一般是对事实的<strong>描述信息</strong>。每一张维表对应现实世界中的一个对象或者概念。<br>例如：用户、商品、日期、地区等。</p>
<h3 id="4-4-2-事实表"><a href="#4-4-2-事实表" class="headerlink" title="4.4.2 事实表"></a>4.4.2 事实表</h3><p><strong>事实表中的每行数据代表一个业务事件（下单、支付、退款、评价等）</strong>。“事实”这个术语表示的是业务事件的<strong>度量值（可统计次数、个数、件数、金额等）</strong>，例如，订单事件中的下单金额。</p>
<p>每一个事实表的行包括：具有可加性的数值型的度量值、与维表相连接的外键、通常具有两个和两个以上的外键、外键之间表示维表之间多对多的关系。</p>
<p><strong>1）事务型事实表</strong></p>
<p>以<strong>每个事务或事件为单位</strong>，例如一个销售订单记录，一笔支付记录等，作为事实表里的一行数据。一旦事务被提交，事实表数据被插入，数据就不再进行更改，其更新方式为增量更新。</p>
<p><strong>2）周期型快照事实表</strong></p>
<p>周期型快照事实表中<strong>不会保留所有数据</strong>，<strong>只保留固定时间间隔的数据</strong>，例如每天或者每月的销售额，或每月的账户余额等。</p>
<p><strong>3）累积型快照事实表</strong></p>
<p><strong>累计快照事实表用于跟踪业务事实的变化。</strong>例如，数据仓库中可能需要累积或者存储订单从下订单开始，到订单商品被打包、运输、和签收的各个业务阶段的时间点数据来跟踪订单声明周期的进展情况。当这个业务过程进行时，事实表的记录也要不断更新。</p>
<table>
<thead>
<tr>
<th>订单id</th>
<th>用户id</th>
<th>下单时间</th>
<th>打包时间</th>
<th>发货时间</th>
<th>签收时间</th>
<th>订单金额</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
<td>3-8</td>
<td>3-8</td>
<td>3-9</td>
<td>3-10</td>
<td></td>
</tr>
</tbody></table>
<h2 id="4-5-同步策略（重点）"><a href="#4-5-同步策略（重点）" class="headerlink" title="4.5 同步策略（重点）"></a>4.5 同步策略（重点）</h2><p><img src="media/19bdf7c32c9bcd942848c2225c84b8d8.png"></p>
<p>实体表，维度表统称维度表，每日全量或者每月（更长时间）全量</p>
<p>事务型事实表：每日增量</p>
<p>周期性事实表：拉链表</p>
<h2 id="4-6-关系型数据库范式理论"><a href="#4-6-关系型数据库范式理论" class="headerlink" title="4.6 关系型数据库范式理论"></a>4.6 关系型数据库范式理论</h2><p>1NF：属性不可再分割（例如不能存在5台电脑的属性，坏处：表都没法用）</p>
<p>2NF：不能存在部分函数依赖（例如主键（学号+课名）–&gt;成绩，姓名，但学号–》姓名，所以姓名部分依赖于主键（学号+课名），所以要去除，坏处：数据冗余）</p>
<p>3NF：不能存在传递函数依赖（学号–》宿舍种类–》价钱，坏处：数据冗余和增删异常）</p>
<p>MySQL关系模型：关系模型主要应用与OLTP系统中，为了保证数据的一致性以及避免冗余，所以大部分业务系统的表都是遵循第三范式的。</p>
<p>Hive 维度模型：维度模型主要应用于OLAP系统中，因为关系模型虽然冗余少，</p>
<p>但是在大规模数据，跨表分析统计查询过程中，会造成多表关联，这会大大降低执行效率。</p>
<p>所以HIVE把相关各种表整理成两种：事实表和维度表两种。所有维度表围绕着事实表进行解释。</p>
<h2 id="4-7-数据模型"><a href="#4-7-数据模型" class="headerlink" title="4.7 数据模型"></a>4.7 数据模型</h2><p>雪花模型、星型模型和星座模型</p>
<p>（在维度建模的基础上又分为三种模型：星型模型、雪花模型、星座模型。）</p>
<p>星型模型（一级维度表），雪花（多级维度），星座模型（星型模型+多个事实表）</p>
<h2 id="4-8-拉链表（重点）"><a href="#4-8-拉链表（重点）" class="headerlink" title="4.8 拉链表（重点）"></a>4.8 拉链表（重点）</h2><p>拉链表处理的业务场景：主要处理缓慢变化维的业务场景。（用户表、订单表）</p>
<p><img src="media/467bc157721839abb81f08a466bff208.png"></p>
<h2 id="4-9-即席查询数据仓库"><a href="#4-9-即席查询数据仓库" class="headerlink" title="4.9 即席查询数据仓库"></a>4.9 即席查询数据仓库</h2><p><img src="media/39c606ab691accda9cc11fa629c33d86.png"></p>
<p>Kylin: T+1</p>
<p>Impala: CDH</p>
<p>Presto: Apache版本框架</p>
<h2 id="4-10-数据仓库每天跑多少张表，大概什么时候运行，运行多久？"><a href="#4-10-数据仓库每天跑多少张表，大概什么时候运行，运行多久？" class="headerlink" title="4.10 数据仓库每天跑多少张表，大概什么时候运行，运行多久？"></a>4.10 数据仓库每天跑多少张表，大概什么时候运行，运行多久？</h2><p>基本一个项目建一个库，表格个数为初始的原始数据表格加上统计结果表格的总数。（一般70-100张表格）</p>
<p>用户行为5张；业务数据23张表 =》ods 24 =》dwd=&gt;20张=》dws<br>6张宽表=&gt;dwt6张宽表=&gt;ads=》30张 =》86张</p>
<p>每天0：30开始运行。=》sqoop 40-50分钟：1点20：=》 5-6个小时运行完指标</p>
<p>所有离线数据报表控制在8小时之内</p>
<p>大数据实时处理部分控制在5分钟之内。（分钟级别、秒级别）</p>
<p>如果是实时推荐系统，需要秒级响应</p>
<h2 id="4-11-活动的话，数据量会增加多少？怎么解决？"><a href="#4-11-活动的话，数据量会增加多少？怎么解决？" class="headerlink" title="4.11 活动的话，数据量会增加多少？怎么解决？"></a>4.11 活动的话，数据量会增加多少？怎么解决？</h2><p>日活增加50%，GMV增加多少。（留转G复活）情人节，促销手纸。</p>
<p>集群资源都留有预量。11.11，6.18，数据量过大，提前动态增加服务器。</p>
<h2 id="4-12-并发峰值多少？大概哪个时间点？"><a href="#4-12-并发峰值多少？大概哪个时间点？" class="headerlink" title="4.12 并发峰值多少？大概哪个时间点？"></a>4.12 并发峰值多少？大概哪个时间点？</h2><p>高峰期晚上7-12点。Kafka里面20m/s 2万/s 并发峰值在1-2万人</p>
<h2 id="4-13-数仓中使用的哪种文件存储格式"><a href="#4-13-数仓中使用的哪种文件存储格式" class="headerlink" title="4.13 数仓中使用的哪种文件存储格式"></a>4.13 数仓中使用的哪种文件存储格式</h2><p>常用的包括：textFile，rcFile，ORC，Parquet，一般企业里使用ORC或者Parquet，因为是列式存储，且压缩比非常高，所以相比于textFile，查询速度快，占用硬盘空间少</p>
<h2 id="4-14-哪张表最费时间，有没有优化"><a href="#4-14-哪张表最费时间，有没有优化" class="headerlink" title="4.14 哪张表最费时间，有没有优化"></a>4.14 哪张表最费时间，有没有优化</h2><p>用户行为宽表，数据量过大。数据倾斜的相关优化手段。（hadoop、hive、spark）</p>
<h2 id="4-15-用什么工具做权限管理"><a href="#4-15-用什么工具做权限管理" class="headerlink" title="4.15 用什么工具做权限管理"></a>4.15 用什么工具做权限管理</h2><p>Ranger或Sentry<br>（用户认证kerberos（张三、李四、王五）=&gt;表级别权限（张三、李四）、字段级别权限（李四））</p>
<h2 id="4-16-数仓当中数据多久删除一次"><a href="#4-16-数仓当中数据多久删除一次" class="headerlink" title="4.16 数仓当中数据多久删除一次"></a>4.16 数仓当中数据多久删除一次</h2><p>1）部分公司永久不删</p>
<p>2）有一年、两年“删除”一次的，这里面说的删除是，先将超时数据压缩下载到单独安装的磁盘上。然后删除集群上数据。<br>很少有公司不备份数据，直接删除的。</p>
<h1 id="第5章-生产经验–测试上线相关"><a href="#第5章-生产经验–测试上线相关" class="headerlink" title="第5章 生产经验–测试上线相关"></a>第5章 生产经验–测试上线相关</h1><h2 id="5-1-测试相关"><a href="#5-1-测试相关" class="headerlink" title="5.1 测试相关"></a>5.1 测试相关</h2><h3 id="5-1-1-公司有多少台测试服务器？"><a href="#5-1-1-公司有多少台测试服务器？" class="headerlink" title="5.1.1 公司有多少台测试服务器？"></a>5.1.1 公司有多少台测试服务器？</h3><p>测试服务器一般三台</p>
<h3 id="5-1-2-测试环境什么样？"><a href="#5-1-2-测试环境什么样？" class="headerlink" title="5.1.2 测试环境什么样？"></a>5.1.2 测试环境什么样？</h3><p>有钱的公司和生产环境电脑配置一样。</p>
<p>一般公司测试环境的配置是生产的一半</p>
<h3 id="5-1-3-测试数据哪来的？"><a href="#5-1-3-测试数据哪来的？" class="headerlink" title="5.1.3 测试数据哪来的？"></a>5.1.3 测试数据哪来的？</h3><p>一部分自己写Java程序自己造（更灵活），一部分从生产环境上取一部分（更真实）。</p>
<h3 id="5-1-4-如何保证写的sql正确性"><a href="#5-1-4-如何保证写的sql正确性" class="headerlink" title="5.1.4 如何保证写的sql正确性"></a>5.1.4 如何保证写的sql正确性</h3><p>需要造一些特定的测试数据，测试。</p>
<p>从生产环境抓取一部分数据，数据有多少你是知道的，运算完毕应该符合你的预期。</p>
<p>离线数据和实时数据分析的结果比较。（日活1万 实时10100），倾向取离线。</p>
<p>先在mysql的业务库里面把结果计算出来；在给你在ads层计算的结果进行比较；</p>
<h3 id="5-1-5-测试之后如何上线？"><a href="#5-1-5-测试之后如何上线？" class="headerlink" title="5.1.5 测试之后如何上线？"></a>5.1.5 测试之后如何上线？</h3><p>大公司：上线的时候，将脚本打包，提交git。先发邮件抄送经理和总监，运维。运维负责上线。</p>
<p>小公司：跟项目经理说一下，项目经理技术把关，项目经理通过了就可以上线了。风险意识。</p>
<p>所谓的上线就是编写脚本，并在azkaban中进行作业调度。</p>
<h2 id="5-2-项目实际工作流程"><a href="#5-2-项目实际工作流程" class="headerlink" title="5.2 项目实际工作流程"></a>5.2 项目实际工作流程</h2><p>以下是<strong>活跃用户</strong>需求的整体开发流程。</p>
<p>产品经理负责收集需求：需求来源与客户反馈、老板的意见</p>
<p><strong>第1步：确定指标的业务口径</strong></p>
<p>由产品经理主导，找到提出该指标的运营负责人沟通。首先要问清楚<strong>指标是怎么定义的</strong>，比如活跃用户是指启动过APP的用户。设备id<br>还是用户id。</p>
<p>邮件/需求文档-》不要口头</p>
<p><strong>第2步：需求评审</strong></p>
<p>由产品经理主导设计原型，对于活跃主题，我们最终要展示的是<strong>最近n天的活跃用户数变化趋势</strong><br>，效果如下图所示。此处<strong>大数据开发工程师、后端开发工程师、前端开发工程师一同参与</strong>，一起说明整个功能的价值和详细的操作流程，确保大家理解的一致。</p>
<p><img src="media/3db14f949fd4ccbd528263eff9df6e98.png"></p>
<p><strong>第3步：大数据开发</strong></p>
<p>大数据开发工程师，通过数据同步的工具如Flume、Sqoop等将数据同步到ODS层，然后就是一层一层的通过SQL计算到DWD、DWS层，最后形成可为应用直接服务的数据填充到ADS层。</p>
<p><strong>第4步：后端开发</strong></p>
<p>后端工程师负责，为大数据工程师提供业务数据接口；</p>
<p>同时还负责读取ADS层分析后，写入MySQL中的数据。</p>
<p><strong>第5步：前端开发</strong></p>
<p>前端工程师负责，前端埋点。</p>
<p>对分析后的结果数据进行可视化展示。</p>
<p><strong>第6步：联调</strong></p>
<p>此时大<strong>数据开发工程师、前端开发工程师、后端开发工程师</strong>都要参与进来。此时会要求大数据开发工程师基于历史的数据执行计算任务，大数据开发工程师承担数据准确性的校验。前后端解决用户操作的相关BUG保证不出现低级的问题完成自测。</p>
<p><strong>第7步：测试</strong></p>
<p>测试工程师对整个大数据系统进行测试。测试的手段包括，边界值、等价类等。</p>
<p>提交测试异常的软件有：<strong>禅道、bugzila</strong>（测试人员记录测试问题1.0，输入是什么，结果是什么，跟预期不一样-&gt;需要开发人员解释，是一个bug，下一个版本解决1.1-&gt;测试人员再测试。测试1.1ok-&gt;测试经理关闭bug）</p>
<p>1周开发写代码 =》2周测试时间</p>
<p><strong>第8步：上线</strong></p>
<p>运维工程师会配合我们的前后端开发工程师更新最新的版本到服务器。此时产品经理要找到该指标的负责人长期跟进指标的准确性。重要的指标还要每过一个周期内部再次验证，从而保证数据的准确性。</p>
<h2 id="5-3-项目中实现一个需求大概多长时间"><a href="#5-3-项目中实现一个需求大概多长时间" class="headerlink" title="5.3 项目中实现一个需求大概多长时间"></a>5.3 项目中实现一个需求大概多长时间</h2><p>刚入职第一个需求大概需要7天左右。</p>
<p>对业务熟悉后，平均一天一个需求。</p>
<p>影响时间的因素：测试服务器购买获取环境准备、对业务熟悉、开会讨论需求、表的权限申请、测试等。新员工培训（公司规章制度、代码规范）</p>
<h2 id="5-4-项目在3年内迭代次数，每一个项目具体是如何迭代的。公司版本迭代多久一次，迭代到哪个版本"><a href="#5-4-项目在3年内迭代次数，每一个项目具体是如何迭代的。公司版本迭代多久一次，迭代到哪个版本" class="headerlink" title="5.4 项目在3年内迭代次数，每一个项目具体是如何迭代的。公司版本迭代多久一次，迭代到哪个版本"></a>5.4 项目在3年内迭代次数，每一个项目具体是如何迭代的。公司版本迭代多久一次，迭代到哪个版本</h2><p>瀑布式开发、敏捷开发</p>
<p>差不多一个月会迭代一次。每月都有节日（元旦、春节、情人节、3.8妇女节、端午节、618、国庆、中秋、1111/6.1/5.1、生日、周末）新产品、新区域</p>
<p>就产品或我们提出优化需求，然后评估时间。每周我们都会开会做下周计划和本周总结。（日报、周报、月报、季度报、年报）需求1周的时间，周三一定完成。周四周五（帮同事写代码、自己学习工作额外的技术）</p>
<p>有时候也会去预研一些新技术。Flink hudi</p>
<p>5.1.2</p>
<p>5是大版本号：必须是重大升级</p>
<p>1：一般是核心模块变动</p>
<p>2：一般版本变化</p>
<h2 id="5-5-项目开发中每天做什么事"><a href="#5-5-项目开发中每天做什么事" class="headerlink" title="5.5 项目开发中每天做什么事"></a>5.5 项目开发中每天做什么事</h2><p>1）新需求（活动、优化、新产品、新市场）。 60%</p>
<p>2）故障分析：数仓的任何步骤出现问题，需要查看问题，比如日活，月活下降或快速上升等。20%</p>
<p>3）新技术的预言（比如flink、数仓建模、数据质量、元数据管理）10%</p>
<p>4）其临时任务 10%</p>
<p>5）晨会-》10做操-》讨论中午吃什么-》12点出去吃1点-》睡到2点-》3点茶歇水果-》晚上吃啥-》吃加班餐-》开会-》晚上6点吃饭-》7点开始干活-10点-》11点</p>
<h2 id="5-6-实时项目数据计算"><a href="#5-6-实时项目数据计算" class="headerlink" title="5.6 实时项目数据计算"></a>5.6 实时项目数据计算</h2><h3 id="5-6-1-跑实时任务，怎么分配内存和CPU资源"><a href="#5-6-1-跑实时任务，怎么分配内存和CPU资源" class="headerlink" title="5.6.1 跑实时任务，怎么分配内存和CPU资源"></a>5.6.1 跑实时任务，怎么分配内存和CPU资源</h3><p>128m数据对应1g内存</p>
<p>1个Kafka分区对应1个CPU</p>
<h3 id="5-6-2-跑实时任务，每天数据量多少？"><a href="#5-6-2-跑实时任务，每天数据量多少？" class="headerlink" title="5.6.2 跑实时任务，每天数据量多少？"></a>5.6.2 跑实时任务，每天数据量多少？</h3><p>用户行为：实时任务用到了用户行为多少张表（20g） 100g/5张表</p>
<p>业务数据：实时任务用到了业务数据多少张表(34m) 1g/30张表</p>
<p>活动、风控、销售、流量</p>
<h1 id="第6章-生产经验—技术"><a href="#第6章-生产经验—技术" class="headerlink" title="第6章 生产经验—技术"></a>第6章 生产经验—技术</h1><h2 id="6-1-可视化报表工具"><a href="#6-1-可视化报表工具" class="headerlink" title="6.1 可视化报表工具"></a>6.1 可视化报表工具</h2><p>Echarts（百度开源）、kibana（开源）、Tableau（功能强大的收费软件）、Superset（功能一般免费）、QuickBI（阿里云收费的离线）、DataV（阿里云收费的实时）</p>
<h2 id="6-2-集群监控工具"><a href="#6-2-集群监控工具" class="headerlink" title="6.2 集群监控工具"></a>6.2 集群监控工具</h2><p>Zabbix+ Grafana</p>
<h2 id="6-3-项目中遇到的问题怎么解决的（重点-）"><a href="#6-3-项目中遇到的问题怎么解决的（重点-）" class="headerlink" title="6.3 项目中遇到的问题怎么解决的（重点*****）"></a>6.3 项目中遇到的问题怎么解决的（重点*****）</h2><p>Shell 中flume停止脚本</p>
<p>Hadoop宕机</p>
<p>Hadoop解决数据倾斜方法</p>
<p>集群资源分配参数（项目中遇到的问题）</p>
<p>HDFS小文件处理</p>
<p>Hadoop优化</p>
<p>Flume挂掉</p>
<p>Flume优化</p>
<p>Kafka挂掉</p>
<p>Kafka丢失</p>
<p>Kafka数据重复</p>
<p>Kafka消息数据积压</p>
<p>Kafka优化</p>
<p>Kafka单条日志传输大小</p>
<p>自定义UDF、UDTF函数</p>
<p>Hive优化</p>
<p>Hive解决数据倾斜方法</p>
<p>7天内连续3次活跃</p>
<p>分摊</p>
<p>Sqoop空值、一致性、数据倾斜</p>
<p>Azkaban任务挂了怎么办？</p>
<p>Azkaban故障报警</p>
<p>Spark数据倾斜</p>
<p>Spark优化</p>
<p>SparkStreaming精确一次性消费</p>
<h2 id="6-4-Linux-Shell-Hadoop-ZK-Flume-kafka-Hive-Sqoop-Azkaban那些事"><a href="#6-4-Linux-Shell-Hadoop-ZK-Flume-kafka-Hive-Sqoop-Azkaban那些事" class="headerlink" title="6.4 Linux+Shell+Hadoop+ZK+Flume+kafka+Hive+Sqoop+Azkaban那些事"></a>6.4 Linux+Shell+Hadoop+ZK+Flume+kafka+Hive+Sqoop+Azkaban那些事</h2><p><img src="media/9deae070925650ce648410e6adb5115a.png"></p>
<h1 id="第7章-生产经验—热点问题"><a href="#第7章-生产经验—热点问题" class="headerlink" title="第7章 生产经验—热点问题"></a>第7章 生产经验—热点问题</h1><h2 id="7-1-元数据管理（Atlas血缘系统）"><a href="#7-1-元数据管理（Atlas血缘系统）" class="headerlink" title="7.1 元数据管理（Atlas血缘系统）"></a>7.1 元数据管理（Atlas血缘系统）</h2><p>依赖关系能够做到：表级别和字段级别</p>
<p>用处：作业执行失败，评估他的影响范围。 主要用于表比较多的公司。</p>
<p>版本问题：</p>
<blockquote>
<p>  0.84版本：2019-06-21</p>
</blockquote>
<blockquote>
<p>  2.0版本：2019-05-13</p>
</blockquote>
<p>框架版本：</p>
<p>Apache 0.84 2.0</p>
<p>CDH 2.0</p>
<h2 id="7-2-数据质量监控（Griffin）"><a href="#7-2-数据质量监控（Griffin）" class="headerlink" title="7.2 数据质量监控（Griffin）"></a>7.2 数据质量监控（Griffin）</h2><h3 id="7-2-1-监控原则"><a href="#7-2-1-监控原则" class="headerlink" title="7.2.1 监控原则"></a>7.2.1 监控原则</h3><p><strong>1）单表数据量监控</strong></p>
<p>一张表的记录数在一个已知的范围内，或者上下浮动不会超过某个阈值</p>
<ul>
<li><p>SQL结果：var 数据量 = select count（*）from 表 where 时间等过滤条件</p>
<ul>
<li><p>  报警触发条件设置：如果数据量不在[数值下限, 数值上限]， 则触发报警</p>
</li>
<li><p>同比增加：如果((本周的数据量 - 上周的数据量)/上周的数据量*100)不在<br>  [比例下线，比例上限]，则触发报警</p>
</li>
<li><p>环比增加：如果((今天的数据量 - 昨天的数据量)/昨天的数据量*100)不在<br>  [比例下线，比例上限]，则触发报警</p>
</li>
<li><p>报警触发条件设置一定要有。如果没有配置的阈值，不能做监控</p>
<p>  日活、周活、月活、留存（日周月）、转化率（日、周、月）GMV（日、周、月）</p>
<p>  复购率（日周月） 30%</p>
</li>
</ul>
</li>
</ul>
<p><strong>2）单表空值检测</strong></p>
<p>某个字段为空的记录数在一个范围内，或者占总量的百分比在某个阈值范围内</p>
<ul>
<li><p>目标字段：选择要监控的字段，不能选“无”</p>
<ul>
<li><p>SQL结果：var 异常数据量 = select count(*) from 表 where 目标字段 is<br>  null</p>
</li>
<li><p>  单次检测：如果(异常数据量)不在[数值下限, 数值上限]，则触发报警</p>
</li>
</ul>
</li>
</ul>
<p><strong>3）单表重复值检测</strong></p>
<p>一个或多个字段是否满足某些规则</p>
<ul>
<li><p>目标字段：第一步先正常统计条数；select count(*) form 表；</p>
<ul>
<li><p>  第二步，去重统计；select count(*) from 表 group by 某个字段</p>
</li>
<li><p>  第一步的值和第二步不的值做减法，看是否在上下线阀值之内</p>
</li>
<li><p>  单次检测：如果(异常数据量)不在[数值下限, 数值上限]， 则触发报警</p>
</li>
</ul>
</li>
</ul>
<p><strong>4）单表值域检测</strong></p>
<p>一个或多个字段没有重复记录</p>
<ul>
<li><p>目标字段：选择要监控的字段，支持多选</p>
<ul>
<li><p>检测规则：填写“目标字段”要满足的条件。其中$1表示第一个目标字段，$2表示第二个目标字段，以此类推。上图中的“检测规则”经过渲染后变为“delivery_fee<br>  = delivery_fee_base+delivery_fee_extra”</p>
</li>
<li><p>  阈值配置与“空值检测”相同</p>
</li>
</ul>
</li>
</ul>
<p><strong>5）跨表数据量对比</strong></p>
<p>主要针对同步流程，监控两张表的数据量是否一致</p>
<ul>
<li><p>SQL结果：count(本表) - count(关联表)</p>
<ul>
<li>  阈值配置与“空值检测”相同</li>
</ul>
</li>
</ul>
<h3 id="7-2-2-数据质量实现"><a href="#7-2-2-数据质量实现" class="headerlink" title="7.2.2 数据质量实现"></a>7.2.2 数据质量实现</h3><h2 id="7-3-权限管理（Ranger）"><a href="#7-3-权限管理（Ranger）" class="headerlink" title="7.3 权限管理（Ranger）"></a>7.3 权限管理（Ranger）</h2><h2 id="7-4-数据治理"><a href="#7-4-数据治理" class="headerlink" title="7.4 数据治理"></a>7.4 数据治理</h2><p>包括：数据质量管理、元数据管理、权限管理（ranger sentry）。</p>
<p>CDH cloudmanager-》sentry； HDP ambari=&gt;ranger</p>
<p>数据治理是一个复杂的系统工程，涉及到企业和单位多个领域，既要做好顶层设计，又要解决好统一标准、统一流程、统一管理体系等问题，同时也要解决好数据采集、数据清洗、数据对接和应用集成等相关问题。</p>
<p>数据治理实施要点主要包含<strong>数据规划、制定数据标准、整理数据、搭建数据管理工具、构建运维体系及推广贯标</strong>六大部分，其中数据规划是纲领、制定数据标准是基础、整理数据是过程、搭建数据管理工具是技术手段、构建运维体系是前提，推广贯标是持续保障。</p>
<p><img src="media/ced4f62d11cd6fcbcf9d812f81517b54.jpeg"></p>
<h2 id="7-5-数据中台"><a href="#7-5-数据中台" class="headerlink" title="7.5 数据中台"></a>7.5 数据中台</h2><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/nXI0nSSOneteIClA7dming">https://mp.weixin.qq.com/s/nXI0nSSOneteIClA7dming</a></p>
<h3 id="7-5-1-什么是中台？"><a href="#7-5-1-什么是中台？" class="headerlink" title="7.5.1 什么是中台？"></a>7.5.1 什么是中台？</h3><p>在传统IT企业，项目的物理结构是什么样的呢？无论项目内部的如何复杂，都可分为“前台”和“后台”这两部分。</p>
<p><strong>什么是前台？</strong></p>
<p>首先，这里所说的“前台”和“前端”并不是一回事。所谓前台即包括各种和用户直接交互的界面，比如web页面，手机app；也包括服务端各种实时响应用户请求的业务逻辑，比如商品查询、订单系统等等。</p>
<p><strong>什么是后台？</strong></p>
<p>后台并不直接面向用户，而是面向运营人员的配置管理系统，比如商品管理、物流管理、结算管理。后台为前台提供了一些简单的配置。</p>
<p><img src="media/784691687e2d74ee0dad27c0c5696df4.png"></p>
<h3 id="7-5-2-传统项目痛点"><a href="#7-5-2-传统项目痛点" class="headerlink" title="7.5.2 传统项目痛点"></a>7.5.2 传统项目痛点</h3><p>痛点：重复造轮子。</p>
<p><img src="media/655cc407aa4000fb194f45dc4b947f79.png"></p>
<h3 id="7-5-3-各家中台"><a href="#7-5-3-各家中台" class="headerlink" title="7.5.3 各家中台"></a>7.5.3 各家中台</h3><p>1）SuperCell公司</p>
<p><img src="media/ded9b7cd971e9cc7f9169cecf0e26226.png"></p>
<p>2）阿里巴巴提出了“大中台，小前台”的战略</p>
<p><img src="media/c4c1e1a9d6330c8a904378eb2c7afa14.png"></p>
<p>3）华为提出了“平台炮火支撑精兵作战”的战略</p>
<p><img src="media/2abaacdcefbe29b2e3b3543189a2ffcb.png"></p>
<h3 id="7-5-4-中台具体划分"><a href="#7-5-4-中台具体划分" class="headerlink" title="7.5.4 中台具体划分"></a>7.5.4 中台具体划分</h3><p>1）业务中台</p>
<p><img src="media/bdda2b87fe0a1a39aaf3c33c6d24d099.png"></p>
<p>2）技术中台</p>
<p><img src="media/248d0f1397e7638bd125d39e486bc28f.png"></p>
<p>3）数据中台</p>
<p><img src="media/7f468e841d3969ba1fb236d8d8830e80.png"></p>
<p>4）算法中台</p>
<p><img src="media/fcceaa915ae10981ca8d9223eb156123.png"></p>
<h3 id="7-5-5-中台使用场景"><a href="#7-5-5-中台使用场景" class="headerlink" title="7.5.5 中台使用场景"></a>7.5.5 中台使用场景</h3><p><strong>1）从0到1的阶段，没有必要搭建中台。</strong></p>
<p>从0到1的创业型公司，首要目的是生存下去，以最快的速度打造出产品，证明自身的市场价值。</p>
<p>这个时候，让项目野蛮生长才是最好的选择。如果不慌不忙地先去搭建中台，恐怕中台还没搭建好，公司早就饿死了。</p>
<p><strong>2）从1到N的阶段，适合搭建中台。</strong></p>
<p>当企业有了一定规模，产品得到了市场的认可，这时候公司的首要目的不再是活下去，而是活的更好。</p>
<p>这个时候，趁着项目复杂度还不是特别高，可以考虑把各项目的通用部分下沉，组建中台，以方便后续新项目的尝试和旧项目的迭代。</p>
<p><strong>3）从N到N+1的阶段，搭建中台势在必行。</strong></p>
<p>当企业已经有了很大的规模，各种产品、服务、部门错综复杂，这时候做架构调整会比较痛苦。</p>
<p>但是长痛不如短痛，为了项目的长期发展，还是需要尽早调整架构，实现平台化，以免日后越来越难以维护。</p>
<h2 id="7-6-数据湖"><a href="#7-6-数据湖" class="headerlink" title="7.6 数据湖"></a>7.6 数据湖</h2><p><strong>数据湖（</strong>Data<br>Lake）<strong>是一个存储企业的各种各样原始数据的大型仓库，其中的数据可供存取、处理、分析及传输。<br>hudi</strong></p>
<p>目前，Hadoop是最常用的部署数据湖的技术，所以很多人会觉得数据湖就是Hadoop集群。数据湖是一个概念，而Hadoop是用于实现这个概念的技术。</p>
<p><img src="media/d9b76a871960cce6b6302de4182cd91f.jpeg"></p>
<table>
<thead>
<tr>
<th><strong>数据仓库</strong></th>
<th><strong>数据湖</strong></th>
</tr>
</thead>
<tbody><tr>
<td>主要<strong>处理历史的、结构化的数据</strong>，而且这些数据必须与数据仓库事先定义的模型吻合。</td>
<td>能处理所有类型的数据，如结构化数据，非结构化数据，半结构化数据等，数据的类型依赖于数据源系统的原始数据格式。非结构化数据（语音、图片、视频等）</td>
</tr>
<tr>
<td>数据仓库分析的指标都是产品经理提前规定好的。按需分析数据。（日活、新增、留存、转化率）</td>
<td>根据海量的数据，挖掘出规律，反应给运营部门。 拥有<strong>非常强的计算能力</strong>用于处理数据。 数据挖掘</td>
</tr>
</tbody></table>
<h2 id="7-7-埋点"><a href="#7-7-埋点" class="headerlink" title="7.7 埋点"></a>7.7 埋点</h2><p>免费的埋点：上课演示。</p>
<p>收费的埋点：神策 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Xp3-alWF4XHvKDP9rNWCoQ">https://mp.weixin.qq.com/s/Xp3-alWF4XHvKDP9rNWCoQ</a></p>
<p>目前主流的埋点方式，有代码埋点（前端/后端）、可视化埋点、全埋点三种。</p>
<p><strong>代码埋点</strong>是通过调用埋点SDK函数，在需要埋点的业务逻辑功能位置调用接口，上报埋点数据。例如，我们对页面中的某个按钮埋点后，当这个按钮被点击时，可以在这个按钮对应的<br>OnClick 函数里面调用SDK提供的数据发送接口，来发送数据。</p>
<p><strong>可视化埋点</strong>只需要研发人员集成采集<br>SDK，不需要写埋点代码，业务人员就可以通过访问分析平台的“圈选”功能，来“圈”出需要对用户行为进行捕捉的控件，并对该事件进行命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集<br>SDK 按照圈选的配置自动进行用户行为数据的采集和发送。</p>
<p><strong>全埋点</strong>是通过在产品中嵌入SDK，前端自动采集页面上的全部用户行为事件，上报埋点数据，相当于做了一个统一的埋点。然后再通过界面配置哪些数据需要在系统里面进行分析。</p>
<h2 id="7-8-电商运营经验"><a href="#7-8-电商运营经验" class="headerlink" title="7.8 电商运营经验"></a>7.8 电商运营经验</h2><h3 id="7-8-1-电商8类基本指标"><a href="#7-8-1-电商8类基本指标" class="headerlink" title="7.8.1 电商8类基本指标"></a>7.8.1 电商8类基本指标</h3><p><img src="media/42b1312d93c6309ab550a9f79f07240e.png"></p>
<p><img src="media/520e108bedddac1b760f8618717eb6a9.png"></p>
<p><img src="media/8c3adc4cdac454cfbbd62738e67ab45d.png"></p>
<p><img src="media/1261826f5f9ea62f9819261ccd0b49b2.png"></p>
<p><img src="media/7196080520cbdadd7d87328a67d47cef.png"></p>
<p><img src="media/5e5e1d9030d44eddd3874a0d92ffef32.png"></p>
<p><img src="media/68ca66d240479a15f179e1e85c731216.png"></p>
<p>8）市场竞争指标：主要分析市场份额以及网站排名，进一步进行调整</p>
<p><img src="media/a658242b947afb60d89723ac6172af15.png"></p>
<h3 id="7-8-2-直播指标"><a href="#7-8-2-直播指标" class="headerlink" title="7.8.2 直播指标"></a>7.8.2 直播指标</h3><p><img src="media/04e74364f9e77123d9ea22d7caf06c6f.png"></p>
<p><img src="media/abb432a142d2dbac248e7e769416fc1b.png"></p>
<p><img src="media/00f7f676de81cd24d847e2202516f9d9.png"></p>
<p><img src="media/4083ce3cb57fd5a36622a083e63341f7.png"></p>
<p><img src="media/7761a957da3a2eba722c1dc415b6e3ea.png"></p>
<p><img src="media/16eb2d3b86f6f595cd5b54bed9eec2a3.png"></p>
<h1 id="第8章-手写代码"><a href="#第8章-手写代码" class="headerlink" title="第8章 手写代码"></a>第8章 手写代码</h1><h2 id="8-1-基本算法"><a href="#8-1-基本算法" class="headerlink" title="8.1 基本算法"></a>8.1 基本算法</h2><h3 id="8-1-1-冒泡排序"><a href="#8-1-1-冒泡排序" class="headerlink" title="8.1.1 冒泡排序"></a>8.1.1 冒泡排序</h3><p>/**</p>
<p>* 冒泡排序 时间复杂度 O(n^2) 空间复杂度O(1)</p>
<p>*/</p>
<p>public class BubbleSort {</p>
<p>public static void bubbleSort(int[] data) {</p>
<p>System.out.println(“开始排序”);</p>
<p>int arrayLength = data.length;</p>
<p>for (int i = 0; i &lt; arrayLength - 1; i++) {</p>
<p>boolean flag = false;</p>
<p>for (int j = 0; j &lt; arrayLength - 1 - i; j++) {</p>
<p>if(data[j] &gt; data[j + 1]){</p>
<p>int temp = data[j + 1];</p>
<p>data[j + 1] = data[j];</p>
<p>data[j] = temp;</p>
<p>flag = true;</p>
<p>}</p>
<p>}</p>
<p>System.out.println(java.util.Arrays.toString(data));</p>
<p>if (!flag)</p>
<p>break;</p>
<p>}</p>
<p>}</p>
<p>public static void main(String[] args) {</p>
<p>int[] data = { 9, -16, 21, 23, -30, -49, 21, 30, 30 };</p>
<p>System.out.println(“排序之前：\n” + java.util.Arrays.toString(data));</p>
<p>bubbleSort(data);</p>
<p>System.out.println(“排序之后：\n” + java.util.Arrays.toString(data));</p>
<p>}</p>
<p>}</p>
<h3 id="8-1-2-二分查找"><a href="#8-1-2-二分查找" class="headerlink" title="8.1.2 二分查找"></a>8.1.2 二分查找</h3><p>图4-二分查找核心思路</p>
<p>实现代码：</p>
<p>/**</p>
<p>* 二分查找 时间复杂度O(log2n);空间复杂度O(1)</p>
<p>*/</p>
<p>def binarySearch(arr:Array[Int],left:Int,right:Int,findVal:Int): Int={</p>
<p>if(left&gt;right){//递归退出条件，找不到，返回-1</p>
<p>-1</p>
<p>}</p>
<p>val midIndex = (left+right)/2</p>
<p>if (findVal &lt; arr(midIndex)){//向左递归查找</p>
<p>binarySearch(arr,left,midIndex-1,findVal)</p>
<p>}else if(findVal &gt; arr(midIndex)){//向右递归查找</p>
<p>binarySearch(arr,midIndex+1,right,findVal)</p>
<p>}else{//查找到，返回下标</p>
<p>midIndex</p>
<p>}</p>
<p>}</p>
<p>拓展需求：当一个有序数组中，有多个相同的数值时，如何将所有的数值都查找到。</p>
<p>代码实现如下：</p>
<p>/*</p>
<p>{1,8, 10, 89, 1000, 1000，1234}<br>当一个有序数组中，有多个相同的数值时，如何将所有的数值都查找到，比如这里的 1000.</p>
<p>//分析</p>
<ol>
<li><p>返回的结果是一个可变数组 ArrayBuffer</p>
</li>
<li><p>在找到结果时，向左边扫描，向右边扫描 [条件]</p>
</li>
<li><p>找到结果后，就加入到ArrayBuffer</p>
</li>
</ol>
<p>*/</p>
<p>def binarySearch2(arr: Array[Int], l: Int, r: Int,</p>
<p>findVal: Int): ArrayBuffer[Int] = {</p>
<p>//找不到条件?</p>
<p>if (l &gt; r) {</p>
<p>return ArrayBuffer()</p>
<p>}</p>
<p>val midIndex = (l + r) / 2</p>
<p>val midVal = arr(midIndex)</p>
<p>if (midVal &gt; findVal) {</p>
<p>//向左进行递归查找</p>
<p>binarySearch2(arr, l, midIndex - 1, findVal)</p>
<p>} else if (midVal &lt; findVal) { //向右进行递归查找</p>
<p>binarySearch2(arr, midIndex + 1, r, findVal)</p>
<p>} else {</p>
<p>println(“midIndex=” + midIndex)</p>
<p>//定义一个可变数组</p>
<p>val resArr = ArrayBuffer<a href="">Int</a></p>
<p>//向左边扫描</p>
<p>var temp = midIndex - 1</p>
<p>breakable {</p>
<p>while (true) {</p>
<p>if (temp &lt; 0 || arr(temp) != findVal) {</p>
<p>break()</p>
<p>}</p>
<p>if (arr(temp) == findVal) {</p>
<p>resArr.append(temp)</p>
<p>}</p>
<p>temp -= 1</p>
<p>}</p>
<p>}</p>
<p>//将中间这个索引加入</p>
<p>resArr.append(midIndex)</p>
<p>//向右边扫描</p>
<p>temp = midIndex + 1</p>
<p>breakable {</p>
<p>while (true) {</p>
<p>if (temp &gt; arr.length - 1 || arr(temp) != findVal) {</p>
<p>break()</p>
<p>}</p>
<p>if (arr(temp) == findVal) {</p>
<p>resArr.append(temp)</p>
<p>}</p>
<p>temp += 1</p>
<p>}</p>
<p>}</p>
<p>return resArr</p>
<p>}</p>
<h3 id="8-1-3-快排"><a href="#8-1-3-快排" class="headerlink" title="8.1.3 快排"></a>8.1.3 快排</h3><p>图1-快速排序核心思想</p>
<p>代码实现：</p>
<p>/**</p>
<p>* 快排</p>
<blockquote>
<p>  * 时间复杂度:平均时间复杂度为O(nlogn)</p>
</blockquote>
<p>* 空间复杂度:O(logn)，因为递归栈空间的使用问题</p>
<p>*/</p>
<p>def quickSort(list: List[Int]): List[Int] = list match {</p>
<p>case Nil =&gt; Nil</p>
<p>case List() =&gt; List()</p>
<p>case head :: tail =&gt;</p>
<p>val (left, right) = tail.partition(_ &lt; head)</p>
<p>quickSort(left) ::: head :: quickSort(right)</p>
<p>}</p>
<h3 id="8-1-4-归并"><a href="#8-1-4-归并" class="headerlink" title="8.1.4 归并"></a>8.1.4 归并</h3><p>图2-归并排序核心思想</p>
<p>核心思想：不断的将大的数组分成两个小数组，直到不能拆分为止，即形成了单个值。此时使用合并的排序思想对已经有序的数组进行合并，合并为一个大的数据，不断重复此过程，直到最终所有数据合并到一个数组为止。</p>
<p>图3-归并排序“治”流程</p>
<p>代码实现：</p>
<p>/**</p>
<p>* 快排</p>
<blockquote>
<p>  * 时间复杂度:O(nlogn)</p>
</blockquote>
<p>* 空间复杂度:O(n)</p>
<p>*/</p>
<p>def merge(left: List[Int], right: List[Int]): List[Int] = (left, right) match {</p>
<p>case (Nil, _) =&gt; right</p>
<p>case (_, Nil) =&gt; left</p>
<p>case (x :: xTail, y :: yTail) =&gt;</p>
<p>if (x &lt;= y) x :: merge(xTail, right)</p>
<p>else y :: merge(left, yTail)</p>
<p>}</p>
<h3 id="8-1-5-二叉树之Scala实现"><a href="#8-1-5-二叉树之Scala实现" class="headerlink" title="8.1.5 二叉树之Scala实现"></a>8.1.5 二叉树之Scala实现</h3><p>1）二叉树概念</p>
<p>2）二叉树的特点</p>
<p>（1）树执行查找、删除、插入的时间复杂度都是O(logN)</p>
<p>（2）遍历二叉树的方法包括前序、中序、后序</p>
<p>（3）非平衡树指的是根的左右两边的子节点的数量不一致</p>
<p>（4）在非空二叉树中，第i层的结点总数不超过 , i&gt;=1；</p>
<p>（5）深度为h的二叉树最多有个结点(h&gt;=1)，最少有h个结点；</p>
<p>（6）对于任意一棵二叉树，如果其叶结点数为N0，而度数为2的结点总数为N2，则N0=N2+1；</p>
<p>3） 二叉树的Scala代码实现</p>
<p>定义节点以及前序、中序、后序遍历</p>
<p>class TreeNode(treeNo:Int){</p>
<p>val no = treeNo</p>
<p>var left:TreeNode = null</p>
<p>var right:TreeNode = null</p>
<p>//后序遍历</p>
<p>def postOrder():Unit={</p>
<p>//向左递归输出左子树</p>
<p>if(this.left != null){</p>
<p>this.left.postOrder</p>
<p>}</p>
<p>//向右递归输出右子树</p>
<p>if (this.right != null) {</p>
<p>this.right.postOrder</p>
<p>}</p>
<p>//输出当前节点值</p>
<p>printf(“节点信息 no=%d \n”,no)</p>
<p>}</p>
<p>//中序遍历</p>
<p>def infixOrder():Unit={</p>
<p>//向左递归输出左子树</p>
<p>if(this.left != null){</p>
<p>this.left.infixOrder()</p>
<p>}</p>
<p>//输出当前节点值</p>
<p>printf(“节点信息 no=%d \n”,no)</p>
<p>//向右递归输出右子树</p>
<p>if (this.right != null) {</p>
<p>this.right.infixOrder()</p>
<p>}</p>
<p>}</p>
<p>//前序遍历</p>
<p>def preOrder():Unit={</p>
<p>//输出当前节点值</p>
<p>printf(“节点信息 no=%d \n”,no)</p>
<p>//向左递归输出左子树</p>
<p>if(this.left != null){</p>
<p>this.left.postOrder()</p>
<p>}</p>
<p>//向右递归输出右子树</p>
<p>if (this.right != null) {</p>
<p>this.right.preOrder()</p>
<p>}</p>
<p>}</p>
<p>//后序遍历查找</p>
<p>def postOrderSearch(no:Int): TreeNode = {</p>
<p>//向左递归输出左子树</p>
<p>var resNode:TreeNode = null</p>
<p>if (this.left != null) {</p>
<p>resNode = this.left.postOrderSearch(no)</p>
<p>}</p>
<p>if (resNode != null) {</p>
<p>return resNode</p>
<p>}</p>
<p>if (this.right != null) {</p>
<p>resNode = this.right.postOrderSearch(no)</p>
<p>}</p>
<p>if (resNode != null) {</p>
<p>return resNode</p>
<p>}</p>
<p>println(“ttt~~“)</p>
<p>if (this.no == no) {</p>
<p>return this</p>
<p>}</p>
<p>resNode</p>
<p>}</p>
<p>//中序遍历查找</p>
<p>def infixOrderSearch(no:Int): TreeNode = {</p>
<p>var resNode : TreeNode = null</p>
<p>//先向左递归查找</p>
<p>if (this.left != null) {</p>
<p>resNode = this.left.infixOrderSearch(no)</p>
<p>}</p>
<p>if (resNode != null) {</p>
<p>return resNode</p>
<p>}</p>
<p>println(“yyy~~“)</p>
<p>if (no == this.no) {</p>
<p>return this</p>
<p>}</p>
<p>//向右递归查找</p>
<p>if (this.right != null) {</p>
<p>resNode = this.right.infixOrderSearch(no)</p>
<p>}</p>
<p>return resNode</p>
<p>}</p>
<p>//前序查找</p>
<p>def preOrderSearch(no:Int): TreeNode = {</p>
<p>if (no == this.no) {</p>
<p>return this</p>
<p>}</p>
<p>//向左递归查找</p>
<p>var resNode : TreeNode = null</p>
<p>if (this.left != null) {</p>
<p>resNode = this.left.preOrderSearch(no)</p>
<p>}</p>
<p>if (resNode != null){</p>
<p>return resNode</p>
<p>}</p>
<p>//向右边递归查找</p>
<p>if (this.right != null) {</p>
<p>resNode = this.right.preOrderSearch(no)</p>
<p>}</p>
<p>return resNode</p>
<p>}</p>
<p>//删除节点</p>
<p>//删除节点规则</p>
<p>//1如果删除的节点是叶子节点，则删除该节点</p>
<p>//2如果删除的节点是非叶子节点，则删除该子树</p>
<p>def delNode(no:Int): Unit = {</p>
<p>//首先比较当前节点的左子节点是否为要删除的节点</p>
<p>if (this.left != null &amp;&amp; this.left.no == no) {</p>
<p>this.left = null</p>
<p>return</p>
<p>}</p>
<p>//比较当前节点的右子节点是否为要删除的节点</p>
<p>if (this.right != null &amp;&amp; this.right.no == no) {</p>
<p>this.right = null</p>
<p>return</p>
<p>}</p>
<p>//向左递归删除</p>
<p>if (this.left != null) {</p>
<p>this.left.delNode(no)</p>
<p>}</p>
<p>//向右递归删除</p>
<p>if (this.right != null) {</p>
<p>this.right.delNode(no)</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>定义二叉树，前序、中序、后序遍历，前序、中序、后序查找，删除节点</p>
<p>class BinaryTree{</p>
<p>var root:TreeNode = null</p>
<p>//后序遍历</p>
<p>def postOrder(): Unit = {</p>
<p>if (root != null){</p>
<p>root.postOrder()</p>
<p>}else {</p>
<p>println(“当前二叉树为空，不能遍历”)</p>
<p>}</p>
<p>}</p>
<p>//中序遍历</p>
<p>def infixOrder(): Unit = {</p>
<p>if (root != null){</p>
<p>root.infixOrder()</p>
<p>}else {</p>
<p>println(“当前二叉树为空，不能遍历”)</p>
<p>}</p>
<p>}</p>
<p>//前序遍历</p>
<p>def preOrder(): Unit = {</p>
<p>if (root != null){</p>
<p>root.preOrder()</p>
<p>}else {</p>
<p>println(“当前二叉树为空，不能遍历”)</p>
<p>}</p>
<p>}</p>
<p>//后序遍历查找</p>
<p>def postOrderSearch(no:Int): TreeNode = {</p>
<p>if (root != null) {</p>
<p>root.postOrderSearch(no)</p>
<p>}else{</p>
<p>null</p>
<p>}</p>
<p>}</p>
<p>//中序遍历查找</p>
<p>def infixOrderSeacher(no:Int): TreeNode = {</p>
<p>if (root != null) {</p>
<p>return root.infixOrderSearch(no)</p>
<p>}else {</p>
<p>return null</p>
<p>}</p>
<p>}</p>
<p>//前序查找</p>
<p>def preOrderSearch(no:Int): TreeNode = {</p>
<p>if (root != null) {</p>
<p>return root.preOrderSearch(no)</p>
<p>}else{</p>
<p>//println(“当前二叉树为空，不能查找”)</p>
<p>return null</p>
<p>}</p>
<p>}</p>
<p>//删除节点</p>
<p>def delNode(no:Int): Unit = {</p>
<p>if (root != null) {</p>
<p>//先处理一下root是不是要删除的</p>
<p>if (root.no == no){</p>
<p>root = null</p>
<p>}else {</p>
<p>root.delNode(no)</p>
<p>}</p>
<p>}</p>
<p>}</p>
<h2 id="8-2-开发代码"><a href="#8-2-开发代码" class="headerlink" title="8.2 开发代码"></a>8.2 开发代码</h2><h3 id="8-2-1-手写Spark-WordCount"><a href="#8-2-1-手写Spark-WordCount" class="headerlink" title="8.2.1 手写Spark-WordCount"></a>8.2.1 手写Spark-WordCount</h3><p>val conf: SparkConf =</p>
<p>new SparkConf().setMaster(“local[*]”).setAppName(“WordCount”)</p>
<p>val sc = new SparkContext(conf)</p>
<p>sc.textFile(“/input”)</p>
<p>.flatMap(_.split(“ “))</p>
<p>.map((_, 1))</p>
<p>.reduceByKey(_ + _)</p>
<p>.saveAsTextFile(“/output”)</p>
<p>sc.stop()</p>
<h2 id="8-3-手写HQL"><a href="#8-3-手写HQL" class="headerlink" title="8.3 手写HQL"></a>8.3 手写HQL</h2><h3 id="8-3-1-手写HQL-第1题"><a href="#8-3-1-手写HQL-第1题" class="headerlink" title="8.3.1 手写HQL 第1题"></a>8.3.1 手写HQL 第1题</h3><p>表结构：uid,subject_id,score</p>
<p>求：找出所有科目成绩都大于某一学科平均成绩的学生</p>
<p>数据集如下</p>
<p>1001 01 90</p>
<p>1001 02 90</p>
<p>1001 03 90</p>
<p>1002 01 85</p>
<p>1002 02 85</p>
<p>1002 03 70</p>
<p>1003 01 70</p>
<p>1003 02 70</p>
<p>1003 03 85</p>
<p>1）建表语句</p>
<p>create table score(</p>
<p>uid string,</p>
<p>subject_id string,</p>
<p>score int)</p>
<p>row format delimited fields terminated by ‘\t’;</p>
<p>2）求出每个学科平均成绩</p>
<p>select</p>
<p>uid,</p>
<p>score,</p>
<p>avg(score) over(partition by subject_id) avg_score</p>
<p>from</p>
<p>score;t1</p>
<p>3）根据是否大于平均成绩记录flag，大于则记为0否则记为1</p>
<p>select</p>
<p>uid,</p>
<p>if(score&gt;avg_score,0,1) flag</p>
<p>from</p>
<p>t1;t2</p>
<p>4）根据学生id进行分组统计flag的和，和为0则是所有学科都大于平均成绩</p>
<p>select</p>
<p>uid</p>
<p>from</p>
<p>t2</p>
<p>group by</p>
<p>uid</p>
<p>having</p>
<p>sum(flag)=0;</p>
<p>5）最终SQL</p>
<p>select</p>
<p>uid</p>
<p>from</p>
<p>(select</p>
<p>uid,</p>
<p>if(score&gt;avg_score,0,1) flag</p>
<p>from</p>
<p>(select</p>
<p>uid,</p>
<p>score,</p>
<p>avg(score) over(partition by subject_id) avg_score</p>
<p>from</p>
<p>score)t1)t2</p>
<p>group by</p>
<p>uid</p>
<p>having</p>
<p>sum(flag)=0;</p>
<h3 id="8-3-2-手写HQL-第2题"><a href="#8-3-2-手写HQL-第2题" class="headerlink" title="8.3.2 手写HQL 第2题"></a>8.3.2 手写HQL 第2题</h3><p>我们有如下的用户访问数据</p>
<table>
<thead>
<tr>
<th>userId</th>
<th>visitDate</th>
<th>visitCount</th>
</tr>
</thead>
<tbody><tr>
<td>u01</td>
<td>2017/1/21</td>
<td>5</td>
</tr>
<tr>
<td>u02</td>
<td>2017/1/23</td>
<td>6</td>
</tr>
<tr>
<td>u03</td>
<td>2017/1/22</td>
<td>8</td>
</tr>
<tr>
<td>u04</td>
<td>2017/1/20</td>
<td>3</td>
</tr>
<tr>
<td>u01</td>
<td>2017/1/23</td>
<td>6</td>
</tr>
<tr>
<td>u01</td>
<td>2017/2/21</td>
<td>8</td>
</tr>
<tr>
<td>U02</td>
<td>2017/1/23</td>
<td>6</td>
</tr>
<tr>
<td>U01</td>
<td>2017/2/22</td>
<td>4</td>
</tr>
</tbody></table>
<p>要求使用SQL统计出每个用户的累积访问次数，如下表所示：</p>
<table>
<thead>
<tr>
<th>用户id</th>
<th>月份</th>
<th>小计</th>
<th>累积</th>
</tr>
</thead>
<tbody><tr>
<td>u01</td>
<td>2017-01</td>
<td>11</td>
<td>11</td>
</tr>
<tr>
<td>u01</td>
<td>2017-02</td>
<td>12</td>
<td>23</td>
</tr>
<tr>
<td>u02</td>
<td>2017-01</td>
<td>12</td>
<td>12</td>
</tr>
<tr>
<td>u03</td>
<td>2017-01</td>
<td>8</td>
<td>8</td>
</tr>
<tr>
<td>u04</td>
<td>2017-01</td>
<td>3</td>
<td>3</td>
</tr>
</tbody></table>
<p>数据集</p>
<p>u01 2017/1/21 5</p>
<p>u02 2017/1/23 6</p>
<p>u03 2017/1/22 8</p>
<p>u04 2017/1/20 3</p>
<p>u01 2017/1/23 6</p>
<p>u01 2017/2/21 8</p>
<p>u02 2017/1/23 6</p>
<p>u01 2017/2/22 4</p>
<p>1）创建表</p>
<p>create table action</p>
<p>(userId string,</p>
<p>visitDate string,</p>
<p>visitCount int)</p>
<p>row format delimited fields terminated by “\t”;</p>
<p>2）修改数据格式</p>
<p>select</p>
<p>userId,</p>
<p>date_format(regexp_replace(visitDate,’/‘,’-‘),’yyyy-MM’) mn,</p>
<p>visitCount</p>
<p>from</p>
<p>action;t1</p>
<p>3）计算每人单月访问量</p>
<p>select</p>
<p>userId,</p>
<p>mn,</p>
<p>sum(visitCount) mn_count</p>
<p>from</p>
<p>t1</p>
<p>group by</p>
<p>userId,mn;t2</p>
<p>4）按月累计访问量</p>
<p>select</p>
<p>userId,</p>
<p>mn,</p>
<p>mn_count,</p>
<p>sum(mn_count) over(partition by userId order by mn)</p>
<p>from t2;</p>
<p>5）最终SQL</p>
<p>select</p>
<p>userId,</p>
<p>mn,</p>
<p>mn_count,</p>
<p>sum(mn_count) over(partition by userId order by mn)</p>
<p>from</p>
<p>( select</p>
<p>userId,</p>
<p>mn,</p>
<p>sum(visitCount) mn_count</p>
<p>from</p>
<p>(select</p>
<p>userId,</p>
<p>date_format(regexp_replace(visitDate,’/‘,’-‘),’yyyy-MM’) mn,</p>
<p>visitCount</p>
<p>from</p>
<p>action)t1</p>
<p>group by userId,mn)t2;</p>
<h3 id="8-3-3-手写HQL-第3题"><a href="#8-3-3-手写HQL-第3题" class="headerlink" title="8.3.3 手写HQL 第3题"></a>8.3.3 手写HQL 第3题</h3><p>有50W个京东店铺，每个顾客访客访问任何一个店铺的任何一个商品时都会产生一条访问日志，访问日志存储的表名为Visit，访客的用户id为user_id，被访问的店铺名称为shop，请统计：</p>
<p>1）每个店铺的UV（访客数）</p>
<p>2）每个店铺访问次数top3的访客信息。输出店铺名称、访客id、访问次数</p>
<p>数据集</p>
<p>u1 a</p>
<p>u2 b</p>
<p>u1 b</p>
<p>u1 a</p>
<p>u3 c</p>
<p>u4 b</p>
<p>u1 a</p>
<p>u2 c</p>
<p>u5 b</p>
<p>u4 b</p>
<p>u6 c</p>
<p>u2 c</p>
<p>u1 b</p>
<p>u2 a</p>
<p>u2 a</p>
<p>u3 a</p>
<p>u5 a</p>
<p>u5 a</p>
<p>u5 a</p>
<p>1）建表</p>
<p>create table visit(user_id string,shop string) row format delimited fields<br>terminated by ‘\t’;</p>
<p>2）每个店铺的UV（访客数）</p>
<p>select shop,count(distinct user_id) from visit group by shop;</p>
<p>3）每个店铺访问次数top3的访客信息。输出店铺名称、访客id、访问次数</p>
<p>（1）查询每个店铺被每个用户访问次数</p>
<p>select shop,user_id,count(*) ct</p>
<p>from visit</p>
<p>group by shop,user_id;t1</p>
<p>（2）计算每个店铺被用户访问次数排名</p>
<p>select shop,user_id,ct,rank() over(partition by shop order by ct) rk</p>
<p>from t1;t2</p>
<p>（3）取每个店铺排名前3的</p>
<p>select shop,user_id,ct</p>
<p>from t2</p>
<p>where rk&lt;=3;</p>
<p>（4）最终SQL</p>
<p>select</p>
<p>shop,</p>
<p>user_id,</p>
<p>ct</p>
<p>from</p>
<p>(select</p>
<p>shop,</p>
<p>user_id,</p>
<p>ct,</p>
<p>rank() over(partition by shop order by ct) rk</p>
<p>from</p>
<p>(select</p>
<p>shop,</p>
<p>user_id,</p>
<p>count(*) ct</p>
<p>from visit</p>
<p>group by</p>
<p>shop,</p>
<p>user_id)t1</p>
<p>)t2</p>
<p>where rk&lt;=3;</p>
<h3 id="8-3-4-手写HQL-第4题"><a href="#8-3-4-手写HQL-第4题" class="headerlink" title="8.3.4 手写HQL 第4题"></a>8.3.4 手写HQL 第4题</h3><p>已知一个表STG.ORDER，有如下字段:Date，Order_id，User_id，amount。请给出sql进行统计:数据样例:2017-01-01,10029028,1000003251,33.57。</p>
<p>1）给出 2017年每个月的订单数、用户数、总成交金额。</p>
<p>2）给出2017年11月的新客数(指在11月才有第一笔订单)</p>
<p>建表</p>
<p>create table order_tab(dt string,order_id string,user_id string,amount<br>decimal(10,2)) row format delimited fields terminated by ‘\t’;</p>
<p>1）给出 2017年每个月的订单数、用户数、总成交金额。</p>
<p>select</p>
<p>date_format(dt,’yyyy-MM’),</p>
<p>count(order_id),</p>
<p>count(distinct user_id),</p>
<p>sum(amount)</p>
<p>from</p>
<p>order_tab</p>
<p>where</p>
<p>date_format(dt,’yyyy’)=’2017’</p>
<p>group by</p>
<p>date_format(dt,’yyyy-MM’);</p>
<p>2）给出2017年11月的新客数(指在11月才有第一笔订单)</p>
<p>select</p>
<p>count(user_id)</p>
<p>from</p>
<p>order_tab</p>
<p>group by</p>
<p>user_id</p>
<p>having</p>
<p>date_format(min(dt),’yyyy-MM’)=’2017-11’;</p>
<h3 id="8-3-5-手写HQL-第5题"><a href="#8-3-5-手写HQL-第5题" class="headerlink" title="8.3.5 手写HQL 第5题"></a>8.3.5 手写HQL 第5题</h3><p>有日志如下，请写出代码求得所有用户和活跃用户的总数及平均年龄。（活跃用户指连续两天都有访问记录的用户）日期<br>用户 年龄</p>
<p>数据集</p>
<p>2019-02-11,test_1,23</p>
<p>2019-02-11,test_2,19</p>
<p>2019-02-11,test_3,39</p>
<p>2019-02-11,test_1,23</p>
<p>2019-02-11,test_3,39</p>
<p>2019-02-11,test_1,23</p>
<p>2019-02-12,test_2,19</p>
<p>2019-02-13,test_1,23</p>
<p>2019-02-15,test_2,19</p>
<p>2019-02-16,test_2,19</p>
<p>1）建表</p>
<p>create table user_age(dt string,user_id string,age int)row format delimited<br>fields terminated by ‘,’;</p>
<p>2）按照日期以及用户分组，按照日期排序并给出排名</p>
<p>select</p>
<p>dt,</p>
<p>user_id,</p>
<p>min(age) age,</p>
<p>rank() over(partition by user_id order by dt) rk</p>
<p>from</p>
<p>user_age</p>
<p>group by</p>
<p>dt,user_id;t1</p>
<p>3）计算日期及排名的差值</p>
<p>select</p>
<p>user_id,</p>
<p>age,</p>
<p>date_sub(dt,rk) flag</p>
<p>from</p>
<p>t1;t2</p>
<p>4）过滤出差值大于等于2的，即为连续两天活跃的用户</p>
<p>select</p>
<p>user_id,</p>
<p>min(age) age</p>
<p>from</p>
<p>t2</p>
<p>group by</p>
<p>user_id,flag</p>
<p>having</p>
<p>count(*)&gt;=2;t3</p>
<p>5）对数据进行去重处理（一个用户可以在两个不同的时间点连续登录），例如：a用户在1月10号1月11号以及1月20号和1月21号4天登录。</p>
<p>select</p>
<p>user_id,</p>
<p>min(age) age</p>
<p>from</p>
<p>t3</p>
<p>group by</p>
<p>user_id;t4</p>
<p>6）计算活跃用户（两天连续有访问）的人数以及平均年龄</p>
<p>select</p>
<p>count(*) ct,</p>
<p>cast(sum(age)/count(*) as decimal(10,2))</p>
<p>from t4;</p>
<p>7）对全量数据集进行按照用户去重</p>
<p>select</p>
<p>user_id,</p>
<p>min(age) age</p>
<p>from</p>
<p>user_age</p>
<p>group by</p>
<p>user_id;t5</p>
<p>8）计算所有用户的数量以及平均年龄</p>
<p>select</p>
<p>count(*) user_count,</p>
<p>cast((sum(age)/count(*)) as decimal(10,1))</p>
<p>from</p>
<p>t5;</p>
<p>9）将第5步以及第7步两个数据集进行union all操作</p>
<p>select</p>
<p>0 user_total_count,</p>
<p>0 user_total_avg_age,</p>
<p>count(*) twice_count,</p>
<p>cast(sum(age)/count(*) as decimal(10,2)) twice_count_avg_age</p>
<p>from</p>
<p>(</p>
<p>select</p>
<p>user_id,</p>
<p>min(age) age</p>
<p>from</p>
<p>(select</p>
<p>user_id,</p>
<p>min(age) age</p>
<p>from</p>
<p>(</p>
<p>select</p>
<p>user_id,</p>
<p>age,</p>
<p>date_sub(dt,rk) flag</p>
<p>from</p>
<p>(</p>
<p>select</p>
<p>dt,</p>
<p>user_id,</p>
<p>min(age) age,</p>
<p>rank() over(partition by user_id order by dt) rk</p>
<p>from</p>
<p>user_age</p>
<p>group by</p>
<p>dt,user_id</p>
<p>)t1</p>
<p>)t2</p>
<p>group by</p>
<p>user_id,flag</p>
<p>having</p>
<p>count(*)&gt;=2)t3</p>
<p>group by</p>
<p>user_id</p>
<p>)t4</p>
<p>union all</p>
<p>select</p>
<p>count(*) user_total_count,</p>
<p>cast((sum(age)/count(*)) as decimal(10,1)),</p>
<p>0 twice_count,</p>
<p>0 twice_count_avg_age</p>
<p>from</p>
<p>(</p>
<p>select</p>
<p>user_id,</p>
<p>min(age) age</p>
<p>from</p>
<p>user_age</p>
<p>group by</p>
<p>user_id</p>
<p>)t5;t6</p>
<p>10）求和并拼接为最终SQL</p>
<p>select</p>
<p>sum(user_total_count),</p>
<p>sum(user_total_avg_age),</p>
<p>sum(twice_count),</p>
<p>sum(twice_count_avg_age)</p>
<p>from</p>
<p>(select</p>
<p>0 user_total_count,</p>
<p>0 user_total_avg_age,</p>
<p>count(*) twice_count,</p>
<p>cast(sum(age)/count(*) as decimal(10,2)) twice_count_avg_age</p>
<p>from</p>
<p>(</p>
<p>select</p>
<p>user_id,</p>
<p>min(age) age</p>
<p>from</p>
<p>(select</p>
<p>user_id,</p>
<p>min(age) age</p>
<p>from</p>
<p>(</p>
<p>select</p>
<p>user_id,</p>
<p>age,</p>
<p>date_sub(dt,rk) flag</p>
<p>from</p>
<p>(</p>
<p>select</p>
<p>dt,</p>
<p>user_id,</p>
<p>min(age) age,</p>
<p>rank() over(partition by user_id order by dt) rk</p>
<p>from</p>
<p>user_age</p>
<p>group by</p>
<p>dt,user_id</p>
<p>)t1</p>
<p>)t2</p>
<p>group by</p>
<p>user_id,flag</p>
<p>having</p>
<p>count(*)&gt;=2)t3</p>
<p>group by</p>
<p>user_id</p>
<p>)t4</p>
<p>union all</p>
<p>select</p>
<p>count(*) user_total_count,</p>
<p>cast((sum(age)/count(*)) as decimal(10,1)),</p>
<p>0 twice_count,</p>
<p>0 twice_count_avg_age</p>
<p>from</p>
<p>(</p>
<p>select</p>
<p>user_id,</p>
<p>min(age) age</p>
<p>from</p>
<p>user_age</p>
<p>group by</p>
<p>user_id</p>
<p>)t5)t6;</p>
<h3 id="8-3-6-手写HQL-第6题"><a href="#8-3-6-手写HQL-第6题" class="headerlink" title="8.3.6 手写HQL 第6题"></a>8.3.6 手写HQL 第6题</h3><p>请用sql写出所有用户中在今年10月份第一次购买商品的金额，表ordertable字段（购买用户：userid，金额：money，购买时间：paymenttime(格式：2017-10-01)，订单id：orderid）</p>
<p>1）建表</p>
<p>create table ordertable(</p>
<p>userid string,</p>
<p>money int,</p>
<p>paymenttime string,</p>
<p>orderid string)</p>
<p>row format delimited fields terminated by ‘\t’;</p>
<p>2）查询出</p>
<p>select</p>
<p>userid,</p>
<p>min(paymenttime) paymenttime</p>
<p>from</p>
<p>ordertable</p>
<p>where</p>
<p>date_format(paymenttime,’yyyy-MM’)=’2017-10’</p>
<p>group by</p>
<p>userid;t1</p>
<p>select</p>
<p>t1.userid,</p>
<p>t1.paymenttime,</p>
<p>od.money</p>
<p>from</p>
<p>t1</p>
<p>join</p>
<p>ordertable od</p>
<p>on</p>
<p>t1.userid=od.userid</p>
<p>and</p>
<p>t1.paymenttime=od.paymenttime;</p>
<p>select</p>
<p>t1.userid,</p>
<p>t1.paymenttime,</p>
<p>od.money</p>
<p>from</p>
<p>(select</p>
<p>userid,</p>
<p>min(paymenttime) paymenttime</p>
<p>from</p>
<p>ordertable</p>
<p>where</p>
<p>date_format(paymenttime,’yyyy-MM’)=’2017-10’</p>
<p>group by</p>
<p>userid)t1</p>
<p>join</p>
<p>ordertable od</p>
<p>on</p>
<p>t1.userid=od.userid</p>
<p>and</p>
<p>t1.paymenttime=od.paymenttime;</p>
<h3 id="8-3-7-手写HQL-第7题"><a href="#8-3-7-手写HQL-第7题" class="headerlink" title="8.3.7 手写HQL 第7题"></a>8.3.7 手写HQL 第7题</h3><p>有一个线上服务器访问日志格式如下（用sql答题）</p>
<p>时间 接口 ip地址</p>
<p>2016-11-09 11：22：05 /api/user/login 110.23.5.33</p>
<p>2016-11-09 11：23：10 /api/user/detail 57.3.2.16</p>
<p>…..</p>
<p>2016-11-09 23：59：40 /api/user/login 200.6.5.166</p>
<p>求11月9号下午14点（14-15点），访问api/user/login接口的top10的ip地址</p>
<p>数据集</p>
<p>2016-11-09 14:22:05 /api/user/login 110.23.5.33</p>
<p>2016-11-09 11:23:10 /api/user/detail 57.3.2.16</p>
<p>2016-11-09 14:59:40 /api/user/login 200.6.5.166</p>
<p>2016-11-09 14:22:05 /api/user/login 110.23.5.34</p>
<p>2016-11-09 14:22:05 /api/user/login 110.23.5.34</p>
<p>2016-11-09 14:22:05 /api/user/login 110.23.5.34</p>
<p>2016-11-09 11:23:10 /api/user/detail 57.3.2.16</p>
<p>2016-11-09 23:59:40 /api/user/login 200.6.5.166</p>
<p>2016-11-09 14:22:05 /api/user/login 110.23.5.34</p>
<p>2016-11-09 11:23:10 /api/user/detail 57.3.2.16</p>
<p>2016-11-09 23:59:40 /api/user/login 200.6.5.166</p>
<p>2016-11-09 14:22:05 /api/user/login 110.23.5.35</p>
<p>2016-11-09 14:23:10 /api/user/detail 57.3.2.16</p>
<p>2016-11-09 23:59:40 /api/user/login 200.6.5.166</p>
<p>2016-11-09 14:59:40 /api/user/login 200.6.5.166</p>
<p>2016-11-09 14:59:40 /api/user/login 200.6.5.166</p>
<p>1）建表</p>
<p>create table ip(</p>
<p>time string,</p>
<p>interface string,</p>
<p>ip string)</p>
<p>row format delimited fields terminated by ‘\t’;</p>
<p>2）最终SQL</p>
<p>select</p>
<p>ip,</p>
<p>interface,</p>
<p>count(*) ct</p>
<p>from</p>
<p>ip</p>
<p>where</p>
<p>date_format(time,’yyyy-MM-dd HH’)&gt;=’2016-11-09 14’</p>
<p>and</p>
<p>date_format(time,’yyyy-MM-dd HH’)&lt;=’2016-11-09 15’</p>
<p>and</p>
<p>interface=’/api/user/login’</p>
<p>group by</p>
<p>ip,interface</p>
<p>order by</p>
<p>ct desc</p>
<p>limit 2;t1</p>
<h3 id="8-3-8-手写SQL-第8题"><a href="#8-3-8-手写SQL-第8题" class="headerlink" title="8.3.8 手写SQL 第8题"></a>8.3.8 手写SQL 第8题</h3><p>有一个账号表如下，请写出SQL语句，查询各自区组的money排名前十的账号（分组取前10）</p>
<p>1）建表（MySQL）</p>
<p>CREATE TABLE `account`</p>
<p>( `dist_id` int（11）DEFAULT NULL COMMENT ‘区组id’,</p>
<p>`account` varchar（100）DEFAULT NULL COMMENT ‘账号’,</p>
<p>`gold` int（11）DEFAULT 0 COMMENT ‘金币’）;</p>
<p>2）最终SQL</p>
<p>select</p>
<p>*</p>
<p>from</p>
<p>account as a</p>
<p>where</p>
<p>(select</p>
<p>count(distinct(a1.gold))</p>
<p>from</p>
<p>account as a1</p>
<p>where</p>
<p>a1.dist_id=a.dist_id</p>
<p>and</p>
<p>a1.gold&gt;a.gold)&lt;3;</p>
<h3 id="8-3-9-手写HQL-第9题"><a href="#8-3-9-手写HQL-第9题" class="headerlink" title="8.3.9 手写HQL 第9题"></a>8.3.9 手写HQL 第9题</h3><p>1）有三张表分别为会员表（member）销售表（sale）退货表（regoods）</p>
<p>（1）会员表有字段memberid（会员id，主键）credits（积分）；</p>
<p>（2）销售表有字段memberid（会员id，外键）购买金额（MNAccount）；</p>
<p>（3）退货表中有字段memberid（会员id，外键）退货金额（RMNAccount）。</p>
<p>2）业务说明</p>
<p>（1）销售表中的销售记录可以是会员购买，也可以是非会员购买。（即销售表中的memberid可以为空）；</p>
<p>（2）销售表中的一个会员可以有多条购买记录；</p>
<p>（3）退货表中的退货记录可以是会员，也可是非会员；</p>
<p>（4）一个会员可以有一条或多条退货记录。</p>
<p>查询需求：分组查出销售表中所有会员购买金额，同时分组查出退货表中所有会员的退货金额，把会员id相同的购买金额-退款金额得到的结果更新到表会员表中对应会员的积分字段（credits）</p>
<p>数据集</p>
<p>sale</p>
<p>1001 50.3</p>
<p>1002 56.5</p>
<p>1003 235</p>
<p>1001 23.6</p>
<p>1005 56.2</p>
<p>25.6</p>
<p>33.5</p>
<p>regoods</p>
<p>1001 20.1</p>
<p>1002 23.6</p>
<p>1001 10.1</p>
<p>23.5</p>
<p>10.2</p>
<p>1005 0.8</p>
<p>1）建表</p>
<p>create table member(memberid string,credits double) row format delimited fields<br>terminated by ‘\t’;</p>
<p>create table sale(memberid string,MNAccount double) row format delimited fields<br>terminated by ‘\t’;</p>
<p>create table regoods(memberid string,RMNAccount double) row format delimited<br>fields terminated by ‘\t’;</p>
<p>2）最终SQL</p>
<p>insert into table member</p>
<p>select</p>
<p>t1.memberid,</p>
<p>MNAccount-RMNAccount</p>
<p>from</p>
<p>(select</p>
<p>memberid,</p>
<p>sum(MNAccount) MNAccount</p>
<p>from</p>
<p>sale</p>
<p>where</p>
<p>memberid!=’’</p>
<p>group by</p>
<p>memberid</p>
<p>)t1</p>
<p>join</p>
<p>(select</p>
<p>memberid,</p>
<p>sum(RMNAccount) RMNAccount</p>
<p>from</p>
<p>regoods</p>
<p>where</p>
<p>memberid!=’’</p>
<p>group by</p>
<p>memberid</p>
<p>)t2</p>
<p>on</p>
<p>t1.memberid=t2.memberid;</p>
<h3 id="8-3-10-手写HQL-第10题"><a href="#8-3-10-手写HQL-第10题" class="headerlink" title="8.3.10 手写HQL 第10题"></a>8.3.10 手写HQL 第10题</h3><p>1.用一条SQL语句查询出每门课都大于80分的学生姓名</p>
<p>name kecheng fenshu</p>
<p>张三 语文 81</p>
<p>张三 数学 75</p>
<p>李四 语文 76</p>
<p>李四 数学 90</p>
<p>王五 语文 81</p>
<p>王五 数学 100</p>
<p>王五 英语 90</p>
<p>A: select distinct name from table where name not in (select distinct name from<br>table where fenshu&lt;=80)</p>
<p>B：select name from table group by name having min(fenshu)&gt;80</p>
<ol start="2">
<li>学生表 如下:<br>自动编号 学号 姓名 课程编号 课程名称 分数<br>1 2005001 张三 0001 数学 69<br>2 2005002 李四 0001 数学 89<br>3 2005001 张三 0001 数学 69<br>删除除了自动编号不同, 其他都相同的学生冗余信息  </li>
</ol>
<p>A: delete tablename where 自动编号 not in(select min(自动编号) from tablename<br>group by学号, 姓名, 课程编号, 课程名称, 分数)</p>
<p>3.一个叫team的表，里面只有一个字段name,一共有4条纪录，分别是a,b,c,d,对应四个球队，现在四个球队进行比赛，用一条sql语句显示所有可能的比赛组合.  </p>
<p>答：select a.name, b.name<br>from team a, team b<br>where a.name &lt; b.name</p>
<p>4.面试题：怎么把这样一个<br>year month amount<br>1991 1 1.1<br>1991 2 1.2<br>1991 3 1.3<br>1991 4 1.4<br>1992 1 2.1<br>1992 2 2.2<br>1992 3 2.3<br>1992 4 2.4<br>查成这样一个结果<br>year m1 m2 m3 m4<br>1991 1.1 1.2 1.3 1.4<br>1992 2.1 2.2 2.3 2.4  </p>
<p>答案<br>select year,<br>(select amount from aaa m where month=1 and m.year=aaa.year) as m1,<br>(select amount from aaa m where month=2 and m.year=aaa.year) as m2,<br>(select amount from aaa m where month=3 and m.year=aaa.year) as m3,<br>(select amount from aaa m where month=4 and m.year=aaa.year) as m4<br>from aaa group by year</p>
<p>*********************************************************************<br>5.说明：复制表(只复制结构,源表名：a新表名：b)  </p>
<p>SQL: select * into b from a where 1&lt;&gt;1 (where1=1，拷贝表结构和数据内容)<br>ORACLE:create table b</p>
<p>As</p>
<p>Select * from a where 1=2</p>
<p>[&lt;&gt;（不等于）(SQL Server Compact)</p>
<p>比较两个表达式。<br>当使用此运算符比较非空表达式时，如果左操作数不等于右操作数，则结果为 TRUE。<br>否则，结果为 FALSE。]</p>
<p>6.</p>
<p>原表:<br>courseid coursename score  </p>
<hr>
<p>1 java 70<br>2 oracle 90<br>3 xml 40<br>4 jsp 30<br>5 servlet 80  </p>
<hr>
<p>为了便于阅读,查询此表后的结果显式如下(及格分数为60):<br>courseid coursename score mark  </p>
<hr>
<p>1 java 70 pass<br>2 oracle 90 pass<br>3 xml 40 fail<br>4 jsp 30 fail<br>5 servlet 80 pass  </p>
<hr>
<p>写出此查询语句<br>select courseid, coursename ,score ,if(score&gt;=60, “pass”,”fail”) as mark from<br>course</p>
<p>7.表名：购物信息</p>
<p>购物人 商品名称 数量</p>
<p>A 甲 2</p>
<p>B 乙 4</p>
<p>C 丙 1</p>
<p>A 丁 2</p>
<p>B 丙 5</p>
<p>……</p>
<p>给出所有购入商品为两种或两种以上的购物人记录</p>
<p>答：select * from 购物信息 where 购物人 in (select 购物人 from 购物信息 group<br>by 购物人 having count(*) &gt;= 2);</p>
<p>8.</p>
<p>info 表</p>
<p>date result</p>
<p>2005-05-09 win</p>
<p>2005-05-09 lose</p>
<p>2005-05-09 lose</p>
<p>2005-05-09 lose</p>
<p>2005-05-10 win</p>
<p>2005-05-10 lose</p>
<p>2005-05-10 lose</p>
<p>如果要生成下列结果, 该如何写sql语句?</p>
<p>win lose</p>
<p>2005-05-09 2 2</p>
<p>2005-05-10 1 2</p>
<p>答案：</p>
<p>(1) select date, sum(case when result = “win” then 1 else 0 end) as “win”,<br>sum(case when result = “lose” then 1 else 0 end) as “lose” from info group by<br>date;</p>
<p>(2) select a.date, a.result as win, b.result as lose</p>
<p>from</p>
<p>(select date, count(result) as result from info where result = “win” group by<br>date) as a</p>
<p>join</p>
<p>(select date, count(result) as result from info where result = “lose” group by<br>date) as b</p>
<p>on a.date = b.date;</p>
<h3 id="8-3-11-手写HQL-第11题"><a href="#8-3-11-手写HQL-第11题" class="headerlink" title="8.3.11 手写HQL 第11题"></a>8.3.11 手写HQL 第11题</h3><p>有一个订单表order。已知字段有：order_id(订单ID), user_id(用户ID),amount(金额),<br>pay_datetime(付费时间),channel_id(渠道ID),dt(分区字段)。</p>
<ol>
<li><p>在Hive中创建这个表。</p>
</li>
<li><p>查询dt=‘2018-09-01‘里每个渠道的订单数，下单人数（去重），总金额。</p>
</li>
<li><p>查询dt=‘2018-09-01‘里每个渠道的金额最大3笔订单。</p>
</li>
<li><p>有一天发现订单数据重复，请分析原因</p>
</li>
</ol>
<p>create external table order(</p>
<p>order_id int,</p>
<p>user_id int,</p>
<p>amount double,</p>
<p>pay_datatime timestamp,</p>
<p>channel_id int</p>
<p>)partitioned by(dt string)</p>
<p>row format delimited fields terminated by ‘\t’;</p>
<p>select</p>
<p>count(order_id),</p>
<p>count(distinct(user_id))</p>
<p>sum(amount)</p>
<p>from</p>
<p>order</p>
<p>where dt=”2019-09-01”</p>
<p>select</p>
<p>order_id</p>
<p>channel_id</p>
<p>channel_id_amount</p>
<p>from(</p>
<p>select</p>
<p>order_id</p>
<p>channel_id,</p>
<p>amount,</p>
<p>max(amount) over(partition by channel_id)</p>
<p>min(amount) over(partition by channel_id)</p>
<p>row_number()</p>
<p>over(</p>
<p>partition by channel_id</p>
<p>order by amount desc</p>
<p>)rank</p>
<p>from</p>
<p>order</p>
<p>where dt=”2019-09-01”</p>
<p>)t</p>
<p>where t.rank&lt;4</p>
<p>订单属于业务数据，在关系型数据库中不会存在数据重复</p>
<p>hive建表时也不会导致数据重复，</p>
<p>我推测是在数据迁移时，迁移失败导致重复迁移数据冗余了</p>
<p>t_order订单表</p>
<p>order_id,//订单id</p>
<p>item_id, //商品id</p>
<p>create_time,//下单时间</p>
<p>amount//下单金额</p>
<p>t_item商品表</p>
<p>item_id,//商品id</p>
<p>item_name,//商品名称</p>
<p>category//品类</p>
<p>t_item商品表</p>
<p>item_id,//商品id</p>
<p>item_name,//名称</p>
<p>category_1,//一级品类</p>
<p>category_2,//二级品类</p>
<ol>
<li>最近一个月，销售数量最多的10个商品</li>
</ol>
<p>select</p>
<p>item_id,</p>
<p>count(order_id)a</p>
<p>from</p>
<p>t_order</p>
<p>where</p>
<p>dataediff(create_time,current_date)&lt;=30</p>
<p>group by</p>
<p>item_id</p>
<p>order by a desc;</p>
<ol start="2">
<li>最近一个月，每个种类里销售数量最多的10个商品</li>
</ol>
<p>#一个订单对应一个商品 一个商品对应一个品类</p>
<p>with(</p>
<p>select</p>
<p>order_id,</p>
<p>item_id,</p>
<p>item_name,</p>
<p>category</p>
<p>from</p>
<p>t_order</p>
<p>join</p>
<p>t_item</p>
<p>on</p>
<p>t_order.item_id = t_item.item_id</p>
<p>) t</p>
<p>select</p>
<p>order_id,</p>
<p>item_id,</p>
<p>item_name,</p>
<p>category,</p>
<p>count(item_id)over(</p>
<p>partition by category</p>
<p>)item_count</p>
<p>from</p>
<p>t</p>
<p>group by category</p>
<p>order by item_count desc</p>
<p>limit 10;</p>
<p>计算平台的每一个用户发过多少日记、获得多少点赞数</p>
<p>with t3 as(</p>
<p>select * from</p>
<p>t1 left join t2</p>
<p>on t1.log_id = t2.log_id</p>
<p>)</p>
<p>select</p>
<p>uid,//用户Id</p>
<p>count(log_id)over(partition by uid)log_cnt,//</p>
<p>count(like_uid)over(partition by log_id)liked_cnt//获得多少点赞数</p>
<p>from</p>
<p>t3</p>
<p>处理产品版本号</p>
<p>1、需求A:找出T1表中最大的版本号</p>
<p>思路：列转行 切割版本号 一列变三列</p>
<p>主版本号 子版本号 阶段版本号</p>
<p>with t2 as(//转换</p>
<p>select</p>
<p>v_id v1,//版本号</p>
<p>v_id v2 //主</p>
<p>from</p>
<p>t1</p>
<p>lateral view explode(v2) tmp as v2</p>
<p>)</p>
<p>select //第一层 找出第一个</p>
<p>v1,</p>
<p>max(v2)</p>
<p>from</p>
<p>t2</p>
<p>——————————————————————————————————————————————————————————————</p>
<p>1、需求A:找出T1表中最大的版本号</p>
<p>select</p>
<p>v_id,//版本号</p>
<p>max(split(v_id,”.”)[0]) v1,//主版本不会为空</p>
<p>max(if(split(v_id,”.”)[1]=””,0,split(v_id,”.”)[1]))v2,//取出子版本并判断是否为空，并给默认值</p>
<p>max(if(split(v_id,”.”)[2]=””,0,split(v_id,”.”)[2]))v3//取出阶段版本并判断是否为空，并给默认值</p>
<p>from</p>
<p>t1</p>
<p>2、需求B：计算出如下格式的所有版本号排序，要求对于相同的版本号，顺序号并列：</p>
<p>select</p>
<p>v_id,</p>
<p>rank() over(partition by v_id order by v_id)seq</p>
<p>from</p>
<p>t1</p>
<h1 id="第9章-JavaSE"><a href="#第9章-JavaSE" class="headerlink" title="第9章 JavaSE"></a>第9章 JavaSE</h1><h2 id="9-1-HhashMap底层源码，数据结构"><a href="#9-1-HhashMap底层源码，数据结构" class="headerlink" title="9.1 HhashMap底层源码，数据结构"></a>9.1 HhashMap底层源码，数据结构</h2><p>hashMap的底层结构在jdk1.7中由数组+链表实现，在jdk1.8中由数组+链表+红黑树实现，以数组+链表的结构为例。</p>
<p><img src="media/662ca9afacbd2ed50758a4bb1274454b.png"></p>
<p><img src="media/ac1e22f8b8d429f6de9df7c580d0a517.png"></p>
<p><strong>JDK1.8之前Put方法：</strong></p>
<p><img src="media/47de90a2da0cf11857af0bcd485f1f6a.png"></p>
<p><strong>JDK1.8之后Put方法：</strong></p>
<p><img src="media/bc7b047ab0fcf2c9878e99db9116518b.png"></p>
<h2 id="9-2-Java自带哪几种线程池？"><a href="#9-2-Java自带哪几种线程池？" class="headerlink" title="9.2 Java自带哪几种线程池？"></a>9.2 Java自带哪几种线程池？</h2><blockquote>
<p>  <strong>1）newCachedThreadPool</strong></p>
</blockquote>
<p>创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。这种类型的线程池特点是：</p>
<p>工作线程的创建数量几乎没有限制（其实也有限制的，数目为Interger. MAX_VALUE）,<br>这样可灵活的往线程池中添加线程。</p>
<p>如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间（默认为1分钟），则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。</p>
<p>在使用CachedThreadPool时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统瘫痪。</p>
<blockquote>
<p>  <strong>2）newFixedThreadPool</strong></p>
</blockquote>
<p>创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。FixedThreadPool是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但是，在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。</p>
<blockquote>
<p>  <strong>3）newSingleThreadExecutor</strong></p>
</blockquote>
<p>创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序（FIFO,<br>LIFO,<br>优先级）执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。</p>
<blockquote>
<p>  <strong>4）newScheduleThreadPool</strong></p>
</blockquote>
<p>创建一个定长的线程池，而且支持定时的以及周期性的任务执行，支持定时及周期性任务执行。延迟3秒执行。</p>
<h2 id="9-3-HashMap和HashTable区别"><a href="#9-3-HashMap和HashTable区别" class="headerlink" title="9.3 HashMap和HashTable区别"></a>9.3 HashMap和HashTable区别</h2><ol>
<li> 线程安全性不同</li>
</ol>
<p>HashMap是线程不安全的，HashTable是线程安全的，其中的方法是Synchronize的，在多线程并发的情况下，可以直接使用HashTabl，但是使用HashMap时必须自己增加同步处理。</p>
<ol>
<li> 是否提供contains方法</li>
</ol>
<p>HashMap只有containsValue和containsKey方法；HashTable有contains、containsKey和containsValue三个方法，其中contains和containsValue方法功能相同。</p>
<ol>
<li> key和value是否允许null值</li>
</ol>
<p>Hashtable中，key和value都不允许出现null值。HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。</p>
<ol>
<li> 数组初始化和扩容机制</li>
</ol>
<p>HashTable在不指定容量的情况下的默认容量为11，而HashMap为16，Hashtable不要求底层数组的容量一定要为2的整数次幂，而HashMap则要求一定为2的整数次幂。</p>
<p>Hashtable扩容时，将容量变为原来的2倍加1，而HashMap扩容时，将容量变为原来的2倍。</p>
<h2 id="9-4-TreeSet和HashSet区别"><a href="#9-4-TreeSet和HashSet区别" class="headerlink" title="9.4 TreeSet和HashSet区别"></a>9.4 TreeSet和HashSet区别</h2><p>HashSet是采用hash表来实现的。其中的元素没有按顺序排列，add()、remove()以及contains()等方法都是复杂度为O(1)的方法。</p>
<p>TreeSet是采用树结构实现（红黑树算法）。元素是按顺序进行排列，但是add()、remove()以及contains()等方法都是复杂度为O(log<br>(n))的方法。它还提供了一些方法来处理排序的set，如first()，last()，headSet()，tailSet()等等。</p>
<h2 id="9-5-String-buffer和String-build区别"><a href="#9-5-String-buffer和String-build区别" class="headerlink" title="9.5 String buffer和String build区别"></a>9.5 String buffer和String build区别</h2><p>1、StringBuffer与StringBuilder中的方法和功能完全是等价的。</p>
<p>2、只是StringBuffer中的方法大都采用了 synchronized<br>关键字进行修饰，因此是线程安全的，而StringBuilder没有这个修饰，可以被认为是线程不安全的。</p>
<p>3、在单线程程序下，StringBuilder效率更快，因为它不需要加锁，不具备多线程安全而StringBuffer则每次都需要判断锁，效率相对更低</p>
<h2 id="9-6-Final、Finally、Finalize"><a href="#9-6-Final、Finally、Finalize" class="headerlink" title="9.6 Final、Finally、Finalize"></a>9.6 Final、Finally、Finalize</h2><p>final：修饰符（关键字）有三种用法：修饰类、变量和方法。修饰类时，意味着它不能再派生出新的子类，即不能被继承，因此它和abstract是反义词。修饰变量时，该变量使用中不被改变，必须在声明时给定初值，在引用中只能读取不可修改，即为常量。修饰方法时，也同样只能使用，不能在子类中被重写。</p>
<p>finally：通常放在try…catch的后面构造最终执行代码块，这就意味着程序无论正常执行还是发生异常，这里的代码只要JVM不关闭都能执行，可以将释放外部资源的代码写在finally块中。</p>
<p>finalize：Object类中定义的方法，Java中允许使用finalize()<br>方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在销毁对象时调用的，通过重写finalize()<br>方法可以整理系统资源或者执行其他清理工作。</p>
<h2 id="9-7-和Equals区别"><a href="#9-7-和Equals区别" class="headerlink" title="9.7 ==和Equals区别"></a>9.7 ==和Equals区别</h2><p>== : 如果比较的是基本数据类型，那么比较的是变量的值</p>
<p>如果比较的是引用数据类型，那么比较的是地址值（两个对象是否指向同一块内存）</p>
<p>equals:如果没重写equals方法比较的是两个对象的地址值。</p>
<p>如果重写了equals方法后我们往往比较的是对象中的属性的内容</p>
<p>equals方法是从Object类中继承的，默认的实现就是使用==</p>
<p><img src="media/8be4ff45cc94624c97cbe549a91503ad.png"></p>
<h1 id="第10章-Redis"><a href="#第10章-Redis" class="headerlink" title="第10章 Redis"></a>第10章 Redis</h1><h2 id="10-1-缓存穿透、缓存雪崩、缓存击穿"><a href="#10-1-缓存穿透、缓存雪崩、缓存击穿" class="headerlink" title="10.1 缓存穿透、缓存雪崩、缓存击穿"></a>10.1 缓存穿透、缓存雪崩、缓存击穿</h2><p>1）缓存穿透是指查询一个一定不存在的数据。由于缓存命不中时会去查询数据库，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。</p>
<p>解决方案：</p>
<ol>
<li> 是将空对象也缓存起来，并给它设置一个很短的过期时间，最长不超过5分钟</li>
</ol>
<p>②<br>采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力</p>
<p>2）如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，就会造成缓存雪崩。</p>
<p>解决方案：</p>
<p>尽量让失效的时间点不分布在同一个时间点</p>
<p>3）缓存击穿，是指一个key非常热点，在不停的扛着大并发，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。</p>
<p>解决方案：</p>
<p>可以设置key永不过期</p>
<h2 id="10-2-哨兵模式"><a href="#10-2-哨兵模式" class="headerlink" title="10.2 哨兵模式"></a>10.2 哨兵模式</h2><p>主从复制中反客为主的自动版，如果主机Down掉，哨兵会从从机中选择一台作为主机，并将它设置为其他从机的主机，而且如果原来的主机再次启动的话也会成为从机。</p>
<h2 id="10-3-数据类型"><a href="#10-3-数据类型" class="headerlink" title="10.3 数据类型"></a>10.3 数据类型</h2><table>
<thead>
<tr>
<th>string</th>
<th>字符串</th>
</tr>
</thead>
<tbody><tr>
<td>list</td>
<td>可以重复的集合</td>
</tr>
<tr>
<td>set</td>
<td>不可以重复的集合</td>
</tr>
<tr>
<td>hash</td>
<td>类似于Map&lt;String,String&gt;</td>
</tr>
<tr>
<td>zset(sorted set）</td>
<td>带分数的set</td>
</tr>
</tbody></table>
<h2 id="10-4-持久化"><a href="#10-4-持久化" class="headerlink" title="10.4 持久化"></a>10.4 持久化</h2><p><strong>1）RDB持久化：</strong></p>
<ol>
<li><p> <strong>在指定的时间间隔内持久化</strong></p>
</li>
<li><p> <strong>服务shutdown会自动持久化</strong></p>
</li>
</ol>
<p><strong>③ 输入bgsave也会持久化</strong></p>
<p><strong>2）AOF : 以日志形式记录每个更新操作</strong></p>
<p>Redis重新启动时读取这个文件，重新执行新建、修改数据的命令恢复数据。</p>
<p><strong>保存策略：</strong></p>
<p>推荐（并且也是默认）的措施为每秒持久化一次，这种策略可以兼顾速度和安全性。</p>
<p><strong>缺点：</strong></p>
<p>1 比起RDB占用更多的磁盘空间</p>
<p>2 恢复备份速度要慢</p>
<p>3 每次读写都同步的话，有一定的性能压力</p>
<p>4 存在个别Bug，造成恢复不能</p>
<p><strong>选择策略：</strong></p>
<p>官方推荐：</p>
<p>如果对数据不敏感，可以选单独用RDB；不建议单独用AOF，因为可能出现Bug;如果只是做纯内存缓存，可以都不用</p>
<h2 id="11-5-悲观锁"><a href="#11-5-悲观锁" class="headerlink" title="11.5 悲观锁"></a>11.5 悲观锁</h2><p>执行操作前假设当前的操作肯定（或有很大几率）会被打断（悲观）。基于这个假设，我们在做操作前就会把相关资源锁定，不允许自己执行期间有其他操作干扰。</p>
<h2 id="11-6-乐观锁"><a href="#11-6-乐观锁" class="headerlink" title="11.6 乐观锁"></a>11.6 乐观锁</h2><p>执行操作前假设当前操作不会被打断（乐观）。基于这个假设，我们在做操作前不会锁定资源，万一发生了其他操作的干扰，那么本次操作将被放弃。Redis使用的就是乐观锁。</p>
<h1 id="第11章-MySql"><a href="#第11章-MySql" class="headerlink" title="第11章 MySql"></a>第11章 MySql</h1><h2 id="11-1-MyISAM与InnoDB的区别"><a href="#11-1-MyISAM与InnoDB的区别" class="headerlink" title="11.1 MyISAM与InnoDB的区别"></a>11.1 MyISAM与InnoDB的区别</h2><table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>MyISAM</strong></th>
<th><strong>InnoDB</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>外键</strong></td>
<td><strong>不支持</strong></td>
<td><strong>支持</strong></td>
</tr>
<tr>
<td><strong>事务</strong></td>
<td><strong>不支持</strong></td>
<td><strong>支持</strong></td>
</tr>
<tr>
<td><strong>行表锁</strong></td>
<td><strong>表锁，即使操作一条记录也会锁住整个表，不适合高并发的操作</strong></td>
<td><strong>行锁,操作时只锁某一行，不对其它行有影响， 适合高并发的操作</strong></td>
</tr>
<tr>
<td><strong>缓存</strong></td>
<td><strong>只缓存索引，不缓存真实数据</strong></td>
<td><strong>不仅缓存索引还要缓存真实数据，对内存要求较高，而且内存大小对性能有决定性的影响</strong></td>
</tr>
</tbody></table>
<h2 id="11-2-索引优化"><a href="#11-2-索引优化" class="headerlink" title="11.2 索引优化"></a>11.2 索引优化</h2><p><strong>数据结构：B+Tree</strong></p>
<p>一般来说能够达到range就可以算是优化了 idx name_deptId</p>
<p><strong>口诀（两个法则加6种索引失效的情况）</strong></p>
<p>全值匹配我最爱，最左前缀要遵守；</p>
<p>带头大哥不能死，中间兄弟不能断；</p>
<p>索引列上少计算，范围之后全失效；</p>
<p>LIKE百分写最右，覆盖索引不写*；</p>
<p>不等空值还有OR，索引影响要注意；</p>
<p>VAR引号不可丢，SQL优化有诀窍。</p>
<h2 id="11-3-b-tree和b-tree的区别"><a href="#11-3-b-tree和b-tree的区别" class="headerlink" title="11.3 b-tree和b+tree的区别"></a>11.3 b-tree和b+tree的区别</h2><ol>
<li>B-树的关键字、索引和记录是放在一起的，<br>B+树的非叶子节点中只有关键字和指向下一个节点的索引，记录只放在叶子节点中。</li>
</ol>
<p>2)<br>在B-树中，越靠近根节点的记录查找时间越快，只要找到关键字即可确定记录的存在；而B+树中每个记录的查找时间基本是一样的，都需要从根节点走到叶子节点，而且在叶子节点中还要再比较关键字。</p>
<h2 id="11-4-redis是单线程的，为什么那么快"><a href="#11-4-redis是单线程的，为什么那么快" class="headerlink" title="11.4 redis是单线程的，为什么那么快"></a>11.4 redis是单线程的，为什么那么快</h2><p>1)完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。</p>
<p>2)数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的</p>
<p>3)采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗<br>CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗</p>
<p>4)使用多路I/O复用模型，非阻塞IO</p>
<p>5)使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM<br>机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求</p>
<h2 id="11-5-MySQL的事务"><a href="#11-5-MySQL的事务" class="headerlink" title="11.5 MySQL的事务"></a>11.5 MySQL的事务</h2><p><strong>一、事务的基本要素（ACID）</strong></p>
<p>1、原子性（Atomicity）：事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位</p>
<p>2、一致性（Consistency）：事务开始前和结束后，数据库的完整性约束没有被破坏<br>。比如A向B转账，不可能A扣了钱，B却没收到。</p>
<p>3、隔离性（Isolation）：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。</p>
<p>4、持久性（Durability）：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。</p>
<p><strong>二、事务的并发问题</strong></p>
<p>1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据</p>
<p>2、不可重复读：事务 A 多次读取同一数据，事务 B<br>在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果<br>不一致</p>
<p>3、幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。</p>
<p>小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表</p>
<p><strong>三、MySQL事务隔离级别</strong></p>
<p>事务隔离级别 脏读 不可重复读 幻读</p>
<p>读未提交（read-uncommitted） 是 是 是</p>
<p>不可重复读（read-committed） 否 是 是</p>
<p>可重复读（repeatable-read） 否 否 是</p>
<p>串行化（serializable） 否 否 否</p>
<h1 id="第12章-JVM"><a href="#第12章-JVM" class="headerlink" title="第12章 JVM"></a>第12章 JVM</h1><h2 id="12-1-JVM内存分哪几个区，每个区的作用是什么"><a href="#12-1-JVM内存分哪几个区，每个区的作用是什么" class="headerlink" title="12.1 JVM内存分哪几个区，每个区的作用是什么?"></a>12.1 JVM内存分哪几个区，每个区的作用是什么?</h2><p><img src="media/289d3ec75a627586ff2ade85f08fe67e.png"></p>
<p>java虚拟机主要分为以下几个区:</p>
<ol>
<li><p> <strong>方法区</strong>：</p>
</li>
<li><p> 有时候也成为永久代，在该区内很少发生垃圾回收，但是并不代表不发生GC，在这里进行的GC主要是对方法区里的常量池和对类型的卸载</p>
</li>
<li><p> 方法区主要用来存储已被虚拟机加载的类的信息、常量、静态变量和即时编译器编译后的代码等数据。</p>
</li>
<li><p> 该区域是被线程共享的。</p>
</li>
<li><p> 方法区里有一个运行时常量池，用于存放静态编译产生的字面量和符号引用。该常量池具有动态性，也就是说常量并不一定是编译时确定，运行时生成的常量也会存在这个常量池中。</p>
</li>
<li><p> <strong>虚拟机栈</strong>:</p>
</li>
<li><p> 虚拟机栈也就是我们平常所称的栈内存,它为java方法服务，每个方法在执行的时候都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接和方法出口等信息。</p>
</li>
<li><p> 虚拟机栈是线程私有的，它的生命周期与线程相同。</p>
</li>
<li><p> 局部变量表里存储的是基本数据类型、returnAddress类型（指向一条字节码指令的地址）和对象引用，这个对象引用有可能是指向对象起始地址的一个指针，也有可能是代表对象的句柄或者与对象相关联的位置。局部变量所需的内存空间在编译器间确定</p>
</li>
<li><p>操作数栈的作用主要用来存储运算结果以及运算的操作数，它不同于局部变量表通过索引来访问，而是压栈和出栈的方式</p>
</li>
<li><p>每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接.动态链接就是将常量池中的符号引用在运行期转化为直接引用。</p>
</li>
<li><p><strong>本地方法栈</strong>：<br>本地方法栈和虚拟机栈类似，只不过本地方法栈为Native方法服务。</p>
</li>
<li><p><strong>堆</strong>：</p>
</li>
</ol>
<p>java堆是所有线程所共享的一块内存，在虚拟机启动时创建，几乎所有的对象实例都在这里创建，因此该区域经常发生垃圾回收操作。</p>
<ol>
<li> <strong>程序计数器：</strong></li>
</ol>
<p>内存空间小，字节码解释器工作时通过改变这个计数值可以选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理和线程恢复等功能都需要依赖这个计数器完成。该内存区域是唯一一个java虚拟机规范没有规定任何OOM情况的区域。</p>
<h2 id="12-2-Java类加载过程"><a href="#12-2-Java类加载过程" class="headerlink" title="12.2 Java类加载过程?"></a>12.2 Java类加载过程?</h2><p>Java类加载需要经历一下几个过程：</p>
<ol>
<li> 加载</li>
</ol>
<p>加载时类加载的第一个过程，在这个阶段，将完成一下三件事情：</p>
<ol>
<li><p> 通过一个类的全限定名获取该类的二进制流。</p>
</li>
<li><p> 将该二进制流中的静态存储结构转化为方法去运行时数据结构。</p>
</li>
<li><p> 在内存中生成该类的Class对象，作为该类的数据访问入口。</p>
</li>
<li><p> 验证</p>
</li>
</ol>
<p>验证的目的是为了确保Class文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:</p>
<ol>
<li><p> 文件格式验证：验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.</p>
</li>
<li><p> 元数据验证:对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。</p>
</li>
<li><p> 字节码验证：是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。</p>
</li>
<li><p> 符号引用验证：这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。</p>
</li>
<li><p> 准备</p>
</li>
</ol>
<p>准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。</p>
<ol>
<li> 解析</li>
</ol>
<p>该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。</p>
<ol>
<li> 初始化</li>
</ol>
<p>初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码。</p>
<h2 id="12-3-java中垃圾收集的方法有哪些"><a href="#12-3-java中垃圾收集的方法有哪些" class="headerlink" title="12.3 java中垃圾收集的方法有哪些?"></a>12.3 java中垃圾收集的方法有哪些?</h2><p><strong>1）引用计数法</strong> 应用于：微软的COM/ActionScrip3/Python等</p>
<p>a) 如果对象没有被引用，就会被回收，缺点：需要维护一个引用计算器</p>
<p><strong>2）复制算法</strong> 年轻代中使用的是Minor GC，这种GC算法采用的是复制算法(Copying)</p>
<p>a) 效率高，缺点：需要内存容量大，比较耗内存</p>
<p>b) 使用在占空间比较小、刷新次数多的新生区</p>
<p><strong>3）标记清除</strong> 老年代一般是由标记清除或者是标记清除与标记整理的混合实现</p>
<p>a) 效率比较低，会差生碎片。</p>
<p><strong>4）标记压缩</strong> 老年代一般是由标记清除或者是标记清除与标记整理的混合实现</p>
<p>a) 效率低速度慢，需要移动对象，但不会产生碎片。</p>
<p><strong>5）标记清除压缩</strong>标记清除-标记压缩的集合，多次GC后才Compact</p>
<p>a) 使用于占空间大刷新次数少的养老区，是3 4的集合体</p>
<h2 id="12-4-如何判断一个对象是否存活-或者GC对象的判定方法"><a href="#12-4-如何判断一个对象是否存活-或者GC对象的判定方法" class="headerlink" title="12.4 如何判断一个对象是否存活?(或者GC对象的判定方法)"></a>12.4 如何判断一个对象是否存活?(或者GC对象的判定方法)</h2><p>判断一个对象是否存活有两种方法:</p>
<ol>
<li><p> 引用计数法</p>
</li>
<li><p> 可达性算法(引用链法)</p>
</li>
</ol>
<h2 id="12-5-什么是类加载器，类加载器有哪些"><a href="#12-5-什么是类加载器，类加载器有哪些" class="headerlink" title="12.5 什么是类加载器，类加载器有哪些?"></a>12.5 什么是类加载器，类加载器有哪些?</h2><p>实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。</p>
<p>主要有一下四种类加载器:</p>
<ol>
<li><p>启动类加载器(Bootstrap<br> ClassLoader)用来加载java核心类库，无法被java程序直接引用。</p>
</li>
<li><p>扩展类加载器(extensions class loader):它用来加载 Java 的扩展库。Java<br> 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。</p>
</li>
<li><p>系统类加载器（system class loader）也叫应用类加载器：它根据 Java<br> 应用的类路径（CLASSPATH）来加载 Java 类。一般来说，Java<br> 应用的类都是由它来完成加载的。可以通过<br> ClassLoader.getSystemClassLoader()来获取它。</p>
</li>
<li><p> 用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。</p>
</li>
</ol>
<h2 id="12-6-简述Java内存分配与回收策略以及Minor-GC和Major-GC（full-GC）"><a href="#12-6-简述Java内存分配与回收策略以及Minor-GC和Major-GC（full-GC）" class="headerlink" title="12.6 简述Java内存分配与回收策略以及Minor GC和Major GC（full GC）"></a>12.6 简述Java内存分配与回收策略以及Minor GC和Major GC（full GC）</h2><p><strong>内存分配：</strong></p>
<ol>
<li><p> <strong>栈区</strong>：栈分为java虚拟机栈和本地方法栈</p>
</li>
<li><p><strong>堆区</strong>：堆被所有线程共享区域，在虚拟机启动时创建，唯一目的存放对象实例。堆区是gc的主要区域，通常情况下分为两个区块年轻代和年老代。更细一点年轻代又分为Eden区，主要放新创建对象，From<br> survivor 和 To survivor 保存gc后幸存下的对象，默认情况下各自占比 8:1:1。</p>
</li>
<li><p><strong>方法区</strong>：被所有线程共享区域，用于存放已被虚拟机加载的类信息，常量，静态变量等数据。被Java虚拟机描述为堆的一个逻辑部分。习惯是也叫它永久代（permanment<br> generation）</p>
</li>
<li><p> <strong>程序计数器</strong>：当前线程所执行的行号指示器。通过改变计数器的值来确定下一条指令，比如循环，分支，跳转，异常处理，线程恢复等都是依赖计数器来完成。线程私有的。</p>
</li>
</ol>
<p><strong>回收策略以及Minor GC和Major GC：</strong></p>
<ol>
<li><p> 对象优先在堆的Eden区分配。</p>
</li>
<li><p> 大对象直接进入老年代。</p>
</li>
<li><p> 长期存活的对象将直接进入老年代。</p>
</li>
</ol>
<p>当Eden区没有足够的空间进行分配时，虚拟机会执行一次Minor GC.Minor<br>GC通常发生在新生代的Eden区，在这个区的对象生存期短，往往发生GC的频率较高，回收速度比较快;Full<br>Gc/Major GC 发生在老年代，一般情况下，触发老年代GC的时候不会触发Minor<br>GC,但是通过配置，可以在Full GC之前进行一次Minor GC这样可以加快老年代的回收速度。</p>
<h1 id="第13章-JUC"><a href="#第13章-JUC" class="headerlink" title="第13章 JUC"></a>第13章 JUC</h1><p><strong>13.1 Synchronized与Lock的区别</strong></p>
<p>1）Synchronized能实现的功能Lock都可以实现，而且Lock比Synchronized更好用，更灵活。</p>
<p>2）Synchronized可以自动上锁和解锁；Lock需要手动上锁和解锁</p>
<p><strong>13.2 Runnable和Callable的区别</strong></p>
<p>1）Runnable接口中的方法没有返回值；Callable接口中的方法有返回值</p>
<p>2）Runnable接口中的方法没有抛出异常；Callable接口中的方法抛出了异常</p>
<p>3）Runnable接口中的落地方法是call方法；Callable接口中的落地方法是run方法</p>
<p><strong>13.3 什么是分布式锁</strong></p>
<p>当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。分布式锁可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存，如<br>Redis，通过set (key,value,nx,px,timeout)方法添加分布式锁。</p>
<p><strong>13.4 什么是分布式事务</strong></p>
<p>分布式事务指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。</p>
<h1 id="第14章-面试说明"><a href="#第14章-面试说明" class="headerlink" title="第14章 面试说明"></a>第14章 面试说明</h1><h2 id="14-1-面试过程最关键的是什么？"><a href="#14-1-面试过程最关键的是什么？" class="headerlink" title="14.1 面试过程最关键的是什么？"></a>14.1 面试过程最关键的是什么？</h2><p>1）大大方方的聊，放松</p>
<p>2）体现优势，避免劣势</p>
<h2 id="14-2-面试时该怎么说？"><a href="#14-2-面试时该怎么说？" class="headerlink" title="14.2 面试时该怎么说？"></a>14.2 面试时该怎么说？</h2><p>1）语言表达清楚</p>
<p>（1）思维逻辑清晰，表达流畅</p>
<p>（2）一二三层次表达</p>
<p>2）所述内容不犯错</p>
<p>（1）不说前东家或者自己的坏话</p>
<p>（2）往自己擅长的方面说</p>
<p>（3）实质，对考官来说，内容听过，就是自我肯定；没听过，那就是个学习的过程。</p>
<h2 id="14-3-面试技巧"><a href="#14-3-面试技巧" class="headerlink" title="14.3 面试技巧"></a>14.3 面试技巧</h2><h3 id="14-3-1-六个常见问题"><a href="#14-3-1-六个常见问题" class="headerlink" title="14.3.1 六个常见问题"></a>14.3.1 六个常见问题</h3><p>1）你的优点是什么？</p>
<p>大胆的说出自己各个方面的优势和特长</p>
<p>2）你的缺点是什么？</p>
<p>不要谈自己真实问题；用“缺点”衬托自己的优点</p>
<p>3）你的离职原因是什么？</p>
<ul>
<li><p>不说前东家坏话，哪怕被伤过</p>
<ul>
<li><p>  合情合理合法</p>
</li>
<li><p>  不要说超过1个以上的原因</p>
</li>
</ul>
</li>
</ul>
<p>4）您对薪资的期望是多少？</p>
<ul>
<li><p>非终面不深谈薪资</p>
<ul>
<li><p>  只说区间，不说具体数字</p>
</li>
<li><p>  底线是不低于当前薪资</p>
</li>
<li><p>  非要具体数字，区间取中间值，或者当前薪资的+20%</p>
</li>
</ul>
</li>
</ul>
<p>5）您还有什么想问的问题？</p>
<ul>
<li><p>这是体现个人眼界和层次的问题</p>
<ul>
<li><p>  问题本身不在于面试官想得到什么样的答案，而在于你跟别的应聘者的对比</p>
</li>
<li><p>标准答案：</p>
<p>  公司希望我入职后的3-6个月内，给公司解决什么样的问题</p>
<p>  公司（或者对这个部门）未来的战略规划是什么样子的？</p>
<p>  以你现在对我的了解，您觉得我需要多长时间融入公司？</p>
</li>
</ul>
</li>
</ul>
<p>6）您最快多长时间能入职？</p>
<p>一周左右，如果公司需要，可以适当提前。</p>
<h3 id="14-3-2-两个注意事项"><a href="#14-3-2-两个注意事项" class="headerlink" title="14.3.2 两个注意事项"></a>14.3.2 两个注意事项</h3><p>1）职业化的语言</p>
<p>2）职业化的形象</p>
<h3 id="14-3-3-自我介绍（控制在4分半以内，不超过5分钟）"><a href="#14-3-3-自我介绍（控制在4分半以内，不超过5分钟）" class="headerlink" title="14.3.3 自我介绍（控制在4分半以内，不超过5分钟）"></a>14.3.3 自我介绍（控制在4分半以内，不超过5分钟）</h3><p>1）个人基本信息</p>
<p>2）工作履历</p>
<p>时间、公司名称、任职岗位、主要工作内容、工作业绩、离职原因</p>
<p>3）深度沟通（也叫压力面试）</p>
<p>刨根问底下沉式追问（注意是下沉式，而不是发散式的）</p>
<p>基本技巧：往自己熟悉的方向说</p>
<h1 id="第15章-LeetCode题目精选"><a href="#第15章-LeetCode题目精选" class="headerlink" title="第15章 LeetCode题目精选"></a>第15章 LeetCode题目精选</h1><p><a target="_blank" rel="noopener" href="https://labuladong.gitbook.io/algo/di-ling-zhang-bi-du-xi-lie/xue-xi-shu-ju-jie-gou-he-suan-fa-de-gao-xiao-fang-fa">https://labuladong.gitbook.io/algo/di-ling-zhang-bi-du-xi-lie/xue-xi-shu-ju-jie-gou-he-suan-fa-de-gao-xiao-fang-fa</a></p>
<h2 id="15-1-两数之和"><a href="#15-1-两数之和" class="headerlink" title="15.1 两数之和"></a>15.1 两数之和</h2><p>问题链接：<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/two-sum/">https://leetcode-cn.com/problems/two-sum/</a></p>
<h3 id="15-1-1-问题描述"><a href="#15-1-1-问题描述" class="headerlink" title="15.1.1 问题描述"></a>15.1.1 问题描述</h3><p>给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个<br>整数，并返回他们的数组下标。</p>
<p>你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。</p>
<blockquote>
<p>  ```</p>
</blockquote>
<blockquote>
<p>  给定 nums = [2, 7, 11, 15], target = 9</p>
</blockquote>
<blockquote>
<p>  因为 nums[0] + nums[1] = 2 + 7 = 9</p>
</blockquote>
<blockquote>
<p>  所以返回 [0, 1]</p>
</blockquote>
<blockquote>
<p>  ```</p>
</blockquote>
<h3 id="15-1-2-参考答案"><a href="#15-1-2-参考答案" class="headerlink" title="15.1.2 参考答案"></a>15.1.2 参考答案</h3><blockquote>
<p>  ```java</p>
</blockquote>
<blockquote>
<p>  class Solution {</p>
</blockquote>
<blockquote>
<p>  public int[] twoSum(int[] nums, int target) {</p>
</blockquote>
<blockquote>
<p>  Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;();</p>
</blockquote>
<blockquote>
<p>  for (int i = 0; i &lt; nums.length; i++) {</p>
</blockquote>
<blockquote>
<p>  int complement = target - nums[i];</p>
</blockquote>
<blockquote>
<p>  if (map.containsKey(complement)) {</p>
</blockquote>
<blockquote>
<p>  return new int[] { map.get(complement), i };</p>
</blockquote>
<blockquote>
<p>  }</p>
</blockquote>
<blockquote>
<p>  map.put(nums[i], i);</p>
</blockquote>
<blockquote>
<p>  }</p>
</blockquote>
<blockquote>
<p>  throw new IllegalArgumentException(“No two sum solution”);</p>
</blockquote>
<blockquote>
<p>  }</p>
</blockquote>
<blockquote>
<p>  }</p>
</blockquote>
<blockquote>
<p>  ```</p>
</blockquote>
<h2 id="15-2-爬楼梯"><a href="#15-2-爬楼梯" class="headerlink" title="15.2 爬楼梯"></a>15.2 爬楼梯</h2><p>问题链接：<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/climbing-stairs/">https://leetcode-cn.com/problems/climbing-stairs/</a></p>
<h3 id="15-2-1-问题描述"><a href="#15-2-1-问题描述" class="headerlink" title="15.2.1 问题描述"></a>15.2.1 问题描述</h3><p>假设你正在爬楼梯。需要 n 阶你才能到达楼顶。</p>
<p>每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？</p>
<p>注意：给定 n 是一个正整数。</p>
<p>示例 1：</p>
<p>```</p>
<p>输入： 2</p>
<p>输出： 2</p>
<p>解释： 有两种方法可以爬到楼顶。</p>
<ol>
<li><p>1 阶 + 1 阶</p>
</li>
<li><p>2 阶</p>
</li>
</ol>
<p>```</p>
<p>示例 2：</p>
<p>```</p>
<p>输入： 3</p>
<p>输出： 3</p>
<p>解释： 有三种方法可以爬到楼顶。</p>
<ol>
<li><p>1 阶 + 1 阶 + 1 阶</p>
</li>
<li><p>1 阶 + 2 阶</p>
</li>
<li><p>2 阶 + 1 阶</p>
</li>
</ol>
<p>```</p>
<h3 id="15-2-2-参考答案"><a href="#15-2-2-参考答案" class="headerlink" title="15.2.2 参考答案"></a>15.2.2 参考答案</h3><p>```java</p>
<p>public class Solution {</p>
<p>public int climbStairs(int n) {</p>
<p>if (n == 1) {</p>
<p>return 1;</p>
<p>}</p>
<p>int[] dp = new int[n + 1];</p>
<p>dp[1] = 1;</p>
<p>dp[2] = 2;</p>
<p>for (int i = 3; i &lt;= n; i++) {</p>
<p>dp[i] = dp[i - 1] + dp[i - 2];</p>
<p>}</p>
<p>return dp[n];</p>
<p>}</p>
<p>}</p>
<p>```</p>
<h2 id="15-3-翻转二叉树"><a href="#15-3-翻转二叉树" class="headerlink" title="15.3 翻转二叉树"></a>15.3 翻转二叉树</h2><p>链接：<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/invert-binary-tree/">https://leetcode-cn.com/problems/invert-binary-tree/</a></p>
<h3 id="15-3-1-问题描述"><a href="#15-3-1-问题描述" class="headerlink" title="15.3.1 问题描述"></a>15.3.1 问题描述</h3><p>翻转一棵二叉树。</p>
<p>示例：</p>
<p>输入：</p>
<p>```</p>
<p>4</p>
<p>/ \</p>
<p>2 7</p>
<p>/ \ / \</p>
<p>1 3 6 9</p>
<p>```</p>
<p>输出：</p>
<p>```</p>
<p>4</p>
<p>/ \</p>
<p>7 2</p>
<p>/ \ / \</p>
<p>9 6 3 1</p>
<p>```</p>
<h3 id="15-3-2-参考答案"><a href="#15-3-2-参考答案" class="headerlink" title="15.3.2 参考答案"></a>15.3.2 参考答案</h3><p>```java</p>
<p>public TreeNode invertTree(TreeNode root) {</p>
<p>if (root == null) {</p>
<p>return null;</p>
<p>}</p>
<p>TreeNode right = invertTree(root.right);</p>
<p>TreeNode left = invertTree(root.left);</p>
<p>root.left = right;</p>
<p>root.right = left;</p>
<p>return root;</p>
<p>}</p>
<p>```</p>
<h2 id="15-4-反转链表"><a href="#15-4-反转链表" class="headerlink" title="15.4 反转链表"></a>15.4 反转链表</h2><p>链接：<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/reverse-linked-list/">https://leetcode-cn.com/problems/reverse-linked-list/</a></p>
<h3 id="15-4-1-问题描述"><a href="#15-4-1-问题描述" class="headerlink" title="15.4.1 问题描述"></a>15.4.1 问题描述</h3><p>反转一个单链表。</p>
<p>示例:</p>
<p>```</p>
<p>输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL</p>
<p>输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL</p>
<p>```</p>
<h3 id="15-4-2-参考答案"><a href="#15-4-2-参考答案" class="headerlink" title="15.4.2 参考答案"></a>15.4.2 参考答案</h3><p>```java</p>
<p>public ListNode reverseList(ListNode head) {</p>
<p>ListNode prev = null;</p>
<p>ListNode curr = head;</p>
<p>while (curr != null) {</p>
<p>ListNode nextTemp = curr.next;</p>
<p>curr.next = prev;</p>
<p>prev = curr;</p>
<p>curr = nextTemp;</p>
<p>}</p>
<p>return prev;</p>
<p>}</p>
<p>```</p>
<h2 id="15-5-LRU缓存机制"><a href="#15-5-LRU缓存机制" class="headerlink" title="15.5 LRU缓存机制"></a>15.5 LRU缓存机制</h2><p>链接：<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/lru-cache/">https://leetcode-cn.com/problems/lru-cache/</a></p>
<h3 id="15-5-1-问题描述"><a href="#15-5-1-问题描述" class="headerlink" title="15.5.1 问题描述"></a>15.5.1 问题描述</h3><p>运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用)<br>缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。</p>
<p>获取数据 get(key) - 如果密钥 (key)<br>存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。</p>
<p>写入数据 put(key, value) -<br>如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。</p>
<p>进阶:</p>
<p>你是否可以在 O(1) 时间复杂度内完成这两种操作？</p>
<p>示例:</p>
<p>```</p>
<p>LRUCache cache = new LRUCache( 2 /* 缓存容量 */ );</p>
<p>cache.put(1, 1);</p>
<p>cache.put(2, 2);</p>
<p>cache.get(1); // 返回 1</p>
<p>cache.put(3, 3); // 该操作会使得密钥 2 作废</p>
<p>cache.get(2); // 返回 -1 (未找到)</p>
<p>cache.put(4, 4); // 该操作会使得密钥 1 作废</p>
<p>cache.get(1); // 返回 -1 (未找到)</p>
<p>cache.get(3); // 返回 3</p>
<p>cache.get(4); // 返回 4</p>
<p>```</p>
<h3 id="15-5-2-参考答案"><a href="#15-5-2-参考答案" class="headerlink" title="15.5.2 参考答案"></a>15.5.2 参考答案</h3><p>```java</p>
<p>class LRUCache extends LinkedHashMap&lt;Integer, Integer&gt;{</p>
<p>private int capacity;</p>
<p>public LRUCache(int capacity) {</p>
<p>super(capacity, 0.75F, true);</p>
<p>this.capacity = capacity;</p>
<p>}</p>
<p>public int get(int key) {</p>
<p>return super.getOrDefault(key, -1);</p>
<p>}</p>
<p>public void put(int key, int value) {</p>
<p>super.put(key, value);</p>
<p>}</p>
<p>@Override</p>
<p>protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) {</p>
<p>return size() &gt; capacity;</p>
<p>}</p>
<p>}</p>
<p>/**</p>
<p>* LRUCache 对象会以如下语句构造和调用:</p>
<p>* LRUCache obj = new LRUCache(capacity);</p>
<p>* int param_1 = obj.get(key);</p>
<p>* obj.put(key,value);</p>
<p>*/</p>
<p>```</p>
<h2 id="15-6-最长回文子串"><a href="#15-6-最长回文子串" class="headerlink" title="15.6 最长回文子串"></a>15.6 最长回文子串</h2><p>链接：<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/longest-palindromic-substring/">https://leetcode-cn.com/problems/longest-palindromic-substring/</a></p>
<h3 id="15-6-1-问题描述"><a href="#15-6-1-问题描述" class="headerlink" title="15.6.1 问题描述"></a>15.6.1 问题描述</h3><p>给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。</p>
<p>示例 1：</p>
<p>```</p>
<p>输入: “babad”</p>
<p>输出: “bab”</p>
<p>注意: “aba” 也是一个有效答案。</p>
<p>```</p>
<p>示例 2：</p>
<p>```</p>
<p>输入: “cbbd”</p>
<p>输出: “bb”</p>
<p>```</p>
<h3 id="15-6-2-参考答案"><a href="#15-6-2-参考答案" class="headerlink" title="15.6.2 参考答案"></a>15.6.2 参考答案</h3><p>```java</p>
<p>public String longestPalindrome(String s) {</p>
<p>if (s == null || s.length() &lt; 1) return “”;</p>
<p>int start = 0, end = 0;</p>
<p>for (int i = 0; i &lt; s.length(); i++) {</p>
<p>int len1 = expandAroundCenter(s, i, i);</p>
<p>int len2 = expandAroundCenter(s, i, i + 1);</p>
<p>int len = Math.max(len1, len2);</p>
<p>if (len &gt; end - start) {</p>
<p>start = i - (len - 1) / 2;</p>
<p>end = i + len / 2;</p>
<p>}</p>
<p>}</p>
<p>return s.substring(start, end + 1);</p>
<p>}</p>
<p>private int expandAroundCenter(String s, int left, int right) {</p>
<p>int L = left, R = right;</p>
<p>while (L &gt;= 0 &amp;&amp; R &lt; s.length() &amp;&amp; s.charAt(L) == s.charAt(R)) {</p>
<p>L–;</p>
<p>R++;</p>
<p>}</p>
<p>return R - L - 1;</p>
<p>}</p>
<p>```</p>
<h2 id="15-7-有效的括号"><a href="#15-7-有效的括号" class="headerlink" title="15.7 有效的括号"></a>15.7 有效的括号</h2><p>链接：<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/valid-parentheses/">https://leetcode-cn.com/problems/valid-parentheses/</a></p>
<h3 id="15-7-1-问题描述"><a href="#15-7-1-问题描述" class="headerlink" title="15.7.1 问题描述"></a>15.7.1 问题描述</h3><p>给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串，判断字符串是否有效。</p>
<p>有效字符串需满足：</p>
<ol>
<li><p>左括号必须用相同类型的右括号闭合。</p>
</li>
<li><p>左括号必须以正确的顺序闭合。</p>
</li>
</ol>
<p>注意空字符串可被认为是有效字符串。</p>
<p>示例 1:</p>
<p>```</p>
<p>输入: “()”</p>
<p>输出: true</p>
<p>```</p>
<p>示例 2:</p>
<p>```</p>
<p>输入: “()[]{}”</p>
<p>输出: true</p>
<p>```</p>
<p>示例 3:</p>
<p>```</p>
<p>输入: “(]”</p>
<p>输出: false</p>
<p>```</p>
<p>示例 4:</p>
<p>```</p>
<p>输入: “([)]”</p>
<p>输出: false</p>
<p>```</p>
<p>示例 5:</p>
<p>```</p>
<p>输入: “{[]}”</p>
<p>输出: true</p>
<p>```</p>
<h3 id="15-7-2-参考答案"><a href="#15-7-2-参考答案" class="headerlink" title="15.7.2 参考答案"></a>15.7.2 参考答案</h3><p>```java</p>
<p>class Solution {</p>
<p>// Hash table that takes care of the mappings.</p>
<p>private HashMap&lt;Character, Character&gt; mappings;</p>
<p>// Initialize hash map with mappings. This simply makes the code easier to read.</p>
<p>public Solution() {</p>
<p>this.mappings = new HashMap&lt;Character, Character&gt;();</p>
<p>this.mappings.put(‘)’, ‘(‘);</p>
<p>this.mappings.put(‘}’, ‘{‘);</p>
<p>this.mappings.put(‘]’, ‘[‘);</p>
<p>}</p>
<p>public boolean isValid(String s) {</p>
<p>// Initialize a stack to be used in the algorithm.</p>
<p>Stack&lt;Character&gt; stack = new Stack&lt;Character&gt;();</p>
<p>for (int i = 0; i &lt; s.length(); i++) {</p>
<p>char c = s.charAt(i);</p>
<p>// If the current character is a closing bracket.</p>
<p>if (this.mappings.containsKey(c)) {</p>
<p>// Get the top element of the stack. If the stack is empty, set a dummy value of<br>‘#‘</p>
<p>char topElement = stack.empty() ? ‘#‘ : stack.pop();</p>
<p>// If the mapping for this bracket doesn’t match the stack’s top element, return<br>false.</p>
<p>if (topElement != this.mappings.get(c)) {</p>
<p>return false;</p>
<p>}</p>
<p>} else {</p>
<p>// If it was an opening bracket, push to the stack.</p>
<p>stack.push(c);</p>
<p>}</p>
<p>}</p>
<p>// If the stack still contains elements, then it is an invalid expression.</p>
<p>return stack.isEmpty();</p>
<p>}</p>
<p>}</p>
<p>```</p>
<h2 id="15-8-数组中的第K个最大元素"><a href="#15-8-数组中的第K个最大元素" class="headerlink" title="15.8 数组中的第K个最大元素"></a>15.8 数组中的第K个最大元素</h2><p>链接：<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/kth-largest-element-in-an-array/">https://leetcode-cn.com/problems/kth-largest-element-in-an-array/</a></p>
<h3 id="15-8-1-问题描述"><a href="#15-8-1-问题描述" class="headerlink" title="15.8.1 问题描述"></a>15.8.1 问题描述</h3><p>在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k<br>个最大的元素，而不是第 k 个不同的元素。</p>
<p>示例 1:</p>
<p>```</p>
<p>输入: [3,2,1,5,6,4] 和 k = 2</p>
<p>输出: 5</p>
<p>```</p>
<p>示例 2:</p>
<p>```</p>
<p>输入: [3,2,3,1,2,4,5,5,6] 和 k = 4</p>
<p>输出: 4</p>
<p>```</p>
<p>说明:</p>
<p>你可以假设 k 总是有效的，且 1 ≤ k ≤ 数组的长度。</p>
<h3 id="15-8-2-参考答案"><a href="#15-8-2-参考答案" class="headerlink" title="15.8.2 参考答案"></a>15.8.2 参考答案</h3><p>```java</p>
<p>import java.util.Random;</p>
<p>class Solution {</p>
<p>int [] nums;</p>
<p>public void swap(int a, int b) {</p>
<p>int tmp = this.nums[a];</p>
<p>this.nums[a] = this.nums[b];</p>
<p>this.nums[b] = tmp;</p>
<p>}</p>
<p>public int partition(int left, int right, int pivot_index) {</p>
<p>int pivot = this.nums[pivot_index];</p>
<p>// 1. move pivot to end</p>
<p>swap(pivot_index, right);</p>
<p>int store_index = left;</p>
<p>// 2. move all smaller elements to the left</p>
<p>for (int i = left; i &lt;= right; i++) {</p>
<p>if (this.nums[i] &lt; pivot) {</p>
<p>swap(store_index, i);</p>
<p>store_index++;</p>
<p>}</p>
<p>}</p>
<p>// 3. move pivot to its final place</p>
<p>swap(store_index, right);</p>
<p>return store_index;</p>
<p>}</p>
<p>public int quickselect(int left, int right, int k_smallest) {</p>
<p>/*</p>
<p>Returns the k-th smallest element of list within left..right.</p>
<p>*/</p>
<p>if (left == right) // If the list contains only one element,</p>
<p>return this.nums[left]; // return that element</p>
<p>// select a random pivot_index</p>
<p>Random random_num = new Random();</p>
<p>int pivot_index = left + random_num.nextInt(right - left);</p>
<p>pivot_index = partition(left, right, pivot_index);</p>
<p>// the pivot is on (N - k)th smallest position</p>
<p>if (k_smallest == pivot_index)</p>
<p>return this.nums[k_smallest];</p>
<p>// go left side</p>
<p>else if (k_smallest &lt; pivot_index)</p>
<p>return quickselect(left, pivot_index - 1, k_smallest);</p>
<p>// go right side</p>
<p>return quickselect(pivot_index + 1, right, k_smallest);</p>
<p>}</p>
<p>public int findKthLargest(int[] nums, int k) {</p>
<p>this.nums = nums;</p>
<p>int size = nums.length;</p>
<p>// kth largest is (N - k)th smallest</p>
<p>return quickselect(0, size - 1, size - k);</p>
<p>}</p>
<p>}</p>
<p>```</p>
<h2 id="15-9-实现-Trie-前缀树"><a href="#15-9-实现-Trie-前缀树" class="headerlink" title="15.9 实现 Trie (前缀树)"></a>15.9 实现 Trie (前缀树)</h2><h3 id="15-9-1-问题描述"><a href="#15-9-1-问题描述" class="headerlink" title="15.9.1 问题描述"></a>15.9.1 问题描述</h3><p>实现一个 Trie (前缀树)，包含 insert, search, 和 startsWith 这三个操作。</p>
<p>示例:</p>
<p>```</p>
<p>Trie trie = new Trie();</p>
<p>trie.insert(“apple”);</p>
<p>trie.search(“apple”); // 返回 true</p>
<p>trie.search(“app”); // 返回 false</p>
<p>trie.startsWith(“app”); // 返回 true</p>
<p>trie.insert(“app”);</p>
<p>trie.search(“app”); // 返回 true</p>
<p>```</p>
<p>说明:</p>
<p>- 你可以假设所有的输入都是由小写字母 a-z 构成的。</p>
<p>- 保证所有输入均为非空字符串。</p>
<h3 id="15-9-2-参考答案"><a href="#15-9-2-参考答案" class="headerlink" title="15.9.2 参考答案"></a>15.9.2 参考答案</h3><p>```java</p>
<p>class Trie {</p>
<p>private TrieNode root;</p>
<p>public Trie() {</p>
<p>root = new TrieNode();</p>
<p>}</p>
<p>// Inserts a word into the trie.</p>
<p>public void insert(String word) {</p>
<p>TrieNode node = root;</p>
<p>for (int i = 0; i &lt; word.length(); i++) {</p>
<p>char currentChar = word.charAt(i);</p>
<p>if (!node.containsKey(currentChar)) {</p>
<p>node.put(currentChar, new TrieNode());</p>
<p>}</p>
<p>node = node.get(currentChar);</p>
<p>}</p>
<p>node.setEnd();</p>
<p>}</p>
<p>// search a prefix or whole key in trie and</p>
<p>// returns the node where search ends</p>
<p>private TrieNode searchPrefix(String word) {</p>
<p>TrieNode node = root;</p>
<p>for (int i = 0; i &lt; word.length(); i++) {</p>
<p>char curLetter = word.charAt(i);</p>
<p>if (node.containsKey(curLetter)) {</p>
<p>node = node.get(curLetter);</p>
<p>} else {</p>
<p>return null;</p>
<p>}</p>
<p>}</p>
<p>return node;</p>
<p>}</p>
<p>// Returns if the word is in the trie.</p>
<p>public boolean search(String word) {</p>
<p>TrieNode node = searchPrefix(word);</p>
<p>return node != null &amp;&amp; node.isEnd();</p>
<p>}</p>
<p>}</p>
<p>```</p>
<h2 id="15-10-编辑距离"><a href="#15-10-编辑距离" class="headerlink" title="15.10 编辑距离"></a>15.10 编辑距离</h2><p>链接：<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/edit-distance/">https://leetcode-cn.com/problems/edit-distance/</a></p>
<h3 id="15-10-1-问题描述"><a href="#15-10-1-问题描述" class="headerlink" title="15.10.1 问题描述"></a>15.10.1 问题描述</h3><p>给定两个单词 word1 和 word2，计算出将 word1 转换成 word2 所使用的最少操作数 。</p>
<p>你可以对一个单词进行如下三种操作：</p>
<ol>
<li><p>插入一个字符</p>
</li>
<li><p>删除一个字符</p>
</li>
<li><p>替换一个字符</p>
</li>
</ol>
<p>示例 1:</p>
<p>```</p>
<p>输入: word1 = “horse”, word2 = “ros”</p>
<p>输出: 3</p>
<p>解释:</p>
<p>horse -&gt; rorse (将 ‘h’ 替换为 ‘r’)</p>
<p>rorse -&gt; rose (删除 ‘r’)</p>
<p>rose -&gt; ros (删除 ‘e’)</p>
<p>```</p>
<p>示例 2:</p>
<p>```</p>
<p>输入: word1 = “intention”, word2 = “execution”</p>
<p>输出: 5</p>
<p>解释:</p>
<p>intention -&gt; inention (删除 ‘t’)</p>
<p>inention -&gt; enention (将 ‘i’ 替换为 ‘e’)</p>
<p>enention -&gt; exention (将 ‘n’ 替换为 ‘x’)</p>
<p>exention -&gt; exection (将 ‘n’ 替换为 ‘c’)</p>
<p>exection -&gt; execution (插入 ‘u’)</p>
<p>```</p>
<h3 id="15-10-2-参考答案"><a href="#15-10-2-参考答案" class="headerlink" title="15.10.2 参考答案"></a>15.10.2 参考答案</h3><p>```java</p>
<p>class Solution {</p>
<p>public int minDistance(String word1, String word2) {</p>
<p>int n = word1.length();</p>
<p>int m = word2.length();</p>
<p>// if one of the strings is empty</p>
<p>if (n * m == 0)</p>
<p>return n + m;</p>
<p>// array to store the convertion history</p>
<p>int [][] d = new int[n + 1][m + 1];</p>
<p>// init boundaries</p>
<p>for (int i = 0; i &lt; n + 1; i++) {</p>
<p>d[i][0] = i;</p>
<p>}</p>
<p>for (int j = 0; j &lt; m + 1; j++) {</p>
<p>d[0][j] = j;</p>
<p>}</p>
<p>// DP compute</p>
<p>for (int i = 1; i &lt; n + 1; i++) {</p>
<p>for (int j = 1; j &lt; m + 1; j++) {</p>
<p>int left = d[i - 1][j] + 1;</p>
<p>int down = d[i][j - 1] + 1;</p>
<p>int left_down = d[i - 1][j - 1];</p>
<p>if (word1.charAt(i - 1) != word2.charAt(j - 1))</p>
<p>left_down += 1;</p>
<p>d[i][j] = Math.min(left, Math.min(down, left_down));</p>
<p>}</p>
<p>}</p>
<p>return d[n][m];</p>
<p>}</p>
<p>}</p>
<p>```</p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='https://link.hhtjim.com/163/407002113.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
		data-enable='false'
        data-ae='false'
        data-ci=''
        data-cs=''
        data-r=''
        data-o=''
        data-a=''
        data-d='false'
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>






</html>
